"""
================================================================================
ARC-ONE: Octonionic Control Overlay for Abstract Reasoning Corpus
================================================================================

Complete implementation of M1-M7:
- M1: Minimal DSL with 24 operations + interpreter
- M2: Perception v0 with task feature extraction
- M3: Octonion guidance with φ embedding
- M4: Associator tension tracking + OCO cost overlay
- M5: OCO-guided beam search
- M6: Controller rotation (Observer/Navigator/Explorer)
- M7: ARC I/O + telemetry + submission packaging

Version: v2.7.9 - pair sampling when consensus fails (beam on first K pairs only)
Enhancements:
- v2.3.1: Centroid-biased shift enumeration for training search
- v2.4: Alignment finisher using bbox + cross-correlation with median aggregation
- v2.5: Block-mask finisher learns k×k (3×3 or 5×5) block-presence masks from
  training targets via majority vote, zeros unwanted tiled blocks for sparse
  tiling tasks, applied after alignment and before palette remapping
- v2.6: Blockwise color finisher learns k×k dominant-color projection from training
  targets, projects each block to learned dominant color for sparse/alternating
  block patterns, applied after alignment/block-mask and before palette
- v2.7: Added tile_masked operation for sparse lattice patterns (cross/border modes),
  enables plus-frame and perimeter-only tiling patterns, controller tuning for
  improved exploration (rotate at 0.98, explorer beam 384)
- v2.7.1: Shape-stage accuracy bias: when pred.shape == target.shape, downweight
  OCO (λ2 × 0.3) and add pixel accuracy bonus (β=0.05), plus early good-enough
  latch for seeds (acc >= 0.80) and mild tile_masked prior (-0.02) during shape stage
- v2.7.2: Baked phase-tile tension vectors (all tiling ops get [1,0,0,0,0,0,1,0] 
  to avoid zero-tension free ride), downweight both λ₁ (×0.50) and λ₂ (×0.30) when
  shape matches, enhanced early latch with palette finisher attempt and tile_masked
  tie-breaker (-0.02) for stable beam expansion on high-accuracy seeds
- v2.7.3: Seed-primed beam initialization via _quick_shape_candidates helper that
  scores all single-step tiling variants (tile, tile_masked, phase_tile_*) by pixel
  accuracy, early-latch returns immediately if seed+palette is exact, beam starts
  from top-3 accuracy-ranked seeds plus empty fallback for robust exploration
- v2.7.4: Consensus one-step sweep at solve_task start evaluates all tiling variants
  across ALL training pairs, accepts if mean accuracy >= 0.80 (fast path), optional
  shift micro-refinement tries ±1 deltas for tile_masked consensus solutions to fix
  off-by-one placement, beam search only runs if consensus fails or is weak
- v2.7.5: Conditional smart refiner in beam loop adds up to 3 targeted branches when
  shape matches and depth < 2 with acc >= 0.60: (1) alignment-guided shift from
  bbox+xcorr, (2) alternate tile_masked mode (swap cross/border), (3) mirror when
  acc >= 0.70; supplements standard successors without changing gating or finishers
- v2.7.6: Pareto-safe palette guard validates palette mapping on all training pairs,
  only applies to test predictions if it maintains or improves accuracy on every
  training pair (prevents palette-induced regressions)
- v2.7.7b: Automatic two-attempts for tile_masked programs ending with mode m∈{0,1},
  always emits {"attempt_1": primary, "attempt_2": alternate_mode} with independent
  finisher application, no flag needed (automatic detection), fallback to attempt_1
  if alternate fails, single-grid output for all other programs
- v2.7.7c: Two-attempts backward scan - finds last tile_masked(ky,kx,m) in program
  even when finishers (palette/shift/etc) follow it, swaps mode 0↔1 while preserving
  all trailing steps for robust alternate attempt generation
- v2.7.7d: Removed palette remapping from TEST predictions (kept Pareto-safe palette
  guard for TRAIN evaluation), applies only alignment/block-mask/blockwise-projection
  finishers to test outputs for cleaner generalization
- v2.7.7e: Bypass ALL finishers on TEST for masked-tiling two-attempts (no alignment,
  block-mask, blockwise-projection, or palette); emit raw program outputs to preserve
  border≈0.852 vs cross≈0.630 distinction; finishers still applied for TRAIN-FIT and
  single-attempt programs
- v2.7.7f: Two-attempts use truncated shape-base programs cut after last tile_masked,
  discarding any trailing finisher steps; both attempts use identical base transform
  with only mode (0↔1) swapped, ensuring pure apples-to-apples comparison of border
  vs cross patterns on TEST
- v2.7.8: Early-abort when ⅓ budget spent without shape progress (returns best-so-far
  or resize fallback); added keep_n_largest and keep_size_range component filter ops
  for connected component selection/filtering in shape family
- v2.7.8b: Added no-improvement early-abort (triggers when no cost improvement over
  ⅓ window); dual abort mechanisms catch both no-shape-progress and optimization
  stalls for faster failure on intractable tasks
- v2.7.8c: Hard-cap timeout immediately returns best-so-far (or resize fallback) instead
  of breaking from loop, eliminating any post-timeout overhead
- v2.7.9: Pair sampling when consensus fails - beam search runs on first K training pairs
  only (default K=2) instead of all pairs, preserving accuracy while cutting time on
  slow tasks; program still evaluated on all pairs for generalization measurement
"""

_NUMPY_IMPORT_ERROR = (
    "ARC-ONE requires the 'numpy' package. Install it with `pip install numpy` "
    "or ensure it is available in your execution environment before running the solver."
)

try:
    import numpy as np  # type: ignore
    _NUMPY_AVAILABLE = True
except ModuleNotFoundError:  # pragma: no cover - depends on environment
    _NUMPY_AVAILABLE = False

    class _MissingNumpy:
        """Proxy that surfaces a helpful error message when numpy is absent."""

        class ndarray:  # minimal stand-in for isinstance checks
            pass

        def __getattr__(self, name):
            raise ModuleNotFoundError(_NUMPY_IMPORT_ERROR)

    np = _MissingNumpy()  # type: ignore
from dataclasses import dataclass, field
from typing import Dict, List, Tuple, Optional, Any, Set, Iterable
from collections import deque, defaultdict
import itertools
import math
import time
import json
import os
import glob
import sys
import argparse
import tempfile
import shutil
import hashlib


# ==================================================================================
# REGISTER TOKENS (for multi-grid operations)
# ==================================================================================

REG_PREV  = "__G_MINUS_1__"
REG_PREV2 = "__G_MINUS_2__"


def resolve_grid_arg(arg, states):
    """Resolve grid references to actual grids from state history."""
    if arg == REG_PREV:  
        return states[-1] if len(states) >= 1 else None
    if arg == REG_PREV2: 
        return states[-2] if len(states) >= 2 else None
    return arg


# ==================================================================================
# OCO-GUIDED TWO-ATTEMPTS HELPERS (Attempt 2 generation + robust task dir resolve)
# ==================================================================================

def _symmetry_flags_np(grid_ll):
    g = np.array(grid_ll)
    H = int(np.array_equal(g, np.fliplr(g)))
    V = int(np.array_equal(g, np.flipud(g)))
    R = int(np.array_equal(g, np.rot90(g, 2)))
    return H, V, R


def _center_on_mass_np(grid_ll):
    g = np.array(grid_ll)
    mask = (g != 0)
    if not mask.any():
        return g.tolist()
    ys, xs = np.where(mask)
    cy, cx = int(np.round(ys.mean())), int(np.round(xs.mean()))
    H, W = g.shape
    ty = (H // 2) - cy
    tx = (W // 2) - cx
    out = np.zeros_like(g)
    y_src0 = max(0, -ty)
    y_dst0 = max(0, ty)
    x_src0 = max(0, -tx)
    x_dst0 = max(0, tx)
    h = min(H - y_dst0, H - y_src0)
    w = min(W - x_dst0, W - x_src0)
    if h > 0 and w > 0:
        out[y_dst0:y_dst0 + h, x_dst0:x_dst0 + w] = g[y_src0:y_src0 + h, x_src0:x_src0 + w]
    return out.tolist()


def _translate_toward_input_centroid_np(pred_ll, x_in_ll):
    g = np.array(pred_ll)
    xin = np.array(x_in_ll)
    if not (g != 0).any() or not (xin != 0).any():
        return g.tolist()
    yP, xP = np.where(g != 0)
    yX, xX = np.where(xin != 0)
    cyP, cxP = int(np.round(yP.mean())), int(np.round(xP.mean()))
    cyX, cxX = int(np.round(yX.mean())), int(np.round(xX.mean()))
    ty, tx = (cyX - cyP), (cxX - cxP)
    out = np.zeros_like(g)
    H, W = g.shape
    y_src0 = max(0, -ty)
    y_dst0 = max(0, ty)
    x_src0 = max(0, -tx)
    x_dst0 = max(0, tx)
    h = min(H - y_dst0, H - y_src0)
    w = min(W - x_dst0, W - x_src0)
    if h > 0 and w > 0:
        out[y_dst0:y_dst0 + h, x_dst0:x_dst0 + w] = g[y_src0:y_src0 + h, x_src0:x_src0 + w]
    return out.tolist()


def _palette_confusion_from_train_pairs(train_pairs, max_colors=5):
    """Build confusion matrix for optimal bijection palette mapping."""
    if not train_pairs:
        return None
    in_colors = []
    out_colors = []
    counts = defaultdict(int)
    for x, y in train_pairs:
        x = np.asarray(x); y = np.asarray(y)
        h = min(x.shape[0], y.shape[0]); w = min(x.shape[1], y.shape[1])
        if h <= 0 or w <= 0: 
            continue
        xs = x[:h, :w]; ys = y[:h, :w]
        for a, b in zip(xs.ravel(), ys.ravel()):
            counts[(int(a), int(b))] += 1
            in_colors.append(int(a)); out_colors.append(int(b))
    if not counts:
        return None
    in_set  = sorted(set(in_colors))
    out_set = sorted(set(out_colors))
    if len(in_set) > max_colors or len(out_set) > max_colors:
        return (in_set, out_set, None)  # will fallback
    # Build confusion matrix
    idx_in  = {c:i for i,c in enumerate(in_set)}
    idx_out = {c:j for j,c in enumerate(out_set)}
    C = np.zeros((len(in_set), len(out_set)), dtype=np.int32)
    for (a,b), cnt in counts.items():
        if a in idx_in and b in idx_out:
            C[idx_in[a], idx_out[b]] += cnt
    return (in_set, out_set, C)


def _optimal_bijection_mapping(train_pairs, max_colors=5):
    """Find optimal bijection using permutation brute-force for small palettes."""
    res = _palette_confusion_from_train_pairs(train_pairs, max_colors=max_colors)
    if res is None:
        return None
    in_set, out_set, C = res
    # Require balanced palettes and a valid confusion matrix
    if C is None or len(in_set) == 0 or len(in_set) != len(out_set) or len(in_set) > max_colors:
        return None
    n = len(in_set)
    best_score = -1
    best_perm = None
    for perm in itertools.permutations(range(len(out_set))):
        score = sum(C[i, perm[i]] for i in range(n))
        if score > best_score:
            best_score = score
            best_perm = perm
    if best_perm is None:
        return None
    mapping = { in_set[i]: out_set[best_perm[i]] for i in range(len(in_set)) }
    # Leave background 0 unmapped if it maps to itself only
    if mapping.get(0, None) == 0:
        mapping.pop(0, None)
    return mapping


def _majority_palette_mapping(train_pairs):
    """Majority heuristic palette mapping (fallback for unbalanced palettes)."""
    if not train_pairs:
        return {}
    tally = {}
    for x, y in train_pairs:
        x = np.asarray(x); y = np.asarray(y)
        h = min(x.shape[0], y.shape[0]); w = min(x.shape[1], y.shape[1])
        if h <= 0 or w <= 0: 
            continue
        xs = x[:h, :w]; ys = y[:h, :w]
        for c in np.unique(xs):
            mask = (xs == c)
            if mask.any():
                targets = ys[mask]
                if targets.size:
                    vals, cnts = np.unique(targets, return_counts=True)
                    target = int(vals[np.argmax(cnts)])
                    tally.setdefault(int(c), {}).setdefault(target, 0)
                    tally[int(c)][target] += int(cnts.max())
    if not tally:
        return {}
    mapping = {c: max(v.items(), key=lambda kv: kv[1])[0] for c, v in tally.items()}
    if mapping.get(0) == 0:
        mapping.pop(0, None)
    return mapping


def _palette_map_from_train_pairs(train_pairs):
    """Try optimal bijection first (balanced small palettes), else majority."""
    m = _optimal_bijection_mapping(train_pairs, max_colors=5)
    if m: 
        return m
    return _majority_palette_mapping(train_pairs)


def _apply_palette_map_ll(grid_ll, mapping):
    if not mapping:
        return grid_ll
    g = np.array(grid_ll, copy=True)
    for src, dst in mapping.items():
        g[g == src] = dst
    return g.tolist()


def _apply_palette_map_safe(pred: np.ndarray, target: np.ndarray, mapping: dict) -> np.ndarray:
    """
    Pareto-safe palette application: only apply mapping if it doesn't decrease accuracy.
    Returns mapped version if accuracy >= before, else returns original pred.
    """
    if not mapping or pred.shape != target.shape:
        return pred
    before = float((pred == target).mean())
    mapped = np.array(_apply_palette_map_ll(pred.tolist(), mapping), dtype=np.int32)
    after  = float((mapped == target).mean())
    return mapped if after >= before else pred


def _alt_mode_program(prog):
    """If prog ends with tile_masked(ky,kx,m) where m∈{0,1}, return
    a new Program with the same steps except the last op uses (1-m).
    Otherwise return None.
    """
    if not prog or not prog.steps:
        return None
    last = prog.steps[-1]
    if last.op != "tile_masked":
        return None
    ky, kx, m = map(int, last.args)
    if m not in (0, 1):
        return None
    alt = 1 - m
    new_steps = list(prog.steps[:-1]) + [Step("tile_masked", (ky, kx, alt))]
    return Program(new_steps)


def _alt_mode_program_scan(prog: Program):
    """
    Return an alternate Program by swapping the last tile_masked(ky,kx,m) where m∈{0,1},
    even if color/shift steps follow it. Keep trailing steps intact. If not found, return None.
    """
    if prog is None or not prog.steps:
        return None
    steps = list(prog.steps)
    for idx in range(len(steps)-1, -1, -1):
        s = steps[idx]
        if s.op == "tile_masked":
            ky, kx, m = map(int, s.args)
            if m in (0,1):
                alt = 1 - m
                alt_steps = steps[:]
                alt_steps[idx] = Step("tile_masked", (ky, kx, alt))
                return Program(alt_steps)
            break
    return None


def _truncate_at_last_tile_masked(prog):
    """Return (base_steps, ky, kx, m) by cutting the program after the last tile_masked(ky,kx,m) where m∈{0,1}.
    If not found, return (None, None, None, None)."""
    if prog is None or not prog.steps:
        return None, None, None, None
    steps = list(prog.steps)
    for idx in range(len(steps)-1, -1, -1):
        s = steps[idx]
        if s.op == "tile_masked":
            ky, kx, m = map(int, s.args)
            if m in (0, 1):
                return steps[:idx+1], ky, kx, m
            break
    return None, None, None, None


def _attempt2_from_strategy(att1_ll, strategy, phi_arr, train_pairs, test_input_ll):
    g = np.array(att1_ll)

    if strategy == "rotate90":
        result = np.rot90(g, 1).tolist()
    elif strategy == "rot180":
        result = np.rot90(g, 2).tolist()
    elif strategy == "flipH":
        result = np.fliplr(g).tolist()
    elif strategy == "flipV":
        result = np.flipud(g).tolist()
    elif strategy == "center":
        result = _center_on_mass_np(att1_ll)
    elif strategy == "toward_input":
        if test_input_ll is None:
            result = att1_ll
        else:
            result = _translate_toward_input_centroid_np(att1_ll, test_input_ll)
    elif strategy == "palette_swap":
        mapping = _palette_map_from_train_pairs(train_pairs or [])
        result = _apply_palette_map_ll(att1_ll, mapping)
    elif strategy == "auto":
        if test_input_ll is not None:
            Hs, Vs, Rs = _symmetry_flags_np(test_input_ll)
            if Hs and not Vs:
                result = np.fliplr(g).tolist()
            elif Vs and not Hs:
                result = np.flipud(g).tolist()
            elif Rs:
                result = np.rot90(g, 2).tolist()
            else:
                if g.shape[0] == g.shape[1]:
                    result = np.rot90(g, 1).tolist()
                else:
                    result = _center_on_mass_np(att1_ll)
        else:
            if g.shape[0] == g.shape[1]:
                result = np.rot90(g, 1).tolist()
            else:
                result = _center_on_mass_np(att1_ll)
    elif strategy == "oco_auto":
        if phi_arr is None or len(phi_arr) != 8:
            result = _attempt2_from_strategy(att1_ll, "auto", None, train_pairs, test_input_ll)
        else:
            a = np.abs(np.array(phi_arr))
            scores = {"geom": float(a[3]), "palette": float(a[2]), "align": float(a[4]), "objectness": float(a[1])}
            fam = max(scores.items(), key=lambda kv: kv[1])[0]
            if fam == "geom":
                if test_input_ll is not None:
                    Hs, Vs, Rs = _symmetry_flags_np(test_input_ll)
                    if Rs:
                        result = np.rot90(g, 2).tolist()
                    elif Hs and not Vs:
                        result = np.fliplr(g).tolist()
                    elif Vs and not Hs:
                        result = np.flipud(g).tolist()
                    else:
                        result = np.rot90(g, 1).tolist()
                else:
                    result = np.rot90(g, 1).tolist()
            elif fam == "palette":
                mapping = _palette_map_from_train_pairs(train_pairs or [])
                if mapping:
                    result = _apply_palette_map_ll(att1_ll, mapping)
                else:
                    result = _attempt2_from_strategy(att1_ll, "auto", None, train_pairs, test_input_ll)
            elif fam == "align":
                result = _center_on_mass_np(att1_ll)
            elif fam == "objectness":
                if test_input_ll is not None:
                    result = _translate_toward_input_centroid_np(att1_ll, test_input_ll)
                else:
                    result = _center_on_mass_np(att1_ll)
            else:
                result = _attempt2_from_strategy(att1_ll, "auto", None, train_pairs, test_input_ll)
    else:
        result = att1_ll

    # Apply palette remapping as a finishing step (like solve_task does)
    # This can improve accuracy by fixing color mismatches
    if train_pairs and strategy not in ("palette_swap",):  # Don't double-apply
        mapping = _palette_map_from_train_pairs(train_pairs)
        if mapping:
            result_with_palette = _apply_palette_map_ll(result, mapping)
            # In a production setting, you might want to score both and pick the best
            # For now, we'll just return the palette-remapped version
            return result_with_palette
    
    return result


def _build_train_test_lookups(tasks_dir):
    train_lookup = {}
    test_lookup = {}
    for task_id, task_json in load_tasks_from_dir(tasks_dir):
        pairs = trains_from_task(task_json)
        tests = tests_from_task(task_json)
        train_lookup[task_id] = pairs
        test_lookup[task_id] = tests
    return train_lookup, test_lookup


def _two_attempts_from_results(results, tasks_dir, strategy="oco_auto"):
    train_lookup, test_lookup = _build_train_test_lookups(tasks_dir)
    out = {}
    for tid, outs in results.items():
        pairs = train_lookup.get(tid, [])
        if pairs:
            tf = task_features(pairs)
            phi = compute_phi(tf)
        else:
            phi = None
        if tid not in test_lookup:
            continue
        if not isinstance(outs, list):
            outs = [outs]
        out_two = []
        for idx, pred_ll in enumerate(outs):
            tin = None
            if test_lookup[tid] and idx < len(test_lookup[tid]):
                tin = test_lookup[tid][idx]
            a2 = _attempt2_from_strategy(pred_ll, strategy, phi, pairs, tin)
            out_two.append({"attempt_1": pred_ll, "attempt_2": a2})
        out[tid] = out_two
    return out


def _find_arc_tasks_dir_fallback(requested_dir):
    if os.path.isdir(requested_dir):
        files = glob.glob(os.path.join(requested_dir, "*.json"))
        if files:
            return requested_dir
    parent = os.path.dirname(requested_dir)
    if parent:
        alt = os.path.join(parent, "arc-agi_test_challenges.json")
        if os.path.isfile(alt):
            test_dir = os.path.join(parent, "test")
            if os.path.isdir(test_dir):
                return test_dir
    return requested_dir


# ============================================================================
# ============ Alignment Finisher Utilities ============
# ============================================================================

def _binary_mask(a):
    import numpy as np
    return (np.asarray(a) != 0).astype(np.int32)

def _bbox_top_left(a):
    import numpy as np
    g = np.asarray(a)
    mask = (g != 0)
    if not mask.any():
        return None
    ys, xs = np.where(mask)
    return int(ys.min()), int(xs.min())

def _overlap_score(maskA, maskB, dy, dx):
    import numpy as np
    A = maskA; B = maskB
    H, W = B.shape
    # place A on B with shift (dy,dx), count overlap of 1s
    y_ps = max(0,  dy);  x_ps = max(0,  dx)
    y_bs = max(0, -dy);  x_bs = max(0, -dx)
    h = H - abs(dy);     w = W - abs(dx)
    if h <= 0 or w <= 0:
        return 0
    return int((A[y_ps:y_ps+h, x_ps:x_ps+w] & B[y_bs:y_bs+h, x_bs:x_bs+w]).sum())

def _propose_alignment_deltas(pred, truth, window=3):
    """
    Return a small candidate set of (dy,dx) using bbox + cross-corr (overlap)
    plus neighbors; clip to ±window.
    """
    import numpy as np
    pm = _binary_mask(pred); tm = _binary_mask(truth)
    # bbox proposal
    bbox = []
    bpp = _bbox_top_left(pred)
    btt = _bbox_top_left(truth)
    if bpp and btt:
        bbox = [(int(btt[0]-bpp[0]), int(btt[1]-bpp[1]))]

    # cross-corr (maximize overlap in a small window)
    best = (0, 0, -1)
    for dy in range(-window, window+1):
        for dx in range(-window, window+1):
            s = _overlap_score(pm, tm, dy, dx)
            if s > best[2]:
                best = (dy, dx, s)

    cand = [(0,0)]
    if bbox:
        by, bx = bbox[0]
        cand += [(by, bx), (by+1, bx), (by-1, bx), (by, bx+1), (by, bx-1)]
    cy, cx = best[0], best[1]
    cand += [(cy, cx), (cy+1, cx), (cy-1, cx), (cy, cx+1), (cy, cx-1)]

    # clip & dedup
    uniq = []
    seen = set()
    for (dy,dx) in cand:
        dy = int(np.clip(dy, -window, window))
        dx = int(np.clip(dx, -window, window))
        if (dy,dx) not in seen:
            uniq.append((dy,dx)); seen.add((dy,dx))
    return uniq

def _current_acc_state(pred: np.ndarray, target: np.ndarray):
    """Return (acc, (cy_p,cx_p), (cy_t,cx_t)) or (0.0, None, None) if shape mismatch."""
    if pred.shape != target.shape: 
        return 0.0, None, None
    acc = float((pred == target).mean())
    def _centroid(g):
        ys, xs = np.where(g != 0)
        return (float(ys.mean()), float(xs.mean())) if ys.size else (None, None)
    return acc, _centroid(pred), _centroid(target)

def _learn_task_alignment(program, train_pairs, phi, settings):
    """
    For each training pair, pick the best (dy,dx) from a small candidate set
    (bbox + xcorr proposals); return the median (dy,dx) over pairs.
    """
    import numpy as np
    dy_list, dx_list = [], []
    for (x, y) in train_pairs:
        try:
            pred = interpret_program(program, x)
        except Exception:
            continue
        # only consider alignment if shapes match
        if pred.shape != y.shape:
            continue
        cand = _propose_alignment_deltas(pred, y, window=3)
        if not cand:
            continue
        # choose by pixel accuracy
        best_acc, best_dy, best_dx = -1.0, 0, 0
        H, W = pred.shape
        for (dy,dx) in cand:
            shifted = np.zeros_like(pred)
            y0 = max(0,  dy);  x0 = max(0,  dx)
            ys = max(0, -dy);  xs = max(0, -dx)
            h = H - abs(dy);    w = W - abs(dx)
            if h <= 0 or w <= 0:
                continue
            shifted[y0:y0+h, x0:x0+w] = pred[ys:ys+h, xs:xs+w]
            acc = (shifted == y).mean()
            if acc > best_acc:
                best_acc, best_dy, best_dx = acc, dy, dx
        dy_list.append(best_dy); dx_list.append(best_dx)
    if not dy_list:
        return None
    return (int(np.median(dy_list)), int(np.median(dx_list)))


# ============================================================================
# ============ Block-Mask Finisher Utilities ============
# ============================================================================

def _divisible_shape(in_shape, out_shape):
    Hin, Win = in_shape
    Hout, Wout = out_shape
    if Hin <= 0 or Win <= 0: 
        return None
    if Hout % Hin != 0 or Wout % Win != 0:
        return None
    return (Hout // Hin, Wout // Win)

def _downsample_block_presence(y, ky, kx):
    """
    For a target grid y with shape (Hout, Wout) and block (Hin, Win) = (Hout/ky, Wout/kx),
    compute a boolean mask M[ky,kx] that marks whether each block is *meaningfully nonzero*.
    """
    import numpy as np
    y = np.asarray(y)
    Hout, Wout = y.shape
    Hin, Win   = Hout // ky, Wout // kx
    M = np.zeros((ky, kx), dtype=bool)
    for by in range(ky):
        for bx in range(kx):
            y0, x0 = by*Hin, bx*Win
            block = y[y0:y0+Hin, x0:x0+Win]
            # "meaningfully nonzero": at least one non-zero AND not trivially dense background (heuristic)
            if np.any(block != 0):
                M[by, bx] = True
    return M

def _learn_block_mask(train_pairs, ky, kx):
    """
    Aggregate block-presence across all training targets; return a majority-vote mask M[ky,kx].
    If ambiguity is high, fall back to all-True (no masking).
    """
    import numpy as np
    votes = np.zeros((ky, kx), dtype=np.int32)
    total = 0
    for (_x, y) in train_pairs:
        y = np.asarray(y)
        Hout, Wout = y.shape
        if Hout % ky != 0 or Wout % kx != 0:
            continue
        M = _downsample_block_presence(y, ky, kx)
        votes += M.astype(np.int32)
        total += 1
    if total == 0:
        return None
    # majority vote (>= half)
    thresh = (total + 1) // 2
    Mmaj = votes >= thresh
    # avoid degenerate all-False; if nearly full, keep all-True (no mask)
    if not Mmaj.any():
        return None
    return Mmaj

def _apply_block_mask(grid, ky, kx, M):
    """
    Zero out blocks where M[by,bx] == False. Assumes grid shape (Hout,Wout) divisible by ky,kx.
    """
    import numpy as np
    g = np.asarray(grid)
    Hout, Wout = g.shape
    Hin, Win   = Hout // ky, Wout // kx
    out = g.copy()
    for by in range(ky):
        for bx in range(kx):
            if not M[by, bx]:
                y0, x0 = by*Hin, bx*Win
                out[y0:y0+Hin, x0:x0+Win] = 0
    return out


# ============================================================================
# ============ Blockwise Color Finisher Utilities ============
# ============================================================================

def _block_shapes(out_shape, ky, kx):
    Hout, Wout = out_shape
    if Hout % ky != 0 or Wout % kx != 0:
        return None
    Hin, Win = Hout // ky, Wout // kx
    return Hin, Win

def _blockwise_dominant_colors(y, ky, kx):
    """
    For a target grid y (Hout,Wout) divisible by ky,kx, return an array D[ky,kx]
    with the dominant (majority) color per block.
    """
    import numpy as np
    y = np.asarray(y)
    Hout, Wout = y.shape
    Hin, Win = _block_shapes((Hout, Wout), ky, kx)
    if Hin is None:
        return None
    D = np.zeros((ky, kx), dtype=np.int32)
    for by in range(ky):
        for bx in range(kx):
            y0, x0 = by*Hin, bx*Win
            block = y[y0:y0+Hin, x0:x0+Win]
            vals, cnts = np.unique(block, return_counts=True)
            D[by, bx] = int(vals[np.argmax(cnts)])
    return D

def _learn_blockwise_projection(train_pairs, ky, kx):
    """
    Learn a blockwise dominant-color projection from training targets.
    Majority-vote the dominant color per block across all pairs.
    Returns D[ky,kx] or None if shapes are inconsistent.
    """
    import numpy as np
    votes = None
    total = 0
    for (_x, y) in train_pairs:
        y = np.asarray(y)
        HinWin = _block_shapes(y.shape, ky, kx)
        if HinWin is None:
            continue
        D = _blockwise_dominant_colors(y, ky, kx)
        if D is None:
            continue
        if votes is None:
            # store as dict of counters per block
            votes = [[{} for _ in range(kx)] for _ in range(ky)]
        for by in range(ky):
            for bx in range(kx):
                c = int(D[by, bx])
                votes[by][bx][c] = votes[by][bx].get(c, 0) + 1
        total += 1

    if votes is None or total == 0:
        return None

    Dmaj = [[0 for _ in range(kx)] for _ in range(ky)]
    for by in range(ky):
        for bx in range(kx):
            cnts = votes[by][bx]
            if not cnts:
                Dmaj[by][bx] = 0
            else:
                # majority color per block
                Dmaj[by][bx] = max(cnts.items(), key=lambda kv: kv[1])[0]
    import numpy as np
    return np.array(Dmaj, dtype=np.int32)

def _apply_blockwise_projection(pred, ky, kx, D):
    """
    Project each block of pred to the learned dominant target color D[by,bx].
    For now, we set all non-zero pixels in the block to D[by,bx] (zero stays zero).
    """
    import numpy as np
    g = np.asarray(pred).copy()
    Hout, Wout = g.shape
    Hin, Win = _block_shapes((Hout, Wout), ky, kx)
    if Hin is None or D is None:
        return g
    for by in range(ky):
        for bx in range(kx):
            y0, x0 = by*Hin, bx*Win
            blk = g[y0:y0+Hin, x0:x0+Win]
            tgt = int(D[by, bx])
            mask = (blk != 0)
            blk[mask] = tgt
            g[y0:y0+Hin, x0:x0+Win] = blk
    return g



# ============================================================================
# ============ dsl/ops.py ============
# ============================================================================

def _crop_border(arr, margin=1):
    """Remove `margin` rows/cols from all 4 sides (for op=shrink)."""
    if arr.shape[0] <= 2*margin or arr.shape[1] <= 2*margin:
        return arr
    return arr[margin:-margin, margin:-margin].copy()

def _pad_expand(arr, amount=1, fillval=0):
    """Add `amount` rows/cols on all 4 sides."""
    return np.pad(arr, amount, mode='constant', constant_values=fillval)

def _isolate_largest_region(arr):
    """Zero everything except the largest connected component (4-neighbor)."""
    from collections import deque
    H, W = arr.shape
    visited = np.zeros((H, W), dtype=bool)
    components = []
    for y0 in range(H):
        for x0 in range(W):
            if not visited[y0, x0] and arr[y0, x0] != 0:
                region = []
                q = deque([(y0, x0)])
                visited[y0, x0] = True
                while q:
                    y, x = q.popleft()
                    region.append((y, x))
                    for dy, dx in [(-1,0),(1,0),(0,-1),(0,1)]:
                        ny, nx = y+dy, x+dx
                        if 0 <= ny < H and 0 <= nx < W:
                            if not visited[ny, nx] and arr[ny, nx] == arr[y, x]:
                                visited[ny, nx] = True
                                q.append((ny, nx))
                components.append(region)
    if not components:
        return np.zeros_like(arr)
    largest = max(components, key=len)
    out = np.zeros_like(arr)
    for (y, x) in largest:
        out[y, x] = arr[y, x]
    return out

def _conn_comps(arr):
    """Extract all connected components (4-neighbor) as (color, pts) tuples."""
    H, W = arr.shape
    seen = np.zeros((H, W), dtype=bool)
    comps = []
    for y in range(H):
        for x in range(W):
            if arr[y, x] != 0 and not seen[y, x]:
                color = arr[y, x]
                q = [(y, x)]
                seen[y, x] = True
                pts = []
                while q:
                    yy, xx = q.pop()
                    pts.append((yy, xx))
                    for dy, dx in [(-1,0),(1,0),(0,-1),(0,1)]:
                        ny, nx = yy+dy, xx+dx
                        if 0 <= ny < H and 0 <= nx < W and not seen[ny, nx] and arr[ny, nx] == color:
                            seen[ny, nx] = True
                            q.append((ny, nx))
                comps.append((color, pts))
    return comps

def _op_keep_n_largest(grid, n):
    """keep_n_largest <n>
    Keep only the n largest connected components by size.
    """
    comps = _conn_comps(grid)
    comps.sort(key=lambda c: len(c[1]), reverse=True)
    keep = set()
    for color, pts in comps[:max(0, int(n))]:
        for y, x in pts:
            keep.add((y, x))
    out = np.zeros_like(grid)
    for (y, x) in keep:
        out[y, x] = grid[y, x]
    return out

def _op_keep_size_range(grid, amin, amax):
    """keep_size_range <amin> <amax>
    Keep only connected components with size in [amin, amax].
    """
    amin, amax = int(amin), int(amax)
    comps = _conn_comps(grid)
    out = np.zeros_like(grid)
    for color, pts in comps:
        if amin <= len(pts) <= amax:
            for y, x in pts:
                out[y, x] = color
    return out

def _op_fill(grid, color):
    """fill <color>"""
    out = np.full_like(grid, color)
    return out

def _op_mirror(grid, axis):
    """mirror <axis>
    axis=0->flipud, axis=1->fliplr
    """
    if axis == 0:
        return np.flipud(grid)
    return np.fliplr(grid)

def _op_rot90(grid, k):
    """rot90 <k>
    k=1 => 90deg, k=2 => 180deg, k=3 => 270deg
    """
    return np.rot90(grid, k=k)

def _op_transpose(grid):
    """transpose"""
    return grid.T

def _op_shrink(grid):
    """shrink (margin=1)"""
    return _crop_border(grid, margin=1)

def _op_grow(grid):
    """grow (pad=1)"""
    return _pad_expand(grid, amount=1, fillval=0)

def _op_add(grid1, grid2):
    """add <grid2>"""
    combined = np.where(grid1 != 0, grid1, grid2)
    return combined

def _op_mask(grid, color):
    """mask <color>"""
    out = np.where(grid == color, grid, 0)
    return out

def _op_invert(grid):
    """invert (mod 10 for ARC)"""
    out = (grid + 5) % 10
    return out

def _op_duplicate(grid):
    """duplicate (identity)"""
    return grid.copy()

def _op_scale(grid, factor):
    """scale <factor>
    Naive nearest-neighbor. factor must be an int>=1.
    """
    if factor <= 0:
        return grid
    H, W = grid.shape
    out = np.zeros((H * factor, W * factor), dtype=grid.dtype)
    for i in range(H):
        for j in range(W):
            val = grid[i, j]
            for di in range(factor):
                for dj in range(factor):
                    out[i*factor+di, j*factor+dj] = val
    return out

def _op_tile(grid, vert, horiz):
    """tile <vert> <horiz>"""
    return np.tile(grid, (vert, horiz))

def _op_tile_masked(grid, ky, kx, mode):
    """
    tile_masked <ky> <kx> <mode>

    Tiling with selective block placement:
      mode=0: cross    -> keep blocks in center row or center column
      mode=1: border   -> keep blocks on the perimeter only
      mode=2: main_diag (optional) -> keep blocks where by == bx (requires ky==kx)
      mode=3: anti_diag (optional) -> keep blocks where by + bx == ky - 1 (requires ky==kx)

    Unselected blocks are set to 0. This enables sparse lattice patterns (e.g., plus frames).
    """
    import numpy as np
    g = np.asarray(grid)
    H, W = g.shape
    out = np.zeros((H*ky, W*kx), dtype=g.dtype)

    same = (ky == kx)
    for by in range(ky):
        for bx in range(kx):
            keep = False
            if mode == 0:
                keep = (by == ky // 2) or (bx == kx // 2)
            elif mode == 1:
                keep = (by == 0) or (by == ky - 1) or (bx == 0) or (bx == kx - 1)
            elif mode == 2 and same:
                keep = (by == bx)
            elif mode == 3 and same:
                keep = ((by + bx) == (ky - 1))
            if keep:
                y0, x0 = by*H, bx*W
                out[y0:y0+H, x0:x0+W] = g
    return out

def _op_crop(grid, color):
    """crop <color>
    Crop to bounding box of `color`.
    """
    mask = (grid == color)
    if not mask.any():
        return grid
    rows = np.any(mask, axis=1)
    cols = np.any(mask, axis=0)
    r0, r1 = np.where(rows)[0][[0, -1]]
    c0, c1 = np.where(cols)[0][[0, -1]]
    return grid[r0:r1+1, c0:c1+1].copy()

def _op_shift(grid, dy, dx):
    """shift <dy> <dx>"""
    H, W = grid.shape
    out = np.zeros_like(grid)
    y_src_start = max(0, -dy)
    y_dst_start = max(0, dy)
    x_src_start = max(0, -dx)
    x_dst_start = max(0, dx)
    h = H - abs(dy)
    w = W - abs(dx)
    if h > 0 and w > 0:
        out[y_dst_start:y_dst_start+h, x_dst_start:x_dst_start+w] = \
            grid[y_src_start:y_src_start+h, x_src_start:x_src_start+w]
    return out

def _op_replacecolor(grid, old_color, new_color):
    """replacecolor <old> <new>"""
    out = grid.copy()
    out[out == old_color] = new_color
    return out

def _op_swapcolors(grid, c1, c2):
    """swapcolors <c1> <c2>"""
    out = grid.copy()
    mask1 = (out == c1)
    mask2 = (out == c2)
    out[mask1] = c2
    out[mask2] = c1
    return out

def _op_outline(grid, color):
    """outline <color>
    Set the perimeter of all non-zero cells to <color>.
    """
    if grid.size == 0:
        return grid.copy()
    mask = (grid != 0)
    edges = np.zeros_like(grid, dtype=bool)
    H, W = grid.shape
    for y in range(H):
        for x in range(W):
            if mask[y, x]:
                for dy, dx in [(-1,0),(1,0),(0,-1),(0,1)]:
                    ny, nx = y+dy, x+dx
                    if 0 <= ny < H and 0 <= nx < W:
                        if not mask[ny, nx]:
                            edges[y, x] = True
                            break
    out = grid.copy()
    out[edges] = color
    return out

def _op_majority(grid):
    """majority
    Fill the entire grid with the most frequent non-zero color.
    """
    vals = grid[grid != 0]
    if vals.size == 0:
        return grid.copy()
    unique, counts = np.unique(vals, return_counts=True)
    maj = unique[np.argmax(counts)]
    return np.full_like(grid, maj)

def _op_minority(grid):
    """minority
    Fill the entire grid with the *least* frequent non-zero color.
    """
    vals = grid[grid != 0]
    if vals.size == 0:
        return grid.copy()
    unique, counts = np.unique(vals, return_counts=True)
    min_color = unique[np.argmin(counts)]
    return np.full_like(grid, min_color)

def _op_threshold(grid, val):
    """threshold <val>
    If pixel >= val => pixel, else 0.
    """
    out = np.where(grid >= val, grid, 0)
    return out

def _op_largest(grid):
    """largest
    Isolate the largest connected component.
    """
    return _isolate_largest_region(grid)

def _op_resize(grid, h, w):
    """resize <h> <w>
    Simple nearest-neighbor resize. If target is smaller, we truncate. If larger, we replicate edge.
    """
    H, W = grid.shape
    if H == h and W == w:
        return grid.copy()
    out = np.zeros((h, w), dtype=grid.dtype)
    for i in range(h):
        for j in range(w):
            src_i = min(int(i * H / h), H-1)
            src_j = min(int(j * W / w), W-1)
            out[i, j] = grid[src_i, src_j]
    return out

def _op_stack(grid1, grid2, axis):
    """stack <grid2> <axis>
    axis=0 => vertical stack, axis=1 => horizontal stack
    """
    if axis == 0:
        return np.vstack([grid1, grid2])
    return np.hstack([grid1, grid2])

def _op_subtract(grid1, grid2):
    """subtract <grid2>
    Wherever grid2 != 0, set grid1 to 0.
    """
    out = grid1.copy()
    out[grid2 != 0] = 0
    return out


def _op_phase_tile(grid, ky, kx, mode=0):
    """
    phase_tile <ky> <kx> [mode]
    mode=0: alternate rot180 on odd (by+bx)
    mode=1: alternate flipud on odd
    mode=2: alternate fliplr on odd
    Creates a k-by-k quilt where blocks with parity (by+bx)%2==1 use a transformed base.
    """
    import numpy as np
    H, W = grid.shape
    out = np.zeros((H*ky, W*kx), dtype=grid.dtype)
    for by in range(ky):
        for bx in range(kx):
            block = grid
            odd = (by + bx) % 2
            if odd:
                if mode == 0:
                    block = np.rot90(block, 2)
                elif mode == 1:
                    block = np.flipud(block)
                elif mode == 2:
                    block = np.fliplr(block)
            y0, x0 = by*H, bx*W
            out[y0:y0+H, x0:x0+W] = block
    return out

def _op_phase_tile_row(grid, ky, kx, mode=2):
    """
    phase_tile_row <ky> <kx> <mode>
    Transform every block in odd block-rows (by % 2 == 1).
    mode=0: rot180, 1: flipud, 2: fliplr (default)
    """
    import numpy as np
    H, W = grid.shape
    out = np.zeros((H*ky, W*kx), dtype=grid.dtype)
    for by in range(ky):
        for bx in range(kx):
            block = grid
            if (by % 2) == 1:
                if mode == 0:
                    block = np.rot90(block, 2)
                elif mode == 1:
                    block = np.flipud(block)
                else:
                    block = np.fliplr(block)
            y0, x0 = by*H, bx*W
            out[y0:y0+H, x0:x0+W] = block
    return out


def _op_phase_tile_col(grid, ky, kx, mode=2):
    """
    phase_tile_col <ky> <kx> <mode>
    Transform every block in odd block-columns (bx % 2 == 1).
    mode=0: rot180, 1: flipud, 2: fliplr (default)
    """
    import numpy as np
    H, W = grid.shape
    out = np.zeros((H*ky, W*kx), dtype=grid.dtype)
    for by in range(ky):
        for bx in range(kx):
            block = grid
            if (bx % 2) == 1:
                if mode == 0:
                    block = np.rot90(block, 2)
                elif mode == 1:
                    block = np.flipud(block)
                else:
                    block = np.fliplr(block)
            y0, x0 = by*H, bx*W
            out[y0:y0+H, x0:x0+W] = block
    return out



OP_NAMES_BASIC = [
    # (name,arity, param_types)
    # Shape-critical operations first for beam efficiency
    ("resize", 3, ["grid", "int", "int"]),
    ("tile", 3, ["grid", "int", "int"]),
    ("tile_masked", 4, ["grid", "int", "int", "int"]),
    ("phase_tile_row", 4, ["grid", "int", "int", "int"]),
    ("phase_tile_col", 4, ["grid", "int", "int", "int"]),
    ("phase_tile", 4, ["grid", "int", "int", "int"]),
    # Geometric and color operations
    ("fill", 2, ["grid", "int"]),
    ("mirror", 2, ["grid", "int"]),
    ("rot90", 2, ["grid", "int"]),
    ("transpose", 1, ["grid"]),
    ("shrink", 1, ["grid"]),
    ("grow", 1, ["grid"]),
    ("add", 2, ["grid", "grid"]),
    ("mask", 2, ["grid", "int"]),
    ("invert", 1, ["grid"]),
    ("duplicate", 1, ["grid"]),
    ("scale", 2, ["grid", "int"]),
    ("crop", 2, ["grid", "int"]),
    ("shift", 3, ["grid", "int", "int"]),
    ("replacecolor", 3, ["grid", "int", "int"]),
    ("swapcolors", 3, ["grid", "int", "int"]),
    ("outline", 2, ["grid", "int"]),
    ("majority", 1, ["grid"]),
    ("minority", 1, ["grid"]),
    ("threshold", 2, ["grid", "int"]),
    ("largest", 1, ["grid"]),
    ("stack", 3, ["grid", "grid", "int"]),
    ("subtract", 2, ["grid", "grid"]),
]

OP_REGISTRY = {
    "fill": _op_fill,
    "mirror": _op_mirror,
    "rot90": _op_rot90,
    "transpose": _op_transpose,
    "shrink": _op_shrink,
    "grow": _op_grow,
    "add": _op_add,
    "mask": _op_mask,
    "invert": _op_invert,
    "duplicate": _op_duplicate,
    "scale": _op_scale,
    "tile": _op_tile,
    "tile_masked": _op_tile_masked,
    "crop": _op_crop,
    "shift": _op_shift,
    "replacecolor": _op_replacecolor,
    "swapcolors": _op_swapcolors,
    "outline": _op_outline,
    "majority": _op_majority,
    "minority": _op_minority,
    "threshold": _op_threshold,
    "largest": _op_largest,
    "keep_n_largest": _op_keep_n_largest,
    "keep_size_range": _op_keep_size_range,
    "resize": _op_resize,
    "stack": _op_stack,
    "subtract": _op_subtract,
    "phase_tile": _op_phase_tile,
    "phase_tile_row": _op_phase_tile_row,
    "phase_tile_col": _op_phase_tile_col,
}


# ============================================================================
# ============ Operation Order and Families (O2) ============
# ============================================================================

# --- Operation order and families (O2) ---
OP_ORDER_O2 = [
    # Tile family first (agent's best ordering)
    "tile", "tile_masked", "phase_tile_row", "phase_tile_col", "phase_tile", "resize",
    # Core geometry next
    "rot90", "mirror", "transpose", "shift", "crop", "grow", "shrink",
    # Composition / structural
    "stack", "add", "subtract", "largest", "keep_n_largest", "keep_size_range", "scale", "duplicate",
    # Color / palette last
    "mask", "replacecolor", "swapcolors", "invert", "fill", "outline", "threshold", "majority", "minority",
]

# Build a map for fast ordering
_OP_RANK = {name: i for i, name in enumerate(OP_ORDER_O2)}

# Whitelists for shape-then-color gating
SHAPE_OPS = {
    "tile", "tile_masked", "phase_tile_row", "phase_tile_col", "phase_tile", "resize",
    "rot90", "mirror", "transpose", "shift", "crop", "grow", "shrink",
    "stack", "add", "subtract", "largest", "keep_n_largest", "keep_size_range", "scale", "duplicate",
}
COLOR_OPS = {
    "mask", "replacecolor", "swapcolors", "invert", "fill", "outline", "threshold", "majority", "minority",
    # permit small finishing shifts too if desired:
    "shift",
}


# ============================================================================
# ============ dsl/program.py ============
# ============================================================================

@dataclass
class Step:
    """A single step in a program: (op_name, args)."""
    op: str
    args: Tuple[Any, ...]

    def __repr__(self):
        return f"Step({self.op}, {self.args})"


@dataclass
class Program:
    """A sequence of Steps."""
    steps: List[Step] = field(default_factory=list)

    def __repr__(self):
        return f"Program({len(self.steps)} steps)"

    def __len__(self):
        return len(self.steps)

    def copy(self):
        return Program([Step(s.op, s.args) for s in self.steps])

    def to_tuple(self):
        """Hashable representation."""
        return tuple((s.op, s.args) for s in self.steps)


def program_to_str(prog: Program) -> str:
    """Human-readable format."""
    lines = []
    for i, step in enumerate(prog.steps):
        lines.append(f"{i+1}. {step.op}{step.args}")
    return "\n".join(lines)


# ============================================================================
# ============ dsl/interpreter.py ============
# ============================================================================

def interpret_program(prog: Program, input_grid: np.ndarray) -> np.ndarray:
    """Execute a program on an input grid, returning the final grid."""
    state = input_grid.copy()
    states = [state.copy()]  # Keep history for register references
    
    for step in prog.steps:
        state = apply_step(state, step, states)
        states.append(state.copy())
    
    return state


def apply_step(grid: np.ndarray, step: Step, states=None) -> np.ndarray:
    """Apply a single step to a grid, with support for grid register references."""
    if states is None:
        states = [grid]
    
    op_name = step.op
    args = step.args

    if op_name not in OP_REGISTRY:
        raise ValueError(f"Unknown op: {op_name}")

    op_fn = OP_REGISTRY[op_name]

    # Handle multi-grid ops with register resolution
    try:
        if step.op in ("add", "subtract"):
            g2 = resolve_grid_arg(args[0], states)
            if g2 is None:
                return grid.copy()
            return op_fn(grid, g2)
        
        if step.op == "stack":
            g2 = resolve_grid_arg(args[0], states)
            if g2 is None:
                return grid.copy()
            axis = int(args[1])
            return op_fn(grid, g2, axis)
        
        # Regular ops
        result = op_fn(grid, *args)
    except Exception as e:
        # If error, return unchanged grid
        result = grid.copy()

    return result


# ============================================================================
# ============ perception/features.py ============
# ============================================================================

def task_features(train_pairs: List[Tuple[np.ndarray, np.ndarray]]) -> Dict[str, Any]:
    """Extract features from train pairs for OCO."""
    if not train_pairs:
        return {
            "n_examples": 0,
            "avg_in_size": (0, 0),
            "avg_out_size": (0, 0),
            "size_stable": False,
            "shape_stable": False,
            "palette_size_in": 0,
            "palette_size_out": 0,
            "complexity": 0.0,
            "aspect_ratio_in": 1.0,
            "aspect_ratio_out": 1.0,
        }

    n = len(train_pairs)
    sizes_in = []
    sizes_out = []
    palettes_in = []
    palettes_out = []

    for (x, y) in train_pairs:
        sizes_in.append(x.shape)
        sizes_out.append(y.shape)
        palettes_in.append(len(np.unique(x)))
        palettes_out.append(len(np.unique(y)))

    avg_in_size = (
        int(np.mean([s[0] for s in sizes_in])),
        int(np.mean([s[1] for s in sizes_in]))
    )
    avg_out_size = (
        int(np.mean([s[0] for s in sizes_out])),
        int(np.mean([s[1] for s in sizes_out]))
    )

    size_stable = all(s == sizes_in[0] for s in sizes_in)
    shape_stable = all(s == sizes_out[0] for s in sizes_out)

    palette_in = int(np.mean(palettes_in))
    palette_out = int(np.mean(palettes_out))

    complexity = (palette_in + palette_out) / 2.0

    aspect_in = avg_in_size[1] / max(avg_in_size[0], 1)
    aspect_out = avg_out_size[1] / max(avg_out_size[0], 1)

    return {
        "n_examples": n,
        "avg_in_size": avg_in_size,
        "avg_out_size": avg_out_size,
        "size_stable": size_stable,
        "shape_stable": shape_stable,
        "palette_size_in": palette_in,
        "palette_size_out": palette_out,
        "complexity": complexity,
        "aspect_ratio_in": aspect_in,
        "aspect_ratio_out": aspect_out,
    }


# ============================================================================
# ============ oco/octonion.py ============
# ============================================================================

def compute_phi(features: Dict[str, Any]) -> np.ndarray:
    """
    Compute 8D octonion embedding φ from task features.
    
    φ = [scale, objectness, palette, geometry, alignment, topology, pattern, composition]
    """
    n = features["n_examples"]
    size_stable = features["size_stable"]
    shape_stable = features["shape_stable"]
    palette_in = features["palette_size_in"]
    palette_out = features["palette_size_out"]
    complexity = features["complexity"]
    aspect_in = features["aspect_ratio_in"]
    aspect_out = features["aspect_ratio_out"]

    # Scale component
    in_h, in_w = features["avg_in_size"]
    out_h, out_w = features["avg_out_size"]
    scale = math.log(max(out_h * out_w, 1)) - math.log(max(in_h * in_w, 1))

    # Objectness (palette difference)
    objectness = palette_out - palette_in

    # Palette (color complexity)
    palette = complexity / 10.0

    # Geometry (aspect ratio change)
    geometry = abs(aspect_out - aspect_in)

    # Alignment (size stability)
    alignment = 1.0 if size_stable else 0.0

    # Topology (shape stability)
    topology = 1.0 if shape_stable else 0.0

    # Pattern (number of examples)
    pattern = n / 10.0

    # Composition (interaction term)
    composition = scale * objectness * 0.1

    phi = np.array([
        scale,
        objectness,
        palette,
        geometry,
        alignment,
        topology,
        pattern,
        composition
    ], dtype=np.float32)

    return phi


def phi_to_family(phi: np.ndarray) -> str:
    """Classify task family from φ."""
    if phi is None or len(phi) != 8:
        return "unknown"

    abs_phi = np.abs(phi)
    idx = np.argmax(abs_phi)

    families = [
        "scale",
        "objectness",
        "palette",
        "geometry",
        "alignment",
        "topology",
        "pattern",
        "composition"
    ]

    return families[idx]


# ============================================================================
# ============ oco/associator.py ============
# ============================================================================

def compute_program_tension(prog: Program, phi: np.ndarray) -> float:
    """
    Compute tension T_prog(φ) between program structure and task embedding.
    
    T_prog = Σ_i |op_i ⊗ φ|
    
    This measures how well the program's operations align with the task's
    octonion structure.
    """
    if phi is None or len(phi) != 8:
        return 0.0

    tension = 0.0
    for step in prog.steps:
        op_vec = op_to_vector(step.op)
        # Simple dot product as pseudo-octonion multiplication
        tension += abs(np.dot(op_vec, phi))

    return tension


def compute_slice_tension(state: np.ndarray, phi: np.ndarray) -> float:
    """
    Compute tension T_slice(φ) between current state and task embedding.
    
    T_slice = |state_features ⊗ φ|
    
    This measures how well the current state aligns with the task's
    expected structure.
    """
    if phi is None or len(phi) != 8:
        return 0.0

    state_vec = state_to_vector(state)
    tension = abs(np.dot(state_vec, phi))

    return tension


def op_to_vector(op_name: str) -> np.ndarray:
    """Map operation to 8D vector for tension computation."""
    # Phase-tiling family gets scale + pattern axes to avoid zero-tension free ride
    PHASE_TILE_VEC = np.array([1, 0, 0, 0, 0, 0, 1, 0], dtype=np.float32)
    
    op_map = {
        "fill": [0, 0, 1, 0, 0, 0, 0, 0],
        "mirror": [0, 0, 0, 1, 1, 0, 0, 0],
        "rot90": [0, 0, 0, 1, 0, 0, 0, 0],
        "transpose": [0, 0, 0, 1, 0, 0, 0, 0],
        "shrink": [1, 0, 0, 0, 0, 0, 0, 0],
        "grow": [1, 0, 0, 0, 0, 0, 0, 0],
        "add": [0, 1, 0, 0, 0, 0, 0, 1],
        "mask": [0, 0, 1, 0, 0, 0, 0, 0],
        "invert": [0, 0, 1, 0, 0, 0, 0, 0],
        "duplicate": [0, 0, 0, 0, 0, 0, 0, 0],
        "scale": [1, 0, 0, 0, 0, 0, 0, 0],
        "tile": PHASE_TILE_VEC.tolist(),
        "tile_masked": PHASE_TILE_VEC.tolist(),
        "phase_tile": PHASE_TILE_VEC.tolist(),
        "phase_tile_row": PHASE_TILE_VEC.tolist(),
        "phase_tile_col": PHASE_TILE_VEC.tolist(),
        "crop": [1, 0, 0, 0, 0, 0, 0, 0],
        "shift": [0, 0, 0, 0, 1, 0, 0, 0],
        "replacecolor": [0, 0, 1, 0, 0, 0, 0, 0],
        "swapcolors": [0, 0, 1, 0, 0, 0, 0, 0],
        "outline": [0, 1, 0, 0, 0, 1, 0, 0],
        "majority": [0, 0, 1, 0, 0, 0, 0, 0],
        "minority": [0, 0, 1, 0, 0, 0, 0, 0],
        "threshold": [0, 0, 1, 0, 0, 0, 0, 0],
        "largest": [0, 1, 0, 0, 0, 1, 0, 0],
        "keep_n_largest": [0, 1, 0, 0, 0, 1, 0, 0],
        "keep_size_range": [0, 1, 0, 0, 0, 1, 0, 0],
        "resize": [1, 0, 0, 0, 0, 0, 0, 0],
        "stack": [0, 0, 0, 0, 0, 0, 0, 1],
        "subtract": [0, 1, 0, 0, 0, 0, 0, 0],
    }

    vec = op_map.get(op_name, [0, 0, 0, 0, 0, 0, 0, 0])
    return np.array(vec, dtype=np.float32)


def state_to_vector(state: np.ndarray) -> np.ndarray:
    """Map grid state to 8D vector for tension computation."""
    h, w = state.shape
    size = math.log(max(h * w, 1))
    n_colors = len(np.unique(state))
    aspect = w / max(h, 1)
    density = np.count_nonzero(state) / max(state.size, 1)

    vec = np.array([
        size,
        n_colors,
        density,
        aspect,
        0.0,  # alignment (computed elsewhere)
        0.0,  # topology (computed elsewhere)
        0.0,  # pattern (computed elsewhere)
        0.0,  # composition (computed elsewhere)
    ], dtype=np.float32)

    return vec


# ============================================================================
# ============ oco/cost.py ============
# ============================================================================

@dataclass
class SearchSettings:
    """Configuration for beam search with OCO."""
    beam_width: int = 128
    max_depth: int = 10
    max_seconds: float = 3.0
    lambda_len: float = 0.20
    lambda1: float = 0.30  # program tension weight
    lambda2: float = 0.20  # slice tension weight
    slice_guard_thresh: float = 0.80
    allow_offslice_early: bool = False
    public_mode: bool = False
    log_every: int = 200
    seed: int = 1337
    _disable_rotation: bool = False


def compute_cost(
    prog: Program,
    input_grid: np.ndarray,
    target_grid: np.ndarray,
    phi: np.ndarray,
    settings: SearchSettings
) -> float:
    """
    OCO-augmented cost function:
    
    C = C_match + λ_len*L + λ1*T_prog + λ2*T_slice
    
    Where:
    - C_match: Pixel mismatch cost
    - L: Program length
    - T_prog: Program tension
    - T_slice: Slice tension
    
    Shape-stage bias: When shape matches, downweight both λ1 and λ2, add accuracy bonus.
    """
    # Execute program
    try:
        pred = interpret_program(prog, input_grid)
    except Exception:
        return 1e9

    # Match cost
    if pred.shape != target_grid.shape:
        c_match = 1.0
    else:
        diff = np.sum(pred != target_grid)
        c_match = diff / max(target_grid.size, 1)

    shape_same = (pred.shape == target_grid.shape)
    acc = (1.0 - c_match) if shape_same else 0.0

    # Keep original lambda1, lambda2 first
    lam1 = settings.lambda1
    lam2 = settings.lambda2

    # AFTER shape-match: reduce both tensions + small pixel-accuracy bonus
    if shape_same:
        lam1 *= 0.50   # downweight program tension when shape matches
        lam2 *= 0.30   # downweight slice tension when shape matches
        beta = 0.05    # pixel-accuracy bonus
    else:
        beta = 0.0

    # Length cost
    c_len = len(prog) * settings.lambda_len

    # OCO costs
    t_prog = compute_program_tension(prog, phi)
    t_slice = compute_slice_tension(pred, phi)

    c_oco = lam1 * t_prog + lam2 * t_slice

    total = c_match + c_len + c_oco - beta * acc

    return float(total)


# ============================================================================
# ============ search/beam.py ============
# ============================================================================

@dataclass
class Candidate:
    """A candidate program with its cost."""
    program: Program
    cost: float
    depth: int = 0

    def __lt__(self, other):
        return self.cost < other.cost


def _centroid_nonzero(a):
    import numpy as np
    ys, xs = np.where(a != 0)
    if ys.size == 0: return None
    return int(np.round(ys.mean())), int(np.round(xs.mean()))


def _quick_shape_candidates(input_grid, target_grid):
    """
    Try a small set of single-step shape transforms; return up to top-3 seeds
    (Program, acc) ranked by pixel accuracy (shape must match).
    Targets sparse tilings like border/cross and basic phase tilings.
    """
    H, W = input_grid.shape
    Ho, Wo = target_grid.shape
    cand = [Program([Step("resize", (Ho, Wo))])]

    if H and W and (Ho % H == 0) and (Wo % W == 0):
        ky, kx = Ho // H, Wo // W
        cand += [
            Program([Step("tile", (ky, kx))]),
            Program([Step("tile_masked", (ky, kx, 0))]),  # cross
            Program([Step("tile_masked", (ky, kx, 1))]),  # border
            Program([Step("phase_tile_row", (ky, kx, 0))]),
            Program([Step("phase_tile_row", (ky, kx, 1))]),
            Program([Step("phase_tile_row", (ky, kx, 2))]),
            Program([Step("phase_tile_col", (ky, kx, 0))]),
            Program([Step("phase_tile_col", (ky, kx, 1))]),
            Program([Step("phase_tile_col", (ky, kx, 2))]),
            Program([Step("phase_tile",     (ky, kx, 0))]),
            Program([Step("phase_tile",     (ky, kx, 1))]),
            Program([Step("phase_tile",     (ky, kx, 2))]),
        ]

    scored, seen = [], set()
    for prog in cand:
        try:
            pred = interpret_program(prog, input_grid)
        except Exception:
            continue
        if pred.shape != target_grid.shape:
            continue
        acc = (pred == target_grid).mean()
        sig = prog.to_tuple()
        if sig not in seen:
            scored.append((float(acc), prog)); seen.add(sig)
    scored.sort(key=lambda t: (-t[0], len(t[1])))
    return scored[:3]


def _consensus_one_step_candidate(train_pairs):
    """
    Quick shape-consensus sweep across train pairs.
    Returns (best_prog_or_None, best_mean_acc) for a small set of one-step programs
    evaluated on ALL train pairs. Only shape-matched predictions are scored.
    """
    try:
        x0, y0 = train_pairs[0]
        H, W = x0.shape
        Ho, Wo = y0.shape
    except Exception:
        return None, 0.0

    cands = [Program([Step("resize", (Ho, Wo))])]

    if H and W and (Ho % H == 0) and (Wo % W == 0):
        ky, kx = Ho // H, Wo // W
        cands += [
            Program([Step("tile",        (ky, kx))]),
            Program([Step("tile_masked", (ky, kx, 0))]),  # cross
            Program([Step("tile_masked", (ky, kx, 1))]),  # border
            Program([Step("phase_tile_row", (ky, kx, 0))]),
            Program([Step("phase_tile_row", (ky, kx, 1))]),
            Program([Step("phase_tile_row", (ky, kx, 2))]),
            Program([Step("phase_tile_col", (ky, kx, 0))]),
            Program([Step("phase_tile_col", (ky, kx, 1))]),
            Program([Step("phase_tile_col", (ky, kx, 2))]),
            Program([Step("phase_tile",     (ky, kx, 0))]),
            Program([Step("phase_tile",     (ky, kx, 1))]),
            Program([Step("phase_tile",     (ky, kx, 2))]),
        ]

    # Dedup by signature
    uniq, seen = [], set()
    for p in cands:
        sig = p.to_tuple()
        if sig not in seen:
            uniq.append(p); seen.add(sig)

    # Score each candidate on all train pairs (pixel acc if shapes match, else 0)
    best_prog, best_mean = None, 0.0
    for prog in uniq:
        accs = []
        for (xi, yi) in train_pairs:
            try:
                pred = interpret_program(prog, xi)
            except Exception:
                accs.append(0.0); continue
            if pred.shape != yi.shape:
                accs.append(0.0)
            else:
                accs.append(float((pred == yi).mean()))
        if accs:
            mean_acc = float(sum(accs) / len(accs))
            if mean_acc > best_mean:
                best_mean, best_prog = mean_acc, prog

    return best_prog, best_mean


def build_synth_context(input_grid, target_grid):
    import numpy as np
    Hin, Win = input_grid.shape
    Hout, Wout = target_grid.shape
    pal_in = sorted(np.unique(input_grid).tolist())
    pal_out = sorted(np.unique(target_grid).tolist())
    return {
        "in_shape": (Hin, Win),
        "out_shape": (Hout, Wout),
        "palette_in": pal_in,
        "palette_out": pal_out,
        "centroid_in": _centroid_nonzero(input_grid),
        "centroid_out": _centroid_nonzero(target_grid),
    }


def _int_space_for(op_name, idx, ctx):
    # idx: index among non-grid integer params
    if op_name == "rot90":
        return [1, 2, 3]
    if op_name == "mirror":
        return [0, 1]  # axis: 0=flipud, 1=fliplr
    if op_name in ("fill", "mask", "outline", "threshold"):
        vals = (ctx.get("palette_out") or []) + (ctx.get("palette_in") or [])
        vals = [v for v in dict.fromkeys(vals) if 0 <= v <= 9]
        return vals or list(range(10))
    if op_name == "resize":
        Hout, Wout = ctx["out_shape"]
        return [(Hout, Wout)]
    if op_name == "tile":
        Hin, Win = ctx["in_shape"]; Hout, Wout = ctx["out_shape"]
        pairs = []
        if Hin and Win and Hout % Hin == 0 and Wout % Win == 0:
            ky, kx = Hout // Hin, Wout // Win
            # ✅ Hard-ban identity tiling
            if ky > 1 or kx > 1:
                pairs.append((ky, kx))
        # Keep a single useful fallback (not identity)
        if not pairs:
            pairs = [(2, 2), (3, 3)]  # choose one or both; neither is (1,1)
        return pairs
    if op_name == "tile_masked":
        Hin, Win = ctx["in_shape"]; Hout, Wout = ctx["out_shape"]
        triples = []
        if Hin and Win and (Hout % Hin == 0) and (Wout % Win == 0):
            ky, kx = Hout // Hin, Wout // Win
            # never enumerate identity
            if not (ky == 1 and kx == 1):
                # v1: only cross(0) and border(1) to keep search tight
                for mode in (0, 1):
                    triples.append((ky, kx, mode))
                # leave diag modes out of enumeration for now
        return triples or []
    if op_name in ("phase_tile", "phase_tile_row", "phase_tile_col"):
        # Return (ky, kx, mode) triples
        Hin, Win = ctx["in_shape"]; Hout, Wout = ctx["out_shape"]
        triples = []
        if Hin and Win and Hout % Hin == 0 and Wout % Win == 0:
            ky, kx = Hout // Hin, Wout // Win
            # Try all three modes
            triples.extend([(ky, kx, 0), (ky, kx, 1), (ky, kx, 2)])
        if not triples:
            # Fallback
            triples = [(2, 2, 0), (3, 3, 0)]
        return triples
    if op_name == "shift":
        base = list(range(-3, 4))
        dy = dx = None
        ci, co = ctx.get("centroid_in"), ctx.get("centroid_out")
        if ci is not None and co is not None:
            dy = int(co[0] - ci[0]); dx = int(co[1] - ci[1])
        if idx == 0:
            return ([dy] + [v for v in base if v != dy]) if dy is not None else base
        if idx == 1:
            return ([dx] + [v for v in base if v != dx]) if dx is not None else base
        return base
    if op_name in ("replacecolor", "swapcolors"):
        pals = (ctx.get("palette_in") or []) + (ctx.get("palette_out") or [])
        pals = [c for c in dict.fromkeys(pals) if 0 <= c <= 9][:6]
        pairs = []
        for i, a in enumerate(pals):
            for b in pals[i+1:]:
                pairs.append((a, b))
        return pairs or [(1, 2), (2, 3), (3, 4)]
    if op_name == "keep_n_largest":
        return [(1,), (2,), (3,)]
    if op_name == "keep_size_range":
        Hout, Wout = ctx["out_shape"]
        A = Hout * Wout if Hout and Wout else 0
        small  = max(1, A // 100)   # ~1%
        medium = max(2, A // 40)    # ~2.5%
        big    = max(3, A // 20)    # ~5%
        return [(small, medium), (medium, big)]
    return list(range(10))


def beam_search_one_pair(
    input_grid: np.ndarray,
    target_grid: np.ndarray,
    phi: np.ndarray,
    settings: SearchSettings,
    logger: Optional[Any] = None
) -> Optional[Program]:
    """
    OCO-guided beam search for a single train pair.
    
    Returns the best program found, or None if time/depth exceeded.
    """
    start_time = time.time()

    # Early-abort tracking (no-shape)
    no_shape_seen = True           # flip to False once any successor matches target shape
    abort_after = settings.max_seconds / 3.0  # early-abort threshold

    # Early-abort tracking (no-improvement)
    last_improve_t = time.time()
    best_cost_seen = 1e9
    stall_window = settings.max_seconds / 3.0  # same scale as no-shape abort

    # Build context for op-specific argument generation
    ctx = build_synth_context(input_grid, target_grid)

    # === Meta-seed discovery ===
    meta = _quick_shape_candidates(input_grid, target_grid)
    seeds = [prog for (acc, prog) in meta]

    # Early-latch: if any seed already strong, prefer it
    best_so_far, best_cost = None, 1e9
    for acc, prog in meta:
        if acc >= 0.80:  # strong shape match
            # try palette immediately; if exact, return
            mapping = _palette_map_from_train_pairs([(input_grid, target_grid)])
            if mapping:
                pred = interpret_program(prog, input_grid)
                p2 = np.array(_apply_palette_map_ll(pred.tolist(), mapping), dtype=np.int32)
                if p2.shape == target_grid.shape and np.array_equal(p2, target_grid):
                    return prog
            # not exact: make the beam expand this branch first
            best_so_far = prog
            best_cost   = 1.0 - acc
            break  # one is enough

    # Initialize beam with scored seeds (plus an empty program fallback)
    beam = []
    visited = set()

    def _seed_cost(p):
        try:
            return compute_cost(p, input_grid, target_grid, phi, settings)
        except Exception:
            return 1e9

    # add deduped seeds
    sig_seen = set()
    for prog in seeds:
        sig = prog.to_tuple()
        if sig in sig_seen: 
            continue
        sig_seen.add(sig)
        beam.append(Candidate(prog, _seed_cost(prog), depth=len(prog)))
        visited.add(sig)

    # always include empty program fallback
    empty_sig = Program([]).to_tuple()
    if empty_sig not in sig_seen:
        beam.append(Candidate(Program([]), cost=1e9, depth=0))
        visited.add(empty_sig)

    # if early-latch found a strong seed, bias its cost so it's explored first
    if best_so_far is not None:
        sig = best_so_far.to_tuple()
        if sig not in sig_seen:
            beam.append(Candidate(best_so_far, best_cost, depth=len(best_so_far)))
            visited.add(sig)

    # prune to beam width right away
    beam.sort()
    beam = beam[:settings.beam_width]

    # Track best solution found
    if best_so_far is None:
        best_so_far = None
        best_cost = 1e9

    # State cache for in-beam gating
    state_cache: Dict[Tuple, np.ndarray] = {}
    def _run(prog: Program) -> np.ndarray:
        sig = prog.to_tuple()
        if sig in state_cache:
            return state_cache[sig]
        out = interpret_program(prog, input_grid)
        state_cache[sig] = out
        return out

    iteration = 0

    while beam:
        iteration += 1

        # Check timeout
        if time.time() - start_time > settings.max_seconds:
            if logger:
                logger.log_iteration(iteration, len(beam), best_cost)
            # Return best-so-far immediately on hard timeout
            if best_so_far is not None:
                return best_so_far
            # Fallback if nothing found
            return Program([Step("resize", (target_grid.shape[0], target_grid.shape[1]))])

        # No-improvement early-abort check
        if time.time() - last_improve_t > stall_window:
            if logger:
                print(f"[early-abort] No cost improvement for {stall_window:.1f}s, returning best")
            # Return best-so-far or resize fallback
            if best_so_far is not None:
                return best_so_far
            # Safe fallback: resize to target shape
            return Program([Step("resize", (target_grid.shape[0], target_grid.shape[1]))])

        # No-shape early-abort check: if ⅓ budget spent with no shape progress
        if time.time() - start_time > abort_after and no_shape_seen:
            if logger:
                print(f"[early-abort] No shape match after {abort_after:.1f}s, returning fallback")
            # Return best-so-far or resize fallback
            if best_so_far is not None:
                return best_so_far
            # Safe fallback: resize to target shape
            return Program([Step("resize", (target_grid.shape[0], target_grid.shape[1]))])

        # Get candidate with lowest cost
        current = beam.pop(0)

        # Track improvement for no-improvement abort
        if current.cost < best_cost_seen - 1e-6:
            best_cost_seen = current.cost
            last_improve_t = time.time()

        # Check if solved
        if current.cost < 0.01:
            best_so_far = current.program
            best_cost = current.cost
            if logger:
                logger.log_iteration(iteration, len(beam), best_cost)
            break

        # Track best
        if current.cost < best_cost:
            best_cost = current.cost
            best_so_far = current.program

        # Check depth limit
        if current.depth >= settings.max_depth:
            continue

        # Compute current state for gating
        try:
            cur_state = _run(current.program)
            # Check if we've seen shape match at this level
            if cur_state.shape == target_grid.shape:
                no_shape_seen = False
        except Exception:
            cur_state = input_grid

        # --- Conditional Smart Refiner (one extra post-shape step) ---
        try:
            if cur_state.shape == target_grid.shape and current.depth < 2:
                acc, _, _ = _current_acc_state(cur_state, target_grid)
                if acc >= 0.60:
                    # 1) alignment-guided shift (best (dy,dx) from existing helper)
                    try:
                        best = _propose_alignment_deltas(cur_state, target_grid, window=3)[:1]
                    except Exception:
                        best = []
                    for (dy, dx) in best:
                        if dy or dx:
                            prog_shift = current.program.copy()
                            prog_shift.steps.append(Step("shift", (int(dy), int(dx))))
                            sig = prog_shift.to_tuple()
                            if sig not in visited:
                                cost = compute_cost(prog_shift, input_grid, target_grid, phi, settings)
                                beam.append(Candidate(prog_shift, cost, current.depth + 1))
                                visited.add(sig)

                    # 2) alternate tile_masked mode (swap cross/border once)
                    if current.program.steps and current.program.steps[-1].op == "tile_masked":
                        ky, kx, m = map(int, current.program.steps[-1].args)
                        if m in (0, 1):
                            alt = 1 - m
                            prog_alt = current.program.copy()
                            prog_alt.steps.append(Step("tile_masked", (ky, kx, alt)))
                            sig = prog_alt.to_tuple()
                            if sig not in visited:
                                cost = compute_cost(prog_alt, input_grid, target_grid, phi, settings)
                                beam.append(Candidate(prog_alt, cost, current.depth + 1))
                                visited.add(sig)

                    # 3) optional mirror when fairly close
                    if acc >= 0.70 and current.program.steps:
                        axis = 1  # horizontal mirror default
                        prog_m = current.program.copy()
                        prog_m.steps.append(Step("mirror", (axis,)))
                        sig = prog_m.to_tuple()
                        if sig not in visited:
                            cost = compute_cost(prog_m, input_grid, target_grid, phi, settings)
                            beam.append(Candidate(prog_m, cost, current.depth + 1))
                            visited.add(sig)

                    # NOTE: we rely on beam pruning to cap to beam_width; refiners add ≤3 branches
        except Exception:
            pass

        # Choose allowed ops based on shape match
        if cur_state.shape == target_grid.shape:
            allowed = COLOR_OPS
        else:
            allowed = SHAPE_OPS

        # Generate successors with gating + O2 ordering
        successors = generate_successors(current.program, ctx, allowed=allowed)

        # Score each successor
        for succ_prog in successors:
            # Skip if visited
            prog_sig = succ_prog.to_tuple()
            if prog_sig in visited:
                continue
            visited.add(prog_sig)

            # Compute cost
            cost = compute_cost(succ_prog, input_grid, target_grid, phi, settings)

            # Check successor output for tie-break, early exit, and shape tracking
            try:
                pred = interpret_program(succ_prog, input_grid)
                
                # Track shape match
                if pred.shape == target_grid.shape:
                    no_shape_seen = False
                
                # Tiny tie-break for tile_masked when shape matches
                if pred.shape == target_grid.shape:
                    last = succ_prog.steps[-1].op if succ_prog.steps else None
                    if last == "tile_masked":
                        cost -= 0.02
                    
                    # Early exit if exact match
                    if np.array_equal(pred, target_grid):
                        return succ_prog
            except Exception:
                pass

            # Add to beam
            new_cand = Candidate(succ_prog, cost, current.depth + 1)
            beam.append(new_cand)

        # Sort beam by cost
        beam.sort()

        # Prune to beam width
        beam = beam[:settings.beam_width]

        # Periodic logging
        if logger and iteration % settings.log_every == 0:
            logger.log_iteration(iteration, len(beam), best_cost)

    if logger:
        logger.log_iteration(iteration, len(beam), best_cost)

    return best_so_far


def generate_successors(prog: Program, ctx, allowed: Optional[Set[str]] = None) -> List[Program]:
    """Generate all valid single-step extensions of a program, filtered & ordered."""
    # Order operations by OP_ORDER_O2, and apply optional whitelist
    ordered = sorted(OP_NAMES_BASIC, key=lambda t: _OP_RANK.get(t[0], 10_000))
    successors = []
    for op_name, arity, param_types in ordered:
        if allowed is not None and op_name not in allowed:
            continue
        arg_combos = enumerate_args(op_name, param_types, ctx)
        for args in arg_combos:
            new_prog = prog.copy()
            new_prog.steps.append(Step(op_name, args))
            successors.append(new_prog)
    return successors


def enumerate_args(op_name: str, param_types: List[str], ctx) -> List[Tuple[Any, ...]]:
    """Enumerate concrete arguments for an operation with context awareness."""
    if not param_types:
        return [()]

    # Convert param_types: first 'grid' is implicit (current state), 
    # subsequent 'grid' become 'grid_ref' for register references
    filtered = []
    seen_grid = False
    for t in param_types:
        if t == "grid":
            if not seen_grid:
                seen_grid = True
                continue  # Skip first grid (current state)
            filtered.append("grid_ref")  # Subsequent grids are references
        else:
            filtered.append(t)
    
    if not filtered:
        return [()]

    # Bundled int-pair/triple ops
    if op_name == "resize":
        return [(h, w) for (h, w) in _int_space_for("resize", 0, ctx)]
    if op_name == "tile":
        return [(v, h) for (v, h) in _int_space_for("tile", 0, ctx)]
    if op_name == "tile_masked":
        return [(ky, kx, mode) for (ky, kx, mode) in _int_space_for("tile_masked", 0, ctx)]
    if op_name == "phase_tile":
        return [(ky, kx, mode) for (ky, kx, mode) in _int_space_for("phase_tile", 0, ctx)]
    if op_name == "phase_tile_row":
        return [(ky, kx, mode) for (ky, kx, mode) in _int_space_for("phase_tile_row", 0, ctx)]
    if op_name == "phase_tile_col":
        return [(ky, kx, mode) for (ky, kx, mode) in _int_space_for("phase_tile_col", 0, ctx)]
    if op_name in ("replacecolor", "swapcolors"):
        return [(a, b) for (a, b) in _int_space_for(op_name, 0, ctx)]
    if op_name == "keep_n_largest":
        return [(n,) for (n,) in _int_space_for("keep_n_largest", 0, ctx)]
    if op_name == "keep_size_range":
        return [(amin, amax) for (amin, amax) in _int_space_for("keep_size_range", 0, ctx)]

    # Generic cartesian product across per-position spaces
    spaces = []
    for i, t in enumerate(filtered):
        if t == "grid_ref":
            spaces.append([REG_PREV, REG_PREV2])
        elif t == "int":
            spaces.append(_int_space_for(op_name, i, ctx))
        else:
            spaces.append([0])

    combos = [()]
    for space in spaces:
        combos = [c + (v,) for c in combos for v in space]
    return combos


# ============================================================================
# ============ controller/modes.py ============
# ============================================================================

@dataclass
class ControllerState:
    """State of the OCO controller."""
    mode: str = "observer"  # observer, navigator, explorer
    rotation_count: int = 0
    last_rotation_cost: float = 1e9


def should_rotate(state: ControllerState, current_cost: float, settings: SearchSettings) -> bool:
    """Decide if controller should rotate to next mode."""
    if settings._disable_rotation:
        return False

    # Rotate if cost isn't improving
    if current_cost >= state.last_rotation_cost * 0.98:
        return True

    # Rotate after 3 attempts in same mode
    if state.rotation_count >= 3:
        return True

    return False


def rotate_mode(state: ControllerState) -> str:
    """Rotate to next controller mode."""
    modes = ["observer", "navigator", "explorer"]
    idx = modes.index(state.mode)
    next_idx = (idx + 1) % len(modes)
    return modes[next_idx]


def apply_mode_bias(settings: SearchSettings, mode: str) -> SearchSettings:
    """Adjust search settings based on controller mode."""
    from dataclasses import replace

    if mode == "observer":
        # Conservative: low beam, high OCO
        return replace(settings, beam_width=64, lambda1=0.40, lambda2=0.30)
    elif mode == "navigator":
        # Balanced: default settings
        return settings
    elif mode == "explorer":
        # Aggressive: high beam, low OCO
        return replace(settings, beam_width=384, lambda1=0.05, lambda2=0.05)
    else:
        return settings


# ============================================================================
# ============ io/tasks.py ============
# ============================================================================

def load_tasks_from_dir(tasks_dir: str) -> List[Tuple[str, dict]]:
    """
    Load ARC tasks from a directory.

    Supports:
      (1) dict-of-tasks: {"id": {"train":[...], "test":[...]}, ...}
      (2) single dict:   {"train":[...], "test":[...]}
      (3) list of dicts: [{"train":...,"test":...}, ...]
      (4) optional wrappers: {"challenges":[...]}, {"training":[...]}, {"evaluation":[...]}, {"test":[...]}
    """
    import glob, json, os
    tasks: List[Tuple[str, dict]] = []

    def normalize(obj, base_id):
        out = []
        # (1) dict-of-tasks
        if isinstance(obj, dict) and not ("train" in obj and "test" in obj):
            ok = False
            for k, v in obj.items():
                if isinstance(v, dict) and "train" in v and "test" in v:
                    out.append((k, {"train": v["train"], "test": v["test"]}))
                    ok = True
            if ok:
                return out
            # wrapped lists
            for bucket in ("challenges", "training", "evaluation", "test"):
                if bucket in obj and isinstance(obj[bucket], list):
                    for i, e in enumerate(obj[bucket]):
                        if isinstance(e, dict) and "train" in e and "test" in e:
                            tid = e.get("task_id") or e.get("id") or f"{base_id}_{bucket}_{i:05d}"
                            out.append((tid, {"train": e["train"], "test": e["test"]}))
                    return out

        # (2) single dict
        if isinstance(obj, dict) and "train" in obj and "test" in obj:
            out.append((base_id, {"train": obj["train"], "test": obj["test"]}))
            return out

        # (3) list of dicts
        if isinstance(obj, list) and obj and isinstance(obj[0], dict):
            for i, e in enumerate(obj):
                if "train" in e and "test" in e:
                    tid = e.get("task_id") or e.get("id") or f"{base_id}_{i:05d}"
                    out.append((tid, {"train": e["train"], "test": e["test"]}))
            return out

        return out

    def load_json(path):
        with open(path, "r") as f:
            return json.load(f)

    # Prefer ARC Prize consolidated files if present
    got_any = False
    for name in ["arc-agi_training_challenges.json",
                 "arc-agi_evaluation_challenges.json",
                 "arc-agi_test_challenges.json"]:
        fp = os.path.join(tasks_dir, name)
        if os.path.exists(fp):
            got_any = True
            obj = load_json(fp)
            tasks.extend(normalize(obj, os.path.splitext(name)[0]))

    # Fallback: any *.json
    if not tasks:
        for fp in sorted(glob.glob(os.path.join(tasks_dir, "*.json"))):
            got_any = True
            try:
                tasks.extend(normalize(load_json(fp), os.path.splitext(os.path.basename(fp))[0]))
            except Exception:
                continue

    if not got_any:
        raise RuntimeError(f"No ARC tasks found under {tasks_dir}.")
    if not tasks:
        raise RuntimeError(f"Found JSON under {tasks_dir} but no (train/test) tasks parsed. Schema mismatch.")
    return tasks


def trains_from_task(task_json: Dict) -> List[Tuple[np.ndarray, np.ndarray]]:
    """Extract training pairs from a task JSON."""
    pairs = []
    for ex in task_json.get("train", []):
        x = np.array(ex["input"], dtype=np.int32)
        y = np.array(ex["output"], dtype=np.int32)
        pairs.append((x, y))
    return pairs


def tests_from_task(task_json: Dict) -> List[np.ndarray]:
    """Extract test inputs from a task JSON."""
    tests = []
    for ex in task_json.get("test", []):
        x = np.array(ex["input"], dtype=np.int32)
        tests.append(x)
    return tests


def write_submission_json(output_path: str, predictions: Dict[str, Any]):
    """Write predictions to submission JSON."""
    with open(output_path, 'w') as f:
        json.dump(predictions, f, indent=2)


# ============================================================================
# ============ io/telemetry.py ============
# ============================================================================

class StepLogger:
    """Logger for telemetry during search."""

    def __init__(self, public_mode: bool = False, jsonl_path: Optional[str] = None, log_every: int = 200):
        self.public_mode = public_mode
        self.jsonl_path = jsonl_path
        self.log_every = log_every
        self.jsonl_file = None

    def __enter__(self):
        if self.jsonl_path:
            self.jsonl_file = open(self.jsonl_path, 'w')
        return self

    def __exit__(self, *args):
        if self.jsonl_file:
            self.jsonl_file.close()

    def log_iteration(self, iteration: int, beam_size: int, best_cost: float):
        """Log iteration progress."""
        if iteration % self.log_every == 0:
            msg = f"Iter {iteration}: beam={beam_size}, cost={best_cost:.4f}"
            print(msg)

        if self.jsonl_file:
            record = {
                "iteration": iteration,
                "beam_size": beam_size,
                "best_cost": best_cost,
            }
            self.jsonl_file.write(json.dumps(record) + "\n")

    def log_task_start(self, task_id: str):
        """Log task start."""
        print(f"\n{'='*60}")
        print(f"Task: {task_id}")
        print(f"{'='*60}")

    def log_task_end(self, task_id: str, success: bool, elapsed: float):
        """Log task end."""
        status = "✓ SOLVED" if success else "✗ UNSOLVED"
        print(f"{status} ({elapsed:.1f}s)")


# ============================================================================
# ============ solver/task_solver.py ============
# ============================================================================

def solve_task(
    task_id: str,
    task_json: Dict,
    settings: SearchSettings,
    logger: Optional[StepLogger] = None
) -> List[Any]:
    """
    Solve a single ARC task.
    
    Returns:
        List of predicted outputs for test inputs.
        - If best_program ends with tile_masked(ky,kx,m) where m in {0,1}:
          Returns list of dicts: [{"attempt_1": grid, "attempt_2": grid_alt}, ...]
          where attempt_2 uses the alternate mode (0↔1 swap)
        - Otherwise: Returns list of grids (np.ndarray format)
    """
    if logger:
        logger.log_task_start(task_id)

    start_time = time.time()

    # Extract training pairs and test inputs
    train_pairs = trains_from_task(task_json)
    test_inputs = tests_from_task(task_json)

    if not train_pairs or not test_inputs:
        if logger:
            logger.log_task_end(task_id, False, time.time() - start_time)
        return [np.array([[0]]) for _ in test_inputs]

    # Compute task features and φ
    features = task_features(train_pairs)
    phi = compute_phi(features)

    # --- Consensus one-step sweep (fast path) ---
    cons_prog, cons_mean = _consensus_one_step_candidate(train_pairs)

    # Learn program from training pairs
    best_program = None
    best_avg_cost = 1e9

    controller = ControllerState(mode="observer")

    # If consensus is strong, accept it and let finishers polish
    if cons_prog is not None and cons_mean >= 0.80:
        best_program = cons_prog
        best_avg_cost = 1.0 - cons_mean
    else:
        # Standard beam search on each training pair
        for pair_idx, (x, y) in enumerate(train_pairs):
            # Apply controller mode bias
            mode_settings = apply_mode_bias(settings, controller.mode)

            # Search for program
            prog = beam_search_one_pair(x, y, phi, mode_settings, logger)

            if prog is None:
                continue

            # Evaluate on all training pairs
            costs = []
            for (x_eval, y_eval) in train_pairs:
                cost = compute_cost(prog, x_eval, y_eval, phi, settings)
                costs.append(cost)

            avg_cost = np.mean(costs)

            if avg_cost < best_avg_cost:
                best_avg_cost = avg_cost
                best_program = prog

            # Check if should rotate controller
            if should_rotate(controller, avg_cost, settings):
                controller.mode = rotate_mode(controller)
                controller.rotation_count = 0
            else:
                controller.rotation_count += 1

            controller.last_rotation_cost = avg_cost

            # Early stop if perfect
            if best_avg_cost < 0.01:
                break

    # --- Micro-refinement for tile_masked consensus (optional shift nudge) ---
    if best_program is not None and len(best_program.steps) == 1:
        step = best_program.steps[0]
        if step.op == "tile_masked" and len(step.args) >= 3:
            # Try a single 'shift' around small deltas to catch off-by-one placement
            candidates = []
            for dy in (-1, 0, 1):
                for dx in (-1, 0, 1):
                    if dy == 0 and dx == 0: 
                        continue
                    prog_shift = Program(best_program.steps + [Step("shift", (dy, dx))])
                    candidates.append(prog_shift)

            # Evaluate train-average cost and keep if better
            def _avg_cost(p):
                cs = []
                for (xi, yi) in train_pairs:
                    try:
                        cs.append(compute_cost(p, xi, yi, phi, settings))
                    except Exception:
                        cs.append(1e9)
                return float(sum(cs) / len(cs)) if cs else 1e9

            if candidates:
                base_avg = _avg_cost(best_program)
                best_cand = min(candidates, key=_avg_cost, default=None)
                if best_cand is not None:
                    cand_avg = _avg_cost(best_cand)
                    if cand_avg + 1e-9 < base_avg:
                        best_program = best_cand
                        best_avg_cost = cand_avg

    # --- Alignment finisher (learn a single task-level shift) ---
    align = _learn_task_alignment(best_program, train_pairs, phi, settings) if best_program is not None else None
    if align is not None:
        dy, dx = align
        prog_shift = Program([Step(s.op, s.args) for s in best_program.steps] + [Step("shift", (dy, dx))])
        # accept shift if it improves average train cost
        avg_best  = float(np.mean([compute_cost(best_program, xi, yi, phi, settings) for (xi, yi) in train_pairs]))
        avg_shift = float(np.mean([compute_cost(prog_shift,   xi, yi, phi, settings) for (xi, yi) in train_pairs]))
        if avg_shift + 1e-9 < avg_best:
            best_program = prog_shift

    # --- Blockwise dominant-color finisher (sparse tiling) ---
    # Engage only when output is an integer multiple of input with small k
    learned_blockwise_proj = None
    try:
        x0, y0 = train_pairs[0]
        div = _divisible_shape(x0.shape, y0.shape) if (x0 is not None and y0 is not None) else None
    except Exception:
        div = None

    if div is not None and best_program is not None:
        ky, kx = div
        if ky in (3, 5) and kx in (3, 5):
            D = _learn_blockwise_projection(train_pairs, ky, kx)
            if D is not None:
                # Accept the projection only if it lowers or matches avg training cost when applied to predictions
                # (Compute average cost over train pairs with projection applied)
                def _avg_cost_with_projection(prog):
                    cs = []
                    for (xi, yi) in train_pairs:
                        try:
                            pi = interpret_program(prog, xi)
                            if pi.shape == yi.shape:
                                pi = _apply_blockwise_projection(pi, ky, kx, D)
                            # compute cost of fixed prediction vs target
                            cs.append(compute_cost(Program([]), pi, yi, phi, settings))
                        except Exception:
                            cs.append(1e9)
                    return float(np.mean(cs)) if cs else 1e9

                avg_cur = float(np.mean([compute_cost(best_program, xi, yi, phi, settings) for (xi, yi) in train_pairs]))
                avg_proj = _avg_cost_with_projection(best_program)
                if avg_proj + 1e-9 <= avg_cur:
                    learned_blockwise_proj = (ky, kx, D)

    # --- Block-mask finisher (sparse tiling) before palette ---
    # Learn a kxk mask only if all train shapes are consistent multiples of inputs
    try:
        # use the first train pair to infer divisibility
        x0, y0 = train_pairs[0]
        div = _divisible_shape(x0.shape, y0.shape) if (x0 is not None and y0 is not None) else None
    except Exception:
        div = None

    if div is not None and best_program is not None:
        ky, kx = div
        # small k only (keep it safe): 3 or 5
        if ky in (3,5) and kx in (3,5):
            M = _learn_block_mask(train_pairs, ky, kx)
            if M is not None:
                # Synthesize a program variant that applies the learned mask at prediction time
                # We do this as a *post-step* transform in training evaluation space:
                def _masked_eval_cost(prog):
                    # avg cost over train pairs with masking applied
                    cs = []
                    for (xi, yi) in train_pairs:
                        try:
                            pi = interpret_program(prog, xi)
                            # only mask if shapes are compatible
                            if pi.shape == yi.shape and _divisible_shape(xi.shape, yi.shape) == (ky, kx):
                                pi = _apply_block_mask(pi, ky, kx, M)
                            cs.append(compute_cost(Program([]), pi, yi, phi, settings))  # cost of fixed prediction vs target
                        except Exception:
                            cs.append(1e9)
                    return float(np.mean(cs)) if cs else 1e9

                avg_cur = _masked_eval_cost(best_program)  # masking applied to current program's outputs
                # Try appending an explicit mask step at inference time by wrapping predictions later;
                # since we don't have a DSL op for masking, we accept as a finisher iff it lowers cost.
                if avg_cur + 1e-9 < float(np.mean([compute_cost(best_program, xi, yi, phi, settings) for (xi, yi) in train_pairs])):
                    # Store learned mask for use on test predictions below
                    learned_block_mask = (ky, kx, M)
                else:
                    learned_block_mask = None
            else:
                learned_block_mask = None
        else:
            learned_block_mask = None
    else:
        learned_block_mask = None

    # Attach palette finisher - try appending palette remapping to improve accuracy
    if best_program is not None:
        mapping = _palette_map_from_train_pairs(train_pairs)
        if mapping:
            prog2 = Program([Step(s.op, s.args) for s in best_program.steps])
            for src, dst in mapping.items():
                prog2.steps.append(Step("replacecolor", (int(src), int(dst))))
            avg_best = np.mean([compute_cost(best_program, xi, yi, phi, settings) for (xi, yi) in train_pairs])
            avg_p2 = np.mean([compute_cost(prog2, xi, yi, phi, settings) for (xi, yi) in train_pairs])
            if avg_p2 < avg_best:
                best_program = prog2

    # Apply best program to test inputs
    # Learn palette map once from training pairs (used for TRAIN evaluation only)
    palette_map = _palette_map_from_train_pairs(train_pairs)
    
    # Validate palette safety on training pairs (for TRAIN-FIT logic)
    palette_is_safe = False
    if palette_map and best_program is not None:
        safe_count = 0
        for (xi, yi) in train_pairs:
            try:
                pred_train = interpret_program(best_program, xi)
                if pred_train.shape == yi.shape:
                    before = float((pred_train == yi).mean())
                    mapped = np.array(_apply_palette_map_ll(pred_train.tolist(), palette_map), dtype=np.int32)
                    after = float((mapped == yi).mean())
                    if after >= before:
                        safe_count += 1
            except Exception:
                continue
        # Palette is safe if it helps or maintains accuracy on all training pairs
        palette_is_safe = (safe_count == len(train_pairs))
    
    # Check if we should generate two attempts using truncated shape-base
    # Truncate program at last tile_masked to get pure shape transformation
    base_steps, ky, kx, m = _truncate_at_last_tile_masked(best_program)
    emit_two_attempts = (base_steps is not None)
    
    # Debug telemetry
    if logger and emit_two_attempts:
        print(f"[two-attempts] tile_masked(ky={ky}, kx={kx}) found, using truncated shape-base (no finishers)")
    
    predictions = []
    
    # TWO-ATTEMPTS MODE: Use truncated shape-base programs, no finishers on TEST
    if emit_two_attempts:
        # Build truncated programs: attempt_1 = original mode, attempt_2 = swapped mode
        prog1 = Program(list(base_steps))  # keep original mode m
        
        # Build alternate with swapped mode
        alt_mode = 1 - m
        alt_steps = list(base_steps)
        alt_steps[-1] = Step("tile_masked", (ky, kx, alt_mode))
        prog2 = Program(alt_steps)
        
        for test_x in test_inputs:
            # Attempt 1: Original mode (raw output)
            try:
                pred1 = interpret_program(prog1, test_x)
            except Exception:
                pred1 = test_x.copy()
            
            # Attempt 2: Swapped mode (raw output)
            try:
                pred2 = interpret_program(prog2, test_x)
            except Exception:
                pred2 = pred1  # Fallback to attempt_1
            
            # Return two-attempts dict (raw outputs, no finishers applied)
            predictions.append({"attempt_1": pred1.tolist(), "attempt_2": pred2.tolist()})
    
    # SINGLE-ATTEMPT MODE: Apply finishers on TEST
    else:
        for test_x in test_inputs:
            if best_program is None:
                pred1 = test_x.copy()
            else:
                try:
                    pred1 = interpret_program(best_program, test_x)
                except Exception:
                    pred1 = test_x.copy()

            # Apply finishers to single-attempt predictions
            # Block-mask finisher
            if 'learned_block_mask' in locals() and learned_block_mask is not None:
                ky, kx, M = learned_block_mask
                div = _divisible_shape(test_x.shape, pred1.shape)
                if div == (ky, kx):
                    pred1 = _apply_block_mask(pred1, ky, kx, M)

            # Blockwise color projection finisher
            if 'learned_blockwise_proj' in locals() and learned_blockwise_proj is not None:
                ky, kx, D = learned_blockwise_proj
                div = _divisible_shape(test_x.shape, pred1.shape)
                if div == (ky, kx):
                    pred1 = _apply_blockwise_projection(pred1, ky, kx, D)

            # Single-output mode
            predictions.append(pred1)

    elapsed = time.time() - start_time
    success = best_avg_cost < 0.01

    if logger:
        logger.log_task_end(task_id, success, elapsed)

    return predictions


# ============================================================================
# ============ solver/batch.py ============
# ============================================================================

def run_dir(
    tasks_dir: str,
    settings: SearchSettings,
    max_tasks: Optional[int] = None,
    logger: Optional[StepLogger] = None
) -> Dict[str, List[Any]]:
    """
    Run solver on all tasks in a directory.
    
    Returns:
        Dict mapping task_id -> list of predicted grids (or dicts if two_attempts mode)
    """
    results = {}

    tasks = list(load_tasks_from_dir(tasks_dir))

    if max_tasks:
        tasks = tasks[:max_tasks]

    for task_id, task_json in tasks:
        preds = solve_task(task_id, task_json, settings, logger)

        # Convert to list of lists for JSON serialization
        # Handle both single-output and two-attempts formats
        if preds and isinstance(preds[0], dict):
            # Two-attempts mode: already in dict format
            preds_ll = preds
        else:
            # Single-output mode: convert np.ndarray to list
            preds_ll = [pred.tolist() if hasattr(pred, 'tolist') else pred for pred in preds]

        results[task_id] = preds_ll

    return results


# ============================================================================
# ============ eval/metrics.py ============
# ============================================================================

def exact_match(pred: np.ndarray, truth: np.ndarray) -> bool:
    """Check if prediction exactly matches ground truth."""
    if pred.shape != truth.shape:
        return False
    return np.array_equal(pred, truth)


def pixel_accuracy(pred: np.ndarray, truth: np.ndarray) -> float:
    """Compute pixel-wise accuracy."""
    if pred.shape != truth.shape:
        return 0.0
    correct = np.sum(pred == truth)
    total = truth.size
    return correct / max(total, 1)


def solve_rate(preds: List[np.ndarray], truths: List[np.ndarray]) -> float:
    """Compute fraction of tasks solved."""
    if not preds or not truths:
        return 0.0
    solved = sum(exact_match(p, t) for p, t in zip(preds, truths))
    return solved / len(truths)


def evaluate_task(preds: List[np.ndarray], 
                 truths: List[np.ndarray]) -> Dict[str, float]:
    """Comprehensive evaluation metrics for one task."""
    if not preds or not truths:
        return {"exact_match": 0.0, "pixel_accuracy": 0.0, "solve_rate": 0.0}
    
    exact = all(exact_match(p, t) for p, t in zip(preds, truths))
    pixel_acc = np.mean([pixel_accuracy(p, t) for p, t in zip(preds, truths)])
    solve = solve_rate(preds, truths)
    
    return {
        "exact_match": float(exact),
        "pixel_accuracy": float(pixel_acc),
        "solve_rate": float(solve),
    }


# ============================================================================
# ============ eval/ablations.py ============
# ============================================================================

def without_oco(settings):
    """Disable OCO: no tension penalties."""
    from dataclasses import replace
    s = replace(settings)
    s.lambda1 = 0.0
    s.lambda2 = 0.0
    return s


def without_slice_guard(settings):
    """Disable slice gating."""
    from dataclasses import replace
    s = replace(settings)
    s.slice_guard_thresh = 1e9
    s.allow_offslice_early = True
    return s


def without_rotation(settings):
    """Disable controller rotations."""
    from dataclasses import replace
    s = replace(settings)
    s._disable_rotation = True
    return s


def get_ablation_config(name: str, base_settings):
    """Get settings for ablation experiment."""
    ablations = {
        "no_oco": without_oco,
        "no_slice": without_slice_guard,
        "no_rotation": without_rotation,
    }
    
    if name == "baseline":
        return base_settings
    elif name in ablations:
        return ablations[name](base_settings)
    else:
        raise ValueError(f"Unknown ablation: {name}")


# ============================================================================
# ============ Recursion-Safe Wrapper Pattern ============
# ============================================================================
# 
# CRITICAL: If you want to monkey-patch beam_search_one_pair or generate_successors
# with custom gating logic, use this pattern to avoid RecursionError.
#
# The problem: If wrapper A calls the patched function, which is now wrapper B,
# which calls wrapper A again → infinite recursion.
#
# The solution: Save the base function ONCE and always call the base.
#
# Example for beam_search_one_pair:
#
# import arc_one
#
# # Step 1: Save base exactly once (before any patching)
# if not hasattr(arc_one, "_BASE_BEAM"):
#     arc_one._BASE_BEAM = arc_one.beam_search_one_pair
#
# # Step 2: Define your wrapper
# def custom_beam_wrapper(input_grid, target_grid, phi, settings, logger):
#     """Your custom shape-then-color gating logic."""
#     # ... preprocessing ...
#     
#     # ALWAYS call the saved base, never the patched version
#     result = arc_one._BASE_BEAM(input_grid, target_grid, phi, settings, logger)
#     
#     # ... postprocessing ...
#     return result
#
# # Step 3: Guard against double-wrapping
# if getattr(arc_one.beam_search_one_pair, "__name__", "") != "custom_beam_wrapper":
#     arc_one.beam_search_one_pair = custom_beam_wrapper
#
# Same pattern for generate_successors:
# - Save as _BASE_GENERATE_SUCCESSORS
# - Always call _BASE_GENERATE_SUCCESSORS inside wrapper
# - Use try/finally to restore if needed
#
# Example with try/finally cleanup:
#
# if not hasattr(arc_one, "_BASE_GENERATE"):
#     arc_one._BASE_GENERATE = arc_one.generate_successors
#
# def custom_generate(prog, ctx):
#     # Filter or modify successor generation
#     successors = arc_one._BASE_GENERATE(prog, ctx)
#     return [s for s in successors if some_condition(s)]
#
# # Temporarily patch
# old_generate = arc_one.generate_successors
# arc_one.generate_successors = custom_generate
# try:
#     # ... run solver ...
#     results = run_dir(...)
# finally:
#     # Restore original
#     arc_one.generate_successors = old_generate
#
# ============================================================================


# ============================================================================
# ============ cli/main.py ============
# ============================================================================

def main():
    """Command-line interface for ARC-ONE solver."""
    parser = argparse.ArgumentParser(
        description="ARC-ONE: Octonionic Control Overlay Solver",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # OCO-guided two attempts (recommended)
  python arc_one.py --tasks_dir ./arc_tasks --two_attempts --out submission.json
  
  # Two attempts with manual strategy (e.g., horizontal flip)
  python arc_one.py --tasks_dir ./arc_tasks --two_attempts --attempt2_strategy flipH
  
  # Legacy single attempt (no change to file structure)
  python arc_one.py --tasks_dir ./arc_tasks --out submission.json
  
  # Quick test with validation
  python arc_one.py --tasks_dir ./arc_tasks --two_attempts --max_tasks 5
  
  # Ablation study
  python arc_one.py --tasks_dir ./arc_tasks --ablation no_oco --out no_oco.json
  
  # With telemetry logging
  python arc_one.py --tasks_dir ./arc_tasks --jsonl telemetry.jsonl --two_attempts
        """
    )
    
    # I/O
    parser.add_argument("--tasks_dir", required=True,
                       help="Directory containing task JSON files")
    parser.add_argument("--out", default="submission.json",
                       help="Output submission file")
    parser.add_argument("--two_attempts", action="store_true",
                       help="Output two attempts per test input (ARC 2025 schema).")
    parser.add_argument("--attempt2_strategy", type=str, default="oco_auto",
                       choices=["oco_auto","auto","rotate90","rot180","flipH","flipV","palette_swap","center","toward_input"],
                       help="How to generate attempt_2.")
    
    # Limits
    parser.add_argument("--max_tasks", type=int, default=None,
                       help="Max number of tasks to solve")
    
    # Search settings
    parser.add_argument("--beam", type=int, default=128,
                       help="Beam width (default: 128)")
    parser.add_argument("--depth", type=int, default=10,
                       help="Max program depth (default: 10)")
    parser.add_argument("--seconds", type=float, default=3.0,
                       help="Max seconds per task (default: 3.0)")
    
    # OCO settings
    parser.add_argument("--lambda_len", type=float, default=0.20,
                       help="Length penalty weight (default: 0.20)")
    parser.add_argument("--lambda1", type=float, default=0.30,
                       help="Program tension weight (default: 0.30)")
    parser.add_argument("--lambda2", type=float, default=0.20,
                       help="Slice tension weight (default: 0.20)")
    
    # Ablations
    parser.add_argument("--ablation", type=str, default=None,
                       choices=["no_oco", "no_slice", "no_rotation"],
                       help="Run ablation experiment")
    
    # Logging
    parser.add_argument("--public_mode", action="store_true",
                       help="Use public-facing terminology in logs")
    parser.add_argument("--log_every", type=int, default=200,
                       help="Log every N iterations (default: 200)")
    parser.add_argument("--jsonl", type=str, default=None,
                       help="Path for JSONL telemetry log")
    
    # Determinism
    parser.add_argument("--seed", type=int, default=1337,
                       help="Random seed for determinism (default: 1337)")
    
    args = parser.parse_args()

    if not _NUMPY_AVAILABLE:
        print(f"ERROR: {_NUMPY_IMPORT_ERROR}", file=sys.stderr)
        sys.exit(1)

    # Banner
    print("=" * 80)
    print("ARC-ONE: Octonionic Control Overlay for Abstract Reasoning")
    print("=" * 80)
    print(f"Configuration:")
    print(f"  Beam width: {args.beam}")
    print(f"  Max depth: {args.depth}")
    print(f"  Max seconds: {args.seconds}")
    print(f"  OCO penalties: λ_len={args.lambda_len}, λ1={args.lambda1}, λ2={args.lambda2}")
    if args.two_attempts:
        print(f"  Two attempts mode: {args.attempt2_strategy}")
    if args.ablation:
        print(f"  Ablation: {args.ablation}")
    print("=" * 80)
    
    # Build settings
    settings = SearchSettings(
        beam_width=args.beam,
        max_depth=args.depth,
        max_seconds=args.seconds,
        lambda_len=args.lambda_len,
        lambda1=args.lambda1,
        lambda2=args.lambda2,
        public_mode=args.public_mode,
        log_every=args.log_every,
        seed=args.seed,
    )
    
    # Apply ablation if specified
    if args.ablation:
        print(f"\n⚠️  Running ablation: {args.ablation}\n")
        settings = get_ablation_config(args.ablation, settings)
    
    # Resolve tasks dir (robust against nested competition paths in Kaggle)
    resolved_tasks_dir = args.tasks_dir  # simplified: supports ARC Prize 2025 layout directly
    print(f"🔍 Using ARC tasks at: {resolved_tasks_dir}\n")
    
    # Run solver
    with StepLogger(args.public_mode, args.jsonl, args.log_every) as logger:
        results = run_dir(resolved_tasks_dir, settings, args.max_tasks, logger)
    
    # Build final predictions object (single- or two-attempts)
    if args.two_attempts:
        predictions = _two_attempts_from_results(
            results,
            tasks_dir=resolved_tasks_dir,
            strategy=args.attempt2_strategy
        )
    else:
        predictions = results  # legacy single-output-per-test schema
    
    # Write submission (function dumps whatever dict we pass)
    write_submission_json(args.out, predictions)
    
    # Summary
    print("\n" + "=" * 80)
    print(f"✅ COMPLETE!")
    print("=" * 80)
    print(f"  Output: {args.out}")
    print(f"  Tasks: {len(results)}")
    if args.two_attempts:
        print(f"  Format: Two attempts ({args.attempt2_strategy})")
    if args.jsonl:
        print(f"  Telemetry: {args.jsonl}")
    print("=" * 80)


if __name__ == "__main__":
    # Check for test mode
    if os.environ.get("ARC_ONE_RUN_TESTS") == "1":
        print("Test mode not included in this artifact - run tests separately")
        print("To use the solver, run: python arc_one.py --tasks_dir <path>")
    else:
        main()
