from __future__ import annotations
# ==================================================================================
# ARC-ONE: Octonionic Control Overlay for Abstract Reasoning Corpus
# ==================================================================================
#
# Version: v2.9.3 — HFP ρ Wiring & Staged Early-Exit
#
# Enhancements:
# - v2.9.3: Real C/ρ tracker wired into settings (_hfp_prevC/_hfp_rho/_hfp_rho_smoothed/_hfp_ready),
#           enabling ρ-adaptive abort windows and bounded A↔B debate.
#           Belief pass-through for TEST predictions (align_dy_dx, block_mask, block_proj) so alternates are targeted.
#           Candidate-pool + IoU diversity guard for non-tilers (useful disagreement > 0).
#           Palette guard fixed (structure-safe mapping via _palette_map_from_train_pairs).
#           attempt2_strategy honored (seeded candidate in alternates list).
#           Staged runner now keeps best-so-far and early-exits when diversity achieved.
#           Duplicate topology-hint block removed; telemetry cleaned.
# - v2.9.2: Two-attempt debate with diversity guard, ρ-adaptive aborts, palette safety, and stage-aware scheduling.

__version__ = "v2.9.3"

last_best_program = None

_NUMPY_IMPORT_ERROR = (
    "ARC-ONE requires the 'numpy' package. Install it with `pip install numpy` "
    "or ensure it is available in your execution environment before running the solver."
)

try:
    import numpy as np  # type: ignore
    _NUMPY_AVAILABLE = True
except ModuleNotFoundError:  # pragma: no cover - depends on environment
    _NUMPY_AVAILABLE = False

    class _MissingNumpy:
        """Proxy that surfaces a helpful error message when numpy is absent."""

        class ndarray:  # minimal stand-in for isinstance checks
            pass

        def __getattr__(self, name):
            raise ModuleNotFoundError(_NUMPY_IMPORT_ERROR)

    np = _MissingNumpy()  # type: ignore
from dataclasses import dataclass, field, replace
from typing import Dict, List, Tuple, Optional, Any, Set, Iterable
from collections import deque, defaultdict
import itertools
import math
import time
import statistics as _stats
import json
import os
import glob
import sys
import argparse
import tempfile
import shutil
import hashlib
from scipy.ndimage import label as _cc_label


# ==================================================================================
# REGISTER TOKENS (for multi-grid operations)
# ==================================================================================

REG_PREV  = "__G_MINUS_1__"
REG_PREV2 = "__G_MINUS_2__"


def resolve_grid_arg(arg, states):
    """Resolve grid references to actual grids from state history."""
    if arg == REG_PREV:  
        return states[-1] if len(states) >= 1 else None
    if arg == REG_PREV2: 
        return states[-2] if len(states) >= 2 else None
    return arg


# ==================================================================================
# OCO-GUIDED TWO-ATTEMPTS HELPERS (Attempt 2 generation + robust task dir resolve)
# ==================================================================================

def _symmetry_flags_np(grid_ll):
    g = np.array(grid_ll)
    H = int(np.array_equal(g, np.fliplr(g)))
    V = int(np.array_equal(g, np.flipud(g)))
    R = int(np.array_equal(g, np.rot90(g, 2)))
    return H, V, R


def _center_on_mass_np(grid_ll):
    """
    Center non-zero mass of a predicted grid. Guaranteed no-crash:
    - No-op if grid is not 2D
    - No-op if mask empty or indices weird
    """
    g = np.asarray(grid_ll)
    # Guard: only operate on 2-D grids
    if g.ndim != 2:
        return g.tolist() if hasattr(g, "tolist") else grid_ll

    mask = (g != 0)
    if not mask.any():
        return g.tolist()

    ys, xs = np.where(mask)
    # Extra guard: require 1D coordinate arrays
    if ys.ndim != 1 or xs.ndim != 1 or ys.size == 0 or xs.size == 0:
        return g.tolist()

    cy, cx = int(np.round(ys.mean())), int(np.round(xs.mean()))
    H, W = g.shape
    dy, dx = H // 2 - cy, W // 2 - cx
    g2 = np.roll(np.roll(g, dy, axis=0), dx, axis=1)
    return g2.tolist()


def _rot90_np(grid_ll):
    g = np.asarray(grid_ll)
    if g.ndim != 2:
        return g.tolist() if hasattr(g, "tolist") else g
    return np.rot90(g, 1).tolist()


def _translate_toward_input_centroid_np(pred_ll, x_in_ll):
    g = np.array(pred_ll)
    xin = np.array(x_in_ll)
    if not (g != 0).any() or not (xin != 0).any():
        return g.tolist()
    yP, xP = np.where(g != 0)
    yX, xX = np.where(xin != 0)
    cyP, cxP = int(np.round(yP.mean())), int(np.round(xP.mean()))
    cyX, cxX = int(np.round(yX.mean())), int(np.round(xX.mean()))
    ty, tx = (cyX - cyP), (cxX - cxP)
    out = np.zeros_like(g)
    H, W = g.shape
    y_src0 = max(0, -ty)
    y_dst0 = max(0, ty)
    x_src0 = max(0, -tx)
    x_dst0 = max(0, tx)
    h = min(H - y_dst0, H - y_src0)
    w = min(W - x_dst0, W - x_src0)
    if h > 0 and w > 0:
        out[y_dst0:y_dst0 + h, x_dst0:x_dst0 + w] = g[y_src0:y_src0 + h, x_src0:x_src0 + w]
    return out.tolist()


def _palette_confusion_from_train_pairs(train_pairs, max_colors=5):
    """Build confusion matrix for optimal bijection palette mapping."""
    if not train_pairs:
        return None
    in_colors = []
    out_colors = []
    counts = defaultdict(int)
    for x, y in train_pairs:
        x = np.asarray(x); y = np.asarray(y)
        h = min(x.shape[0], y.shape[0]); w = min(x.shape[1], y.shape[1])
        if h <= 0 or w <= 0: 
            continue
        xs = x[:h, :w]; ys = y[:h, :w]
        for a, b in zip(xs.ravel(), ys.ravel()):
            counts[(int(a), int(b))] += 1
            in_colors.append(int(a)); out_colors.append(int(b))
    if not counts:
        return None
    in_set  = sorted(set(in_colors))
    out_set = sorted(set(out_colors))
    if len(in_set) > max_colors or len(out_set) > max_colors:
        return (in_set, out_set, None)  # will fallback
    # Build confusion matrix
    idx_in  = {c:i for i,c in enumerate(in_set)}
    idx_out = {c:j for j,c in enumerate(out_set)}
    C = np.zeros((len(in_set), len(out_set)), dtype=np.int32)
    for (a,b), cnt in counts.items():
        if a in idx_in and b in idx_out:
            C[idx_in[a], idx_out[b]] += cnt
    return (in_set, out_set, C)


def _optimal_bijection_mapping(train_pairs, max_colors=5):
    """Find optimal bijection using permutation brute-force for small palettes."""
    res = _palette_confusion_from_train_pairs(train_pairs, max_colors=max_colors)
    if res is None:
        return None
    in_set, out_set, C = res
    # Require balanced palettes and a valid confusion matrix
    if C is None or len(in_set) == 0 or len(in_set) != len(out_set) or len(in_set) > max_colors:
        return None
    n = len(in_set)
    best_score = -1
    best_perm = None
    for perm in itertools.permutations(range(len(out_set))):
        score = sum(C[i, perm[i]] for i in range(n))
        if score > best_score:
            best_score = score
            best_perm = perm
    if best_perm is None:
        return None
    mapping = { in_set[i]: out_set[best_perm[i]] for i in range(len(in_set)) }
    # Leave background 0 unmapped if it maps to itself only
    if mapping.get(0, None) == 0:
        mapping.pop(0, None)
    return mapping


def _majority_palette_mapping(train_pairs):
    """Majority heuristic palette mapping (fallback for unbalanced palettes)."""
    if not train_pairs:
        return {}
    tally = {}
    for x, y in train_pairs:
        x = np.asarray(x); y = np.asarray(y)
        h = min(x.shape[0], y.shape[0]); w = min(x.shape[1], y.shape[1])
        if h <= 0 or w <= 0: 
            continue
        xs = x[:h, :w]; ys = y[:h, :w]
        for c in np.unique(xs):
            mask = (xs == c)
            if mask.any():
                targets = ys[mask]
                if targets.size:
                    vals, cnts = np.unique(targets, return_counts=True)
                    target = int(vals[np.argmax(cnts)])
                    tally.setdefault(int(c), {}).setdefault(target, 0)
                    tally[int(c)][target] += int(cnts.max())
    if not tally:
        return {}
    mapping = {c: max(v.items(), key=lambda kv: kv[1])[0] for c, v in tally.items()}
    if mapping.get(0) == 0:
        mapping.pop(0, None)
    return mapping


def _palette_map_from_train_pairs(train_pairs):
    """Try optimal bijection first (balanced small palettes), else majority."""
    m = _optimal_bijection_mapping(train_pairs, max_colors=5)
    if m: 
        return m
    return _majority_palette_mapping(train_pairs)


def _apply_palette_map_ll(grid_ll, mapping):
    if not mapping:
        return grid_ll
    g = np.array(grid_ll, copy=True)
    for src, dst in mapping.items():
        g[g == src] = dst
    return g.tolist()


def _apply_palette_map_safe(pred: np.ndarray, target: np.ndarray, mapping: dict) -> np.ndarray:
    """
    Pareto-safe palette application: only apply mapping if it doesn't decrease accuracy.
    Returns mapped version if accuracy >= before, else returns original pred.
    """
    if not mapping or pred.shape != target.shape:
        return pred
    before = float((pred == target).mean())
    mapped = np.array(_apply_palette_map_ll(pred.tolist(), mapping), dtype=np.int32)
    after  = float((mapped == target).mean())
    return mapped if after >= before else pred


def _alt_mode_program(prog):
    """If prog ends with tile_masked(ky,kx,m) where m∈{0,1}, return
    a new Program with the same steps except the last op uses (1-m).
    Otherwise return None.
    """
    if not prog or not prog.steps:
        return None
    last = prog.steps[-1]
    if last.op != "tile_masked":
        return None
    ky, kx, m = map(int, last.args)
    if m not in (0, 1):
        return None
    alt = 1 - m
    new_steps = list(prog.steps[:-1]) + [Step("tile_masked", (ky, kx, alt))]
    return Program(new_steps)


def _alt_mode_program_scan(prog):  # type: ignore[override]
    """
    Return an alternate Program by swapping the last tile_masked(ky,kx,m) where m∈{0,1},
    even if color/shift steps follow it. Keep trailing steps intact. If not found, return None.
    """
    if prog is None or not prog.steps:
        return None
    steps = list(prog.steps)
    for idx in range(len(steps)-1, -1, -1):
        s = steps[idx]
        if s.op == "tile_masked":
            ky, kx, m = map(int, s.args)
            if m in (0,1):
                alt = 1 - m
                alt_steps = steps[:]
                alt_steps[idx] = Step("tile_masked", (ky, kx, alt))
                return Program(alt_steps)
            break
    return None


def _truncate_at_last_tile_masked(prog):
    """Return (base_steps, ky, kx, m) by cutting the program after the last tile_masked(ky,kx,m) where m∈{0,1}.
    If not found, return (None, None, None, None)."""
    if prog is None or not prog.steps:
        return None, None, None, None
    steps = list(prog.steps)
    for idx in range(len(steps)-1, -1, -1):
        s = steps[idx]
        if s.op == "tile_masked":
            ky, kx, m = map(int, s.args)
            if m in (0, 1):
                return steps[:idx+1], ky, kx, m
            break
    return None, None, None, None


def _attempt2_from_strategy(att1_ll, strategy, phi_arr, train_pairs, test_input_ll):
    g = np.array(att1_ll)
    fam_hint = None
    if phi_arr is not None:
        try:
            fam_hint = phi_to_family(np.asarray(phi_arr))
        except Exception:
            fam_hint = None

    if strategy == "rotate90":
        result = np.rot90(g, 1).tolist()
    elif strategy == "rot180":
        result = np.rot90(g, 2).tolist()
    elif strategy == "flipH":
        result = np.fliplr(g).tolist()
    elif strategy == "flipV":
        result = np.flipud(g).tolist()
    elif strategy == "center":
        result = _center_on_mass_np(att1_ll)
    elif strategy == "toward_input":
        if test_input_ll is None:
            result = att1_ll
        else:
            result = _translate_toward_input_centroid_np(att1_ll, test_input_ll)
    elif strategy == "palette_swap":
        mapping = _palette_map_from_train_pairs(train_pairs or [])
        result = _apply_palette_map_ll(att1_ll, mapping)
    elif strategy == "auto":
        if test_input_ll is not None:
            Hs, Vs, Rs = _symmetry_flags_np(test_input_ll)
            if Hs and not Vs:
                result = np.fliplr(g).tolist()
            elif Vs and not Hs:
                result = np.flipud(g).tolist()
            elif Rs:
                result = np.rot90(g, 2).tolist()
            else:
                if g.shape[0] == g.shape[1]:
                    result = np.rot90(g, 1).tolist()
                else:
                    result = _center_on_mass_np(att1_ll)
        else:
            if g.shape[0] == g.shape[1]:
                result = np.rot90(g, 1).tolist()
            else:
                result = _center_on_mass_np(att1_ll)
    elif strategy == "oco_auto":
        if phi_arr is None or len(phi_arr) != 8:
            result = _attempt2_from_strategy(att1_ll, "auto", None, train_pairs, test_input_ll)
        else:
            a = np.abs(np.array(phi_arr))
            scores = {"geometry": float(a[3]), "palette": float(a[2]), "alignment": float(a[4]), "objectness": float(a[1])}
            fam = max(scores.items(), key=lambda kv: kv[1])[0]
            if fam == "geometry":
                if test_input_ll is not None:
                    Hs, Vs, Rs = _symmetry_flags_np(test_input_ll)
                    if Rs:
                        result = np.rot90(g, 2).tolist()
                    elif Hs and not Vs:
                        result = np.fliplr(g).tolist()
                    elif Vs and not Hs:
                        result = np.flipud(g).tolist()
                    else:
                        result = np.rot90(g, 1).tolist()
                else:
                    result = np.rot90(g, 1).tolist()
            elif fam == "palette":
                mapping = _palette_map_from_train_pairs(train_pairs or [])
                if mapping:
                    result = _apply_palette_map_ll(att1_ll, mapping)
                else:
                    result = _attempt2_from_strategy(att1_ll, "auto", None, train_pairs, test_input_ll)
            elif fam == "alignment":
                try:
                    result = _center_on_mass_np(att1_ll)
                except Exception:
                    result = att1_ll
            elif fam == "objectness":
                if test_input_ll is not None:
                    result = _translate_toward_input_centroid_np(att1_ll, test_input_ll)
                else:
                    result = _center_on_mass_np(att1_ll)
            else:
                result = _attempt2_from_strategy(att1_ll, "auto", None, train_pairs, test_input_ll)
    elif fam_hint in ("alignment", "geometry"):
        try:
            result = _rot90_np(att1_ll)
        except Exception:
            result = att1_ll
    else:
        result = att1_ll

    att2 = result
    try:
        if fam_hint in ("alignment", "geometry"):
            base_np = np.asarray(att1_ll)
            res_np = np.asarray(att2)
            if res_np.ndim != 2 or np.array_equal(res_np, base_np):
                att2 = _rot90_np(att1_ll)
    except Exception:
        att2 = att1_ll

    att2_np = np.asarray(att2)
    if att2_np.ndim != 2:
        return att1_ll
    return att2_np.tolist()


def _pixel_iou(a, b):
    """Micro-averaged IoU across colors. Returns 0.0 if shapes differ or empty."""
    if a is None or b is None:
        return 0.0
    if len(a) == 0 or len(b) == 0:
        return 0.0
    H1, W1 = len(a), len(a[0]) if a[0] else 0
    H2, W2 = len(b), len(b[0]) if b[0] else 0
    if H1 != H2 or W1 != W2:
        return 0.0
    colors = set()
    for i in range(H1):
        for j in range(W1):
            colors.add(a[i][j])
            colors.add(b[i][j])
    if not colors:
        return 0.0
    inter_sum = 0
    union_sum = 0
    for c in colors:
        inter = 0
        union = 0
        for i in range(H1):
            for j in range(W1):
                in_a = a[i][j] == c
                in_b = b[i][j] == c
                if in_a and in_b:
                    inter += 1
                if in_a or in_b:
                    union += 1
        if union > 0:
            inter_sum += inter
            union_sum += union
    return (inter_sum / union_sum) if union_sum > 0 else 0.0


def _grid_shape(grid):
    if grid is None or len(grid) == 0:
        return (0, 0)
    return len(grid), len(grid[0]) if grid[0] else 0


def _shift_grid(grid, dy, dx, fill=0):
    H, W = _grid_shape(grid)
    if H == 0 or W == 0:
        return grid
    out = [[fill] * W for _ in range(H)]
    for i in range(H):
        for j in range(W):
            ni, nj = i + dy, j + dx
            if 0 <= ni < H and 0 <= nj < W:
                out[ni][nj] = grid[i][j]
    return out


def _rot180(grid):
    return [row[::-1] for row in grid[::-1]] if grid else grid


def _mirror_lr(grid):
    return [row[::-1] for row in grid] if grid else grid


def _largest_component_only(grid, bg=0):
    """Keep the largest 4-connected component across all non-bg pixels; zero elsewhere."""
    H, W = _grid_shape(grid)
    if H == 0 or W == 0:
        return grid
    seen = [[False] * W for _ in range(H)]
    best_area = 0
    best_pts = None
    best_color = bg
    for i in range(H):
        for j in range(W):
            if grid[i][j] == bg or seen[i][j]:
                continue
            color = grid[i][j]
            stack = [(i, j)]
            seen[i][j] = True
            pts = [(i, j)]
            while stack:
                y, x = stack.pop()
                for ny, nx in ((y - 1, x), (y + 1, x), (y, x - 1), (y, x + 1)):
                    if 0 <= ny < H and 0 <= nx < W and not seen[ny][nx] and grid[ny][nx] == color:
                        seen[ny][nx] = True
                        stack.append((ny, nx))
                        pts.append((ny, nx))
            if len(pts) > best_area:
                best_area = len(pts)
                best_pts = pts
                best_color = color
    if best_pts is None:
        return grid
    out = [[bg] * W for _ in range(H)]
    for y, x in best_pts:
        out[y][x] = best_color
    return out


def _surrogate_conf(pred, belief):
    """Cheap [0,1] confidence surrogate from belief metadata."""
    H, W = _grid_shape(pred)
    if H * W == 0:
        return 0.0
    bg = belief.get("bg", 0) if belief else 0
    occ = 0.0
    if H > 0 and W > 0:
        occ = sum(1 for i in range(H) for j in range(W) if pred[i][j] != bg) / (H * W)
    prog_len = max(1, int(belief.get("prog_len", 10))) if belief else 1
    prior_len = 1.0 / (1.0 + prog_len / 10.0)
    tension = 1.0 - min(1.0, float(belief.get("tension", 0.5))) if belief else 0.5
    varp = 1.0 - min(1.0, float(belief.get("train_cost_var", 0.5))) if belief else 0.5
    pal = 1.0 if belief and belief.get("palette_safe", False) else 0.0
    conf = 0.25 * occ + 0.20 * prior_len + 0.25 * tension + 0.20 * varp + 0.10 * pal
    return max(0.0, min(1.0, conf))


def _build_belief(meta, train_pairs=None, phi=None, palette_safe=None, learned_mask=None, learned_proj=None, align=None):
    belief = {
        "ky_kx": meta.get("ky_kx") if isinstance(meta, dict) else None,
        "block_mask": learned_mask if learned_mask is not None else (meta.get("block_mask") if isinstance(meta, dict) else None),
        "block_proj": learned_proj if learned_proj is not None else (meta.get("block_proj") if isinstance(meta, dict) else None),
        "align_dy_dx": align if align is not None else (meta.get("align_dy_dx") if isinstance(meta, dict) else None),
        "palette_safe": bool(palette_safe if palette_safe is not None else (meta.get("palette_safe", False) if isinstance(meta, dict) else False)),
        "prog_len": meta.get("prog_len", 10) if isinstance(meta, dict) else 10,
        "tension": meta.get("tension", 0.5) if isinstance(meta, dict) else 0.5,
        "train_cost_var": meta.get("train_cost_var", 0.5) if isinstance(meta, dict) else 0.5,
        "family": meta.get("family") if isinstance(meta, dict) else None,
        "bg": meta.get("bg", 0) if isinstance(meta, dict) else 0,
    }
    return belief


def _alternates_from_belief(att1, test_x, belief, train_pairs):
    alts = []
    dy, dx = (0, 0)
    if belief.get("align_dy_dx") and len(belief["align_dy_dx"]) == 2:
        dy, dx = belief["align_dy_dx"]
    alts.append({"tag": "align_shift", "grid": _shift_grid(att1, dy, dx, fill=belief.get("bg", 0))})

    block_mask = belief.get("block_mask")
    if block_mask is not None and _grid_shape(block_mask) == _grid_shape(att1):
        H, W = _grid_shape(att1)
        bg = belief.get("bg", 0)
        masked = [[att1[i][j] if block_mask[i][j] else bg for j in range(W)] for i in range(H)]
        alts.append({"tag": "block_mask", "grid": masked})

    block_proj = belief.get("block_proj")
    if block_proj is not None and callable(block_proj):
        try:
            alts.append({"tag": "block_proj", "grid": block_proj(att1)})
        except Exception:
            pass

    alts.append({"tag": "rot180", "grid": _rot180(att1)})
    alts.append({"tag": "mirror", "grid": _mirror_lr(att1)})
    alts.append({"tag": "keep_n_largest(1)", "grid": _largest_component_only(att1, bg=belief.get("bg", 0))})

    for alt in alts:
        alt["conf"] = _surrogate_conf(alt["grid"], belief)
    return alts


def _select_best_pair(att1, candidates, belief, lambda_div=0.20, iou_cap=0.97):
    confA = _surrogate_conf(att1, belief)
    best_choice = None
    best_grid = None
    best_score = -1e9
    for alt in candidates:
        grid = alt.get("grid")
        if grid is None:
            continue
        iou = _pixel_iou(att1, grid)
        if iou >= iou_cap:
            continue
        confB = alt.get("conf", _surrogate_conf(grid, belief))
        score = confA + confB + lambda_div * (1.0 - iou)
        if score > best_score:
            best_score = score
            best_grid = grid
            best_choice = {"tag": alt.get("tag", "?"), "iou": iou, "conf1": confA, "conf2": confB}
    if best_grid is None:
        if candidates:
            fallback = min(candidates, key=lambda c: _pixel_iou(att1, c.get("grid")))
            best_grid = fallback.get("grid", att1)
            best_choice = {"tag": fallback.get("tag", "fallback"), "iou": _pixel_iou(att1, best_grid), "conf1": confA, "conf2": _surrogate_conf(best_grid, belief)}
        else:
            best_grid = att1
            best_choice = {"tag": "self", "iou": 1.0, "conf1": confA, "conf2": confA}
    return att1, best_grid, best_choice


def _smooth_rho(settings, rho_raw):
    if rho_raw is None:
        return getattr(settings, "_hfp_rho_smoothed", None)
    prev = getattr(settings, "_hfp_rho_smoothed", None)
    smoothed = 0.5 * (prev if prev is not None else rho_raw) + 0.5 * rho_raw
    setattr(settings, "_hfp_rho_smoothed", smoothed)
    return smoothed


def _rho_band(rho):
    if rho is None:
        return "<0.70"
    if rho < 0.70:
        return "<0.70"
    if rho < 0.95:
        return "0.70–0.95"
    return "≥0.95"


def _cheap_revision(grid, belief, which="align_or_block"):
    if which == "align_or_block":
        align = belief.get("align_dy_dx") if belief else None
        if isinstance(align, (tuple, list)) and len(align) == 2:
            dy, dx = align
            return _shift_grid(grid, dy, dx, fill=belief.get("bg", 0) if belief else 0)
        bm = belief.get("block_mask") if belief else None
        if bm is not None and _grid_shape(bm) == _grid_shape(grid):
            H, W = _grid_shape(grid)
            bg = belief.get("bg", 0) if belief else 0
            return [[grid[i][j] if bm[i][j] else bg for j in range(W)] for i in range(H)]
    return _mirror_lr(grid)


def _bounded_debate(att1, att2, test_x, belief, rho_s, max_bounces):
    bounces = 0
    if rho_s is None or rho_s < 0.70:
        return att1, att2, bounces
    cap = 1 if rho_s < 0.95 else 2
    if isinstance(max_bounces, int) and max_bounces >= 0:
        cap = min(cap, max_bounces)
    confA = _surrogate_conf(att1, belief)
    confB = _surrogate_conf(att2, belief)
    if confB <= confA + 0.03:
        return att1, att2, bounces
    A1 = _cheap_revision(att1, belief, "align_or_block")
    bounces += 1
    if bounces >= cap:
        return A1, att2, bounces
    confA1 = _surrogate_conf(A1, belief)
    if confA1 > confB + 0.03:
        B1 = _cheap_revision(att2, belief, "align_or_block")
        bounces += 1
        return A1, B1, bounces
    return A1, att2, bounces


def _apply_test_palette_policy_pair(att1, att2, train_pairs, test_x, settings):
    policy = getattr(settings, "test_palette_policy", "second_only_guarded")
    if policy in (None, "none"):
        return att1, att2, "no_palette"
    if policy == "both":
        policy = "second_only_guarded"
    mapped, ok = _palette_map_if_pareto_safe(att2, train_pairs, guard="structure")
    if policy == "second_only":
        return att1, (mapped if ok else att2), "second_only" if ok else "second_only(noop)"
    if policy == "second_only_guarded":
        return att1, (mapped if ok else att2), "second_only_guarded" if ok else "guard_blocked"
    return att1, att2, "unknown_policy"


def _telemetry_note(meta_or_res, **kw):
    try:
        if isinstance(meta_or_res, dict):
            meta_or_res.setdefault("_telemetry", {}).update({k: v for k, v in kw.items() if v is not None})
    except Exception:
        pass


def _is_masked_tiling(meta):
    if isinstance(meta, dict):
        if meta.get("path_tag") == "masked_tiling":
            return True
        if meta.get("tile_masked"):
            return True
        if meta.get("ky_kx") and meta.get("block_mask") is not None and meta.get("no_finishers", True):
            return True
    return False


def _masked_tiling_alternate(att1, meta):
    H, W = _grid_shape(att1)
    if H * W == 0:
        return att1
    bm = meta.get("block_mask") if isinstance(meta, dict) else None
    if bm is not None and _grid_shape(bm) == (H, W):
        return [[att1[i][j] if not bm[i][j] else 0 for j in range(W)] for i in range(H)]
    return [[0 if att1[i][j] != 0 else 1 for j in range(W)] for i in range(H)]


def _structure_ok(a, b, train_pairs=None):
    if _grid_shape(a) != _grid_shape(b):
        return False
    Ha, Wa = _grid_shape(a)
    if Ha * Wa == 0:
        return True
    def _checksum(g):
        H, W = _grid_shape(g)
        return sum((i + 1) * (j + 1) * g[i][j] for i in range(H) for j in range(W))
    return _checksum(a) == _checksum(b)


def _palette_map_if_pareto_safe(grid, train_pairs, guard="structure"):
    try:
        mapping = _palette_map_from_train_pairs(train_pairs or [])
        if not mapping:
            return grid, False
        candidate = _apply_palette_map_ll(grid, mapping)
        if guard == "structure" and not _structure_ok(grid, candidate, train_pairs):
            return grid, False
        return candidate, True
    except Exception:
        return grid, False


def _adjust_abort_windows(settings, seconds, have_shape_match):
    stall = seconds / 3.0
    abort_after = seconds / 3.0
    rho_s = getattr(settings, "_hfp_rho_smoothed", None)
    ready = bool(getattr(settings, "_hfp_ready", False))
    if rho_s is not None and have_shape_match and rho_s >= 0.95:
        stall = max(stall, 0.80 * seconds)
        abort_after = max(abort_after, 0.80 * seconds)
    if ready and rho_s is not None and rho_s < 0.70:
        stall = min(stall, 0.20 * seconds)
        abort_after = min(abort_after, 0.20 * seconds)
    return stall, abort_after


def _median2(values):
    if not values:
        return 0.0
    if len(values) == 1:
        return float(values[-1])
    return float(_stats.median(values[-2:]))


def _maybe_escalate_K(cons_mean, rho_samples, settings, n_train_pairs):
    K = getattr(settings, "max_train_pairs_for_beam", 2)
    cons_mean = cons_mean or 0.0
    rho_med = _median2(rho_samples)
    if cons_mean >= 0.70 or rho_med >= 0.90:
        K = max(3, K)
    return min(n_train_pairs, K)


def _run_dir_staged(tasks_dir, seconds=None, panel_ids=None, logger=None, settings_proto=None):
    stages = [(24, 3, 4.0), (96, 7, 45.0), (128, 8, 120.0)]
    if seconds is not None:
        stages = [(24, 3, 4.0), (96, 7, float(seconds))] if seconds >= 10.0 else [(96, 7, float(seconds))]
    loader = globals().get("_load_test_challenges") or globals().get("load_tasks")
    if loader is None:
        raise RuntimeError("task loader not found")
    challenges = loader(tasks_dir)
    tids = list(panel_ids) if panel_ids else list(challenges.keys())
    outputs = {}

    def _score(preds):
        if not preds:
            return (0, 0.0)
        if isinstance(preds[0], dict) and "attempt_1" in preds[0]:
            ious = [_pixel_iou(d["attempt_1"], d["attempt_2"]) for d in preds]
            div = 1.0 - float(sum(ious) / len(ious))
            return (2, div)
        return (1, 0.0)

    for tid in tids:
        task_json = challenges[tid]
        best, best_tag = None, (0, 0.0)
        for beam, depth, secs in stages:
            proto = settings_proto or SearchSettings()
            stage_settings = replace(proto, beam_width=beam, max_depth=depth, max_seconds=secs, use_meta_controller=True)
            start = time.time()
            res = solve_task(tid, task_json, stage_settings, logger=logger)
            elapsed = time.time() - start
            try:
                _telemetry_note(res if isinstance(res, dict) else {}, stage_time=elapsed, stage=(beam, depth, secs))
            except Exception:
                pass
            sc = _score(res)
            if best is None or sc > best_tag:
                best, best_tag = res, sc
                outputs[tid] = best
            if best_tag[0] == 2 and best_tag[1] >= 0.03:
                break
    return outputs

def _build_train_test_lookups(tasks_dir):
    train_lookup = {}
    test_lookup = {}
    for task_id, task_json in load_tasks_from_dir(tasks_dir):
        pairs = trains_from_task(task_json)
        tests = tests_from_task(task_json)
        train_lookup[task_id] = pairs
        test_lookup[task_id] = tests
    return train_lookup, test_lookup


def _two_attempts_from_results(results, tasks_dir, strategy="oco_auto", settings=None):
    train_lookup, test_lookup = _build_train_test_lookups(tasks_dir)
    out = {}
    for tid, pred_ll in results.items():
        base_obj = pred_ll
        if isinstance(base_obj, dict) and "attempt_1" in base_obj and "attempt_2" in base_obj:
            out[tid] = [{"attempt_1": base_obj["attempt_1"], "attempt_2": base_obj["attempt_2"]}]
            continue
        if isinstance(base_obj, list) and len(base_obj) == 1 and isinstance(base_obj[0], dict) and "attempt_1" in base_obj[0] and "attempt_2" in base_obj[0]:
            out[tid] = [{"attempt_1": base_obj[0]["attempt_1"], "attempt_2": base_obj[0]["attempt_2"]}]
            continue

        pairs = train_lookup.get(tid, [])
        phi = None
        if pairs:
            try:
                phi = compute_phi(task_features(pairs))
            except Exception:
                phi = None
        tests = test_lookup.get(tid, [])
        outs = base_obj if isinstance(base_obj, list) else [base_obj]
        out_two = []
        for idx, pred in enumerate(outs):
            if isinstance(pred, dict) and "attempt_1" in pred:
                base = pred["attempt_1"]
                meta = pred
            elif isinstance(pred, dict) and "grid" in pred:
                base = pred["grid"]
                meta = pred.get("meta", pred)
            else:
                base = pred
                meta = pred if isinstance(pred, dict) else {}
            base_np = np.asarray(base)
            if base_np.ndim != 2:
                base_list = base_np.tolist() if hasattr(base_np, "tolist") else base
                out_two.append({"attempt_1": base_list, "attempt_2": base_list})
                continue

            test_in = tests[idx] if tests and idx < len(tests) else None
            belief = _build_belief(meta if isinstance(meta, dict) else {}, train_pairs=pairs)

            if _is_masked_tiling(meta):
                att2 = _masked_tiling_alternate(base, meta)
                policy_tag = "masked_raw"
                bounces = 0
                try:
                    _telemetry_note(
                        meta if isinstance(meta, dict) else {},
                        iou=_pixel_iou(base, att2),
                        alt_tag="masked_raw_pair",
                        conf1=None,
                        conf2=None,
                        bounces=bounces,
                        rho_band=_rho_band(None),
                        attempt_policy=policy_tag,
                    )
                except Exception:
                    pass
            else:
                candidates = []
                if strategy and strategy != "oco_auto":
                    try:
                        forced = _attempt2_from_strategy(base, strategy, phi, pairs, test_in)
                        candidates.append({"tag": f"forced:{strategy}", "grid": forced, "conf": _surrogate_conf(forced, belief)})
                    except Exception:
                        pass
                candidates.extend(_alternates_from_belief(base, test_in, belief, pairs))
                lambda_div = getattr(settings, "div_lambda", 0.20) if settings else 0.20
                iou_cap = getattr(settings, "iou_cap", 0.97) if settings else 0.97
                att1_sel, att2_sel, choice = _select_best_pair(base, candidates, belief, lambda_div=lambda_div, iou_cap=iou_cap)
                base = att1_sel
                att2 = att2_sel
                rho_raw = getattr(settings, "_hfp_rho", None) if settings else None
                rho_sm = _smooth_rho(settings, rho_raw) if settings else rho_raw
                max_b = getattr(settings, "max_bounces", -1) if settings else -1
                att1_deb, att2_deb, bounces = _bounded_debate(base, att2, test_in, belief, rho_sm, max_b)
                base = att1_deb
                att2 = att2_deb
                base, att2, policy_tag = _apply_test_palette_policy_pair(base, att2, pairs, test_in, settings or SearchSettings())
                try:
                    _telemetry_note(meta if isinstance(meta, dict) else {},
                                    iou=_pixel_iou(base, att2),
                                    alt_tag=choice.get("tag") if choice else None,
                                    conf1=choice.get("conf1") if choice else None,
                                    conf2=choice.get("conf2") if choice else None,
                                    bounces=bounces,
                                    rho_band=_rho_band(rho_sm),
                                    attempt_policy=policy_tag)
                except Exception:
                    pass

            base_np = np.asarray(base)
            att2_np = np.asarray(att2)
            if att2_np.ndim != 2 or att2_np.shape != base_np.shape:
                att2_np = base_np
            out_two.append({"attempt_1": base_np.tolist(), "attempt_2": att2_np.tolist()})
        out[tid] = out_two
    return out


def _find_arc_tasks_dir_fallback(requested_dir):
    if os.path.isdir(requested_dir):
        files = glob.glob(os.path.join(requested_dir, "*.json"))
        if files:
            return requested_dir
    parent = os.path.dirname(requested_dir)
    if parent:
        alt = os.path.join(parent, "arc-agi_test_challenges.json")
        if os.path.isfile(alt):
            test_dir = os.path.join(parent, "test")
            if os.path.isdir(test_dir):
                return test_dir
    return requested_dir


# ============================================================================
# ============ Alignment Finisher Utilities ============
# ============================================================================

def _binary_mask(a):
    import numpy as np
    return (np.asarray(a) != 0).astype(np.int32)

def _bbox_top_left(a):
    import numpy as np
    g = np.asarray(a)
    mask = (g != 0)
    if not mask.any():
        return None
    ys, xs = np.where(mask)
    return int(ys.min()), int(xs.min())

def _overlap_score(maskA, maskB, dy, dx):
    import numpy as np
    A = maskA; B = maskB
    H, W = B.shape
    # place A on B with shift (dy,dx), count overlap of 1s
    y_ps = max(0,  dy);  x_ps = max(0,  dx)
    y_bs = max(0, -dy);  x_bs = max(0, -dx)
    h = H - abs(dy);     w = W - abs(dx)
    if h <= 0 or w <= 0:
        return 0
    return int((A[y_ps:y_ps+h, x_ps:x_ps+w] & B[y_bs:y_bs+h, x_bs:x_bs+w]).sum())

def _propose_alignment_deltas(pred, truth, window=3):
    """
    Return a small candidate set of (dy,dx) using bbox + cross-corr (overlap)
    plus neighbors; clip to ±window.
    """
    import numpy as np
    pm = _binary_mask(pred); tm = _binary_mask(truth)
    # bbox proposal
    bbox = []
    bpp = _bbox_top_left(pred)
    btt = _bbox_top_left(truth)
    if bpp and btt:
        bbox = [(int(btt[0]-bpp[0]), int(btt[1]-bpp[1]))]

    # cross-corr (maximize overlap in a small window)
    best = (0, 0, -1)
    for dy in range(-window, window+1):
        for dx in range(-window, window+1):
            s = _overlap_score(pm, tm, dy, dx)
            if s > best[2]:
                best = (dy, dx, s)

    cand = [(0,0)]
    if bbox:
        by, bx = bbox[0]
        cand += [(by, bx), (by+1, bx), (by-1, bx), (by, bx+1), (by, bx-1)]
    cy, cx = best[0], best[1]
    cand += [(cy, cx), (cy+1, cx), (cy-1, cx), (cy, cx+1), (cy, cx-1)]

    # clip & dedup
    uniq = []
    seen = set()
    for (dy,dx) in cand:
        dy = int(np.clip(dy, -window, window))
        dx = int(np.clip(dx, -window, window))
        if (dy,dx) not in seen:
            uniq.append((dy,dx)); seen.add((dy,dx))
    return uniq

def _current_acc_state(pred: np.ndarray, target: np.ndarray):
    """Return (acc, (cy_p,cx_p), (cy_t,cx_t)) or (0.0, None, None) if shape mismatch."""
    if pred.shape != target.shape: 
        return 0.0, None, None
    acc = float((pred == target).mean())
    def _centroid(g):
        ys, xs = np.where(g != 0)
        return (float(ys.mean()), float(xs.mean())) if ys.size else (None, None)
    return acc, _centroid(pred), _centroid(target)

def _learn_task_alignment(program, train_pairs, phi, settings):
    """
    For each training pair, pick the best (dy,dx) from a small candidate set
    (bbox + xcorr proposals); return the median (dy,dx) over pairs.
    """
    import numpy as np
    dy_list, dx_list = [], []
    for (x, y) in train_pairs:
        try:
            pred = interpret_program(program, x)
        except Exception:
            continue
        # only consider alignment if shapes match
        if pred.shape != y.shape:
            continue
        cand = _propose_alignment_deltas(pred, y, window=3)
        if not cand:
            continue
        # choose by pixel accuracy
        best_acc, best_dy, best_dx = -1.0, 0, 0
        H, W = pred.shape
        for (dy,dx) in cand:
            shifted = np.zeros_like(pred)
            y0 = max(0,  dy);  x0 = max(0,  dx)
            ys = max(0, -dy);  xs = max(0, -dx)
            h = H - abs(dy);    w = W - abs(dx)
            if h <= 0 or w <= 0:
                continue
            shifted[y0:y0+h, x0:x0+w] = pred[ys:ys+h, xs:xs+w]
            acc = (shifted == y).mean()
            if acc > best_acc:
                best_acc, best_dy, best_dx = acc, dy, dx
        dy_list.append(best_dy); dx_list.append(best_dx)
    if not dy_list:
        return None
    return (int(np.median(dy_list)), int(np.median(dx_list)))


# ============================================================================
# ============ Block-Mask Finisher Utilities ============
# ============================================================================

def _divisible_shape(in_shape, out_shape):
    Hin, Win = in_shape
    Hout, Wout = out_shape
    if Hin <= 0 or Win <= 0: 
        return None
    if Hout % Hin != 0 or Wout % Win != 0:
        return None
    return (Hout // Hin, Wout // Win)

def _downsample_block_presence(y, ky, kx):
    """
    For a target grid y with shape (Hout, Wout) and block (Hin, Win) = (Hout/ky, Wout/kx),
    compute a boolean mask M[ky,kx] that marks whether each block is *meaningfully nonzero*.
    """
    import numpy as np
    y = np.asarray(y)
    Hout, Wout = y.shape
    Hin, Win   = Hout // ky, Wout // kx
    M = np.zeros((ky, kx), dtype=bool)
    for by in range(ky):
        for bx in range(kx):
            y0, x0 = by*Hin, bx*Win
            block = y[y0:y0+Hin, x0:x0+Win]
            # "meaningfully nonzero": at least one non-zero AND not trivially dense background (heuristic)
            if np.any(block != 0):
                M[by, bx] = True
    return M

def _learn_block_mask(train_pairs, ky, kx):
    """
    Aggregate block-presence across all training targets; return a majority-vote mask M[ky,kx].
    If ambiguity is high, fall back to all-True (no masking).
    """
    import numpy as np
    votes = np.zeros((ky, kx), dtype=np.int32)
    total = 0
    for (_x, y) in train_pairs:
        y = np.asarray(y)
        Hout, Wout = y.shape
        if Hout % ky != 0 or Wout % kx != 0:
            continue
        M = _downsample_block_presence(y, ky, kx)
        votes += M.astype(np.int32)
        total += 1
    if total == 0:
        return None
    # majority vote (>= half)
    thresh = (total + 1) // 2
    Mmaj = votes >= thresh
    # avoid degenerate all-False; if nearly full, keep all-True (no mask)
    if not Mmaj.any():
        return None
    return Mmaj

def _apply_block_mask(grid, ky, kx, M):
    """
    Zero out blocks where M[by,bx] == False. Assumes grid shape (Hout,Wout) divisible by ky,kx.
    """
    import numpy as np
    g = np.asarray(grid)
    Hout, Wout = g.shape
    Hin, Win   = Hout // ky, Wout // kx
    out = g.copy()
    for by in range(ky):
        for bx in range(kx):
            if not M[by, bx]:
                y0, x0 = by*Hin, bx*Win
                out[y0:y0+Hin, x0:x0+Win] = 0
    return out


# ============================================================================
# ============ Blockwise Color Finisher Utilities ============
# ============================================================================

def _block_shapes(out_shape, ky, kx):
    Hout, Wout = out_shape
    if Hout % ky != 0 or Wout % kx != 0:
        return None
    Hin, Win = Hout // ky, Wout // kx
    return Hin, Win

def _blockwise_dominant_colors(y, ky, kx):
    """
    For a target grid y (Hout,Wout) divisible by ky,kx, return an array D[ky,kx]
    with the dominant (majority) color per block.
    """
    import numpy as np
    y = np.asarray(y)
    Hout, Wout = y.shape
    Hin, Win = _block_shapes((Hout, Wout), ky, kx)
    if Hin is None:
        return None
    D = np.zeros((ky, kx), dtype=np.int32)
    for by in range(ky):
        for bx in range(kx):
            y0, x0 = by*Hin, bx*Win
            block = y[y0:y0+Hin, x0:x0+Win]
            vals, cnts = np.unique(block, return_counts=True)
            D[by, bx] = int(vals[np.argmax(cnts)])
    return D

def _learn_blockwise_projection(train_pairs, ky, kx):
    """
    Learn a blockwise dominant-color projection from training targets.
    Majority-vote the dominant color per block across all pairs.
    Returns D[ky,kx] or None if shapes are inconsistent.
    """
    import numpy as np
    votes = None
    total = 0
    for (_x, y) in train_pairs:
        y = np.asarray(y)
        HinWin = _block_shapes(y.shape, ky, kx)
        if HinWin is None:
            continue
        D = _blockwise_dominant_colors(y, ky, kx)
        if D is None:
            continue
        if votes is None:
            # store as dict of counters per block
            votes = [[{} for _ in range(kx)] for _ in range(ky)]
        for by in range(ky):
            for bx in range(kx):
                c = int(D[by, bx])
                votes[by][bx][c] = votes[by][bx].get(c, 0) + 1
        total += 1

    if votes is None or total == 0:
        return None

    Dmaj = [[0 for _ in range(kx)] for _ in range(ky)]
    for by in range(ky):
        for bx in range(kx):
            cnts = votes[by][bx]
            if not cnts:
                Dmaj[by][bx] = 0
            else:
                # majority color per block
                Dmaj[by][bx] = max(cnts.items(), key=lambda kv: kv[1])[0]
    import numpy as np
    return np.array(Dmaj, dtype=np.int32)

def _apply_blockwise_projection(pred, ky, kx, D):
    """
    Project each block of pred to the learned dominant target color D[by,bx].
    For now, we set all non-zero pixels in the block to D[by,bx] (zero stays zero).
    """
    import numpy as np
    g = np.asarray(pred).copy()
    Hout, Wout = g.shape
    Hin, Win = _block_shapes((Hout, Wout), ky, kx)
    if Hin is None or D is None:
        return g
    for by in range(ky):
        for bx in range(kx):
            y0, x0 = by*Hin, bx*Win
            blk = g[y0:y0+Hin, x0:x0+Win]
            tgt = int(D[by, bx])
            mask = (blk != 0)
            blk[mask] = tgt
            g[y0:y0+Hin, x0:x0+Win] = blk
    return g



# ============================================================================
# ============ dsl/ops.py ============
# ============================================================================

def _crop_border(arr, margin=1):
    """Remove `margin` rows/cols from all 4 sides (for op=shrink)."""
    if arr.shape[0] <= 2*margin or arr.shape[1] <= 2*margin:
        return arr
    return arr[margin:-margin, margin:-margin].copy()

def _pad_expand(arr, amount=1, fillval=0):
    """Add `amount` rows/cols on all 4 sides."""
    return np.pad(arr, amount, mode='constant', constant_values=fillval)

def _isolate_largest_region(arr):
    """Zero everything except the largest connected component (4-neighbor)."""
    from collections import deque
    H, W = arr.shape
    visited = np.zeros((H, W), dtype=bool)
    components = []
    for y0 in range(H):
        for x0 in range(W):
            if not visited[y0, x0] and arr[y0, x0] != 0:
                region = []
                q = deque([(y0, x0)])
                visited[y0, x0] = True
                while q:
                    y, x = q.popleft()
                    region.append((y, x))
                    for dy, dx in [(-1,0),(1,0),(0,-1),(0,1)]:
                        ny, nx = y+dy, x+dx
                        if 0 <= ny < H and 0 <= nx < W:
                            if not visited[ny, nx] and arr[ny, nx] == arr[y, x]:
                                visited[ny, nx] = True
                                q.append((ny, nx))
                components.append(region)
    if not components:
        return np.zeros_like(arr)
    largest = max(components, key=len)
    out = np.zeros_like(arr)
    for (y, x) in largest:
        out[y, x] = arr[y, x]
    return out

def _conn_comps(arr):
    """Extract all connected components (4-neighbor) as (color, pts) tuples."""
    H, W = arr.shape
    seen = np.zeros((H, W), dtype=bool)
    comps = []
    for y in range(H):
        for x in range(W):
            if arr[y, x] != 0 and not seen[y, x]:
                color = arr[y, x]
                q = [(y, x)]
                seen[y, x] = True
                pts = []
                while q:
                    yy, xx = q.pop()
                    pts.append((yy, xx))
                    for dy, dx in [(-1,0),(1,0),(0,-1),(0,1)]:
                        ny, nx = yy+dy, xx+dx
                        if 0 <= ny < H and 0 <= nx < W and not seen[ny, nx] and arr[ny, nx] == color:
                            seen[ny, nx] = True
                            q.append((ny, nx))
                comps.append((color, pts))
    return comps


def _flood_holes_mask(mask: np.ndarray) -> np.ndarray:
    """Return a boolean mask marking interior background pixels (holes) of a binary mask."""
    H, W = mask.shape
    bg = mask == 0
    ext = np.zeros_like(bg, dtype=bool)
    q = deque()

    for x in range(W):
        if bg[0, x]:
            q.append((0, x))
        if bg[H - 1, x]:
            q.append((H - 1, x))
    for y in range(H):
        if bg[y, 0]:
            q.append((y, 0))
        if bg[y, W - 1]:
            q.append((y, W - 1))

    while q:
        y, x = q.popleft()
        if not (0 <= y < H and 0 <= x < W):
            continue
        if ext[y, x] or not bg[y, x]:
            continue
        ext[y, x] = True
        for dy, dx in [(-1, 0), (1, 0), (0, -1), (0, 1)]:
            q.append((y + dy, x + dx))

    return bg & ~ext


def _holes_count(grid: np.ndarray) -> int:
    """Count interior holes (background regions fully enclosed by non-zero cells)."""
    mask = (grid != 0).astype(np.uint8)
    return int(_flood_holes_mask(mask).sum())


def _component_count(grid: np.ndarray) -> int:
    """Return number of connected non-zero components (4-conn)."""
    from scipy.ndimage import label

    return int(label(grid != 0)[1])


def _pixel_acc(pred_item, truth_np: np.ndarray) -> float:
    """Best-of-two if dict; otherwise single. Shapes must match."""
    if isinstance(pred_item, dict) and "attempt_1" in pred_item:
        p1 = np.asarray(pred_item["attempt_1"])
        p2 = np.asarray(pred_item["attempt_2"])

        def _acc(p):
            return float((p == truth_np).mean()) if p.shape == truth_np.shape else 0.0

        return max(_acc(p1), _acc(p2))
    p = np.asarray(pred_item)
    return float((p == truth_np).mean()) if p.shape == truth_np.shape else 0.0


def get_test_truths(solutions_obj: dict, tid: str):
    """
    Local notebook helper (submission-safe): robustly load test truths across common shapes.
    • {"tid": {"test":[...grids...]}}
    • {"tid": [...grids...]}
    • {"tid": {"outputs":[...grids...]}}  # rare legacy
    Each entry may be {"output":[...]} or the grid directly.
    """
    entry = solutions_obj.get(tid)
    if entry is None:
        return []
    if isinstance(entry, dict) and "test" in entry:
        outs = entry["test"]
    elif isinstance(entry, list):
        outs = entry
    elif isinstance(entry, dict) and "outputs" in entry:
        outs = entry["outputs"]
    else:
        outs = []
    resolved = []
    for out in outs:
        if isinstance(out, dict) and "output" in out:
            resolved.append(np.array(out["output"], dtype=np.int32))
        else:
            resolved.append(np.array(out, dtype=np.int32))
    return resolved


# ===== v2.9.0: Policy Prior =====


class PolicyPrior:
    """Return an additive logit (prior) for an operator under context."""

    def op_logit(self, op_name: str, ctx) -> float:
        return 0.0


class HeuristicPrior(PolicyPrior):
    """
    Heuristic prior using φ-family hints (scale, objectness, palette, alignment, geometry, pattern, topology, composition)
    and simple grid checks (divisibility for tiling, symmetry flags if available).
    """

    def op_logit(self, op_name: str, ctx) -> float:
        # ctx: dict with fields we pass from the beam: phi, grid, grid_shape, divs, sym, family
        divs = ctx.get("divs", ())
        family = ctx.get("family", None)

        score = 0.0

        if ("tile" in op_name or "phase_tile" in op_name) and divs:
            score += 0.6

        if family in ("alignment", "geometry") and (
            "mirror" in op_name or "rot" in op_name
        ):
            score += 0.3

        if family in ("objectness", "topology") and (
            "keep_n_largest" in op_name
            or "fill_holes" in op_name
            or "remove_isolated" in op_name
        ):
            score += 0.25

        if family == "palette" and any(word in op_name for word in ("palette_map", "recolor")):
            score += 0.2
        elif family != "palette" and "palette" in op_name:
            score -= 0.15

        return score


_DEFAULT_POLICY_PRIOR = HeuristicPrior()


def _op_fill_holes(grid: np.ndarray) -> np.ndarray:
    """Fill interior holes of each color component independently."""
    out = grid.copy()
    for c in np.unique(grid):
        if c == 0:
            continue
        mask = (grid == c).astype(np.uint8)
        if not mask.any():
            continue
        holes = _flood_holes_mask(mask)
        if holes.any():
            out[holes] = c
    return out


def _op_keep_rings(grid: np.ndarray) -> np.ndarray:
    """Keep only components (per color) that contain at least one hole; zero everything else."""
    out = np.zeros_like(grid)
    structure = np.ones((3, 3), dtype=int)
    for c in np.unique(grid):
        if c == 0:
            continue
        mask = (grid == c).astype(np.uint8)
        if not mask.any():
            continue
        lbl, n = _cc_label(mask, structure=structure)
        for k in range(1, n + 1):
            comp = (lbl == k).astype(np.uint8)
            holes = _flood_holes_mask(comp)
            if holes.any():
                out[comp == 1] = c
    return out


def _op_remove_isolated(grid: np.ndarray, min_size: int) -> np.ndarray:
    """Remove color components smaller than min_size (per color), keep the rest."""
    out = grid.copy()
    structure = np.ones((3, 3), dtype=int)
    for c in np.unique(grid):
        if c == 0:
            continue
        mask = (grid == c).astype(np.uint8)
        if not mask.any():
            continue
        lbl, n = _cc_label(mask, structure=structure)
        for k in range(1, n + 1):
            comp = lbl == k
            if np.count_nonzero(comp) < int(min_size):
                out[comp] = 0
    return out


def _op_keep_n_largest(grid, n):
    """keep_n_largest <n>
    Keep only the n largest connected components by size.
    """
    comps = _conn_comps(grid)
    comps.sort(key=lambda c: len(c[1]), reverse=True)
    keep = set()
    for color, pts in comps[:max(0, int(n))]:
        for y, x in pts:
            keep.add((y, x))
    out = np.zeros_like(grid)
    for (y, x) in keep:
        out[y, x] = grid[y, x]
    return out

def _op_keep_size_range(grid, amin, amax):
    """keep_size_range <amin> <amax>
    Keep only connected components with size in [amin, amax].
    """
    amin, amax = int(amin), int(amax)
    comps = _conn_comps(grid)
    out = np.zeros_like(grid)
    for color, pts in comps:
        if amin <= len(pts) <= amax:
            for y, x in pts:
                out[y, x] = color
    return out

def _op_fill(grid, color):
    """fill <color>"""
    out = np.full_like(grid, color)
    return out

def _op_mirror(grid, axis):
    """mirror <axis>
    axis=0->flipud, axis=1->fliplr
    """
    if axis == 0:
        return np.flipud(grid)
    return np.fliplr(grid)

def _op_rot90(grid, k):
    """rot90 <k>
    k=1 => 90deg, k=2 => 180deg, k=3 => 270deg
    """
    return np.rot90(grid, k=k)

def _op_transpose(grid):
    """transpose"""
    return grid.T

def _op_shrink(grid):
    """shrink (margin=1)"""
    return _crop_border(grid, margin=1)

def _op_grow(grid):
    """grow (pad=1)"""
    return _pad_expand(grid, amount=1, fillval=0)

def _op_add(grid1, grid2):
    """add <grid2>"""
    combined = np.where(grid1 != 0, grid1, grid2)
    return combined

def _op_mask(grid, color):
    """mask <color>"""
    out = np.where(grid == color, grid, 0)
    return out

def _op_invert(grid):
    """invert (mod 10 for ARC)"""
    out = (grid + 5) % 10
    return out

def _op_duplicate(grid):
    """duplicate (identity)"""
    return grid.copy()

def _op_scale(grid, factor):
    """scale <factor>
    Naive nearest-neighbor. factor must be an int>=1.
    """
    if factor <= 0:
        return grid
    H, W = grid.shape
    out = np.zeros((H * factor, W * factor), dtype=grid.dtype)
    for i in range(H):
        for j in range(W):
            val = grid[i, j]
            for di in range(factor):
                for dj in range(factor):
                    out[i*factor+di, j*factor+dj] = val
    return out

def _op_tile(grid, vert, horiz):
    """tile <vert> <horiz>"""
    return np.tile(grid, (vert, horiz))

def _op_tile_masked(grid, ky, kx, mode):
    """
    tile_masked <ky> <kx> <mode>

    Tiling with selective block placement:
      mode=0: cross    -> keep blocks in center row or center column
      mode=1: border   -> keep blocks on the perimeter only
      mode=2: main_diag (optional) -> keep blocks where by == bx (requires ky==kx)
      mode=3: anti_diag (optional) -> keep blocks where by + bx == ky - 1 (requires ky==kx)

    Unselected blocks are set to 0. This enables sparse lattice patterns (e.g., plus frames).
    """
    import numpy as np
    g = np.asarray(grid)
    H, W = g.shape
    out = np.zeros((H*ky, W*kx), dtype=g.dtype)

    same = (ky == kx)
    for by in range(ky):
        for bx in range(kx):
            keep = False
            if mode == 0:
                keep = (by == ky // 2) or (bx == kx // 2)
            elif mode == 1:
                keep = (by == 0) or (by == ky - 1) or (bx == 0) or (bx == kx - 1)
            elif mode == 2 and same:
                keep = (by == bx)
            elif mode == 3 and same:
                keep = ((by + bx) == (ky - 1))
            if keep:
                y0, x0 = by*H, bx*W
                out[y0:y0+H, x0:x0+W] = g
    return out

def _op_crop(grid, color):
    """crop <color>
    Crop to bounding box of `color`.
    """
    mask = (grid == color)
    if not mask.any():
        return grid
    rows = np.any(mask, axis=1)
    cols = np.any(mask, axis=0)
    r0, r1 = np.where(rows)[0][[0, -1]]
    c0, c1 = np.where(cols)[0][[0, -1]]
    return grid[r0:r1+1, c0:c1+1].copy()

def _op_shift(grid, dy, dx):
    """shift <dy> <dx>"""
    H, W = grid.shape
    out = np.zeros_like(grid)
    y_src_start = max(0, -dy)
    y_dst_start = max(0, dy)
    x_src_start = max(0, -dx)
    x_dst_start = max(0, dx)
    h = H - abs(dy)
    w = W - abs(dx)
    if h > 0 and w > 0:
        out[y_dst_start:y_dst_start+h, x_dst_start:x_dst_start+w] = \
            grid[y_src_start:y_src_start+h, x_src_start:x_src_start+w]
    return out

def _op_replacecolor(grid, old_color, new_color):
    """replacecolor <old> <new>"""
    out = grid.copy()
    out[out == old_color] = new_color
    return out

def _op_swapcolors(grid, c1, c2):
    """swapcolors <c1> <c2>"""
    out = grid.copy()
    mask1 = (out == c1)
    mask2 = (out == c2)
    out[mask1] = c2
    out[mask2] = c1
    return out

def _op_outline(grid, color):
    """outline <color>
    Set the perimeter of all non-zero cells to <color>.
    """
    if grid.size == 0:
        return grid.copy()
    mask = (grid != 0)
    edges = np.zeros_like(grid, dtype=bool)
    H, W = grid.shape
    for y in range(H):
        for x in range(W):
            if mask[y, x]:
                for dy, dx in [(-1,0),(1,0),(0,-1),(0,1)]:
                    ny, nx = y+dy, x+dx
                    if 0 <= ny < H and 0 <= nx < W:
                        if not mask[ny, nx]:
                            edges[y, x] = True
                            break
    out = grid.copy()
    out[edges] = color
    return out

def _op_majority(grid):
    """majority
    Fill the entire grid with the most frequent non-zero color.
    """
    vals = grid[grid != 0]
    if vals.size == 0:
        return grid.copy()
    unique, counts = np.unique(vals, return_counts=True)
    maj = unique[np.argmax(counts)]
    return np.full_like(grid, maj)

def _op_minority(grid):
    """minority
    Fill the entire grid with the *least* frequent non-zero color.
    """
    vals = grid[grid != 0]
    if vals.size == 0:
        return grid.copy()
    unique, counts = np.unique(vals, return_counts=True)
    min_color = unique[np.argmin(counts)]
    return np.full_like(grid, min_color)

def _op_threshold(grid, val):
    """threshold <val>
    If pixel >= val => pixel, else 0.
    """
    out = np.where(grid >= val, grid, 0)
    return out

def _op_largest(grid):
    """largest
    Isolate the largest connected component.
    """
    return _isolate_largest_region(grid)

def _op_resize(grid, h, w):
    """resize <h> <w>
    Simple nearest-neighbor resize. If target is smaller, we truncate. If larger, we replicate edge.
    """
    H, W = grid.shape
    if H == h and W == w:
        return grid.copy()
    out = np.zeros((h, w), dtype=grid.dtype)
    for i in range(h):
        for j in range(w):
            src_i = min(int(i * H / h), H-1)
            src_j = min(int(j * W / w), W-1)
            out[i, j] = grid[src_i, src_j]
    return out

def _op_stack(grid1, grid2, axis):
    """stack <grid2> <axis>
    axis=0 => vertical stack, axis=1 => horizontal stack
    """
    if axis == 0:
        return np.vstack([grid1, grid2])
    return np.hstack([grid1, grid2])

def _op_subtract(grid1, grid2):
    """subtract <grid2>
    Wherever grid2 != 0, set grid1 to 0.
    """
    out = grid1.copy()
    out[grid2 != 0] = 0
    return out


def _op_phase_tile(grid, ky, kx, mode=0):
    """
    phase_tile <ky> <kx> [mode]
    mode=0: alternate rot180 on odd (by+bx)
    mode=1: alternate flipud on odd
    mode=2: alternate fliplr on odd
    Creates a k-by-k quilt where blocks with parity (by+bx)%2==1 use a transformed base.
    """
    import numpy as np
    H, W = grid.shape
    out = np.zeros((H*ky, W*kx), dtype=grid.dtype)
    for by in range(ky):
        for bx in range(kx):
            block = grid
            odd = (by + bx) % 2
            if odd:
                if mode == 0:
                    block = np.rot90(block, 2)
                elif mode == 1:
                    block = np.flipud(block)
                elif mode == 2:
                    block = np.fliplr(block)
            y0, x0 = by*H, bx*W
            out[y0:y0+H, x0:x0+W] = block
    return out

def _op_phase_tile_row(grid, ky, kx, mode=2):
    """
    phase_tile_row <ky> <kx> <mode>
    Transform every block in odd block-rows (by % 2 == 1).
    mode=0: rot180, 1: flipud, 2: fliplr (default)
    """
    import numpy as np
    H, W = grid.shape
    out = np.zeros((H*ky, W*kx), dtype=grid.dtype)
    for by in range(ky):
        for bx in range(kx):
            block = grid
            if (by % 2) == 1:
                if mode == 0:
                    block = np.rot90(block, 2)
                elif mode == 1:
                    block = np.flipud(block)
                else:
                    block = np.fliplr(block)
            y0, x0 = by*H, bx*W
            out[y0:y0+H, x0:x0+W] = block
    return out


def _op_phase_tile_col(grid, ky, kx, mode=2):
    """
    phase_tile_col <ky> <kx> <mode>
    Transform every block in odd block-columns (bx % 2 == 1).
    mode=0: rot180, 1: flipud, 2: fliplr (default)
    """
    import numpy as np
    H, W = grid.shape
    out = np.zeros((H*ky, W*kx), dtype=grid.dtype)
    for by in range(ky):
        for bx in range(kx):
            block = grid
            if (bx % 2) == 1:
                if mode == 0:
                    block = np.rot90(block, 2)
                elif mode == 1:
                    block = np.flipud(block)
                else:
                    block = np.fliplr(block)
            y0, x0 = by*H, bx*W
            out[y0:y0+H, x0:x0+W] = block
    return out



OP_NAMES_BASIC = [
    # (name,arity, param_types)
    # Shape-critical operations first for beam efficiency
    ("resize", 3, ["grid", "int", "int"]),
    ("tile", 3, ["grid", "int", "int"]),
    ("tile_masked", 4, ["grid", "int", "int", "int"]),
    ("phase_tile_row", 4, ["grid", "int", "int", "int"]),
    ("phase_tile_col", 4, ["grid", "int", "int", "int"]),
    ("phase_tile", 4, ["grid", "int", "int", "int"]),
    # Geometric and color operations
    ("fill", 2, ["grid", "int"]),
    ("mirror", 2, ["grid", "int"]),
    ("rot90", 2, ["grid", "int"]),
    ("transpose", 1, ["grid"]),
    ("shrink", 1, ["grid"]),
    ("grow", 1, ["grid"]),
    ("add", 2, ["grid", "grid"]),
    ("mask", 2, ["grid", "int"]),
    ("invert", 1, ["grid"]),
    ("duplicate", 1, ["grid"]),
    ("scale", 2, ["grid", "int"]),
    ("crop", 2, ["grid", "int"]),
    ("shift", 3, ["grid", "int", "int"]),
    ("replacecolor", 3, ["grid", "int", "int"]),
    ("swapcolors", 3, ["grid", "int", "int"]),
    ("outline", 2, ["grid", "int"]),
    ("majority", 1, ["grid"]),
    ("minority", 1, ["grid"]),
    ("threshold", 2, ["grid", "int"]),
    ("largest", 1, ["grid"]),
    ("fill_holes", 1, ["grid"]),
    ("keep_rings", 1, ["grid"]),
    ("remove_isolated", 2, ["grid", "int"]),
    ("stack", 3, ["grid", "grid", "int"]),
    ("subtract", 2, ["grid", "grid"]),
]

OP_REGISTRY = {
    "fill": _op_fill,
    "mirror": _op_mirror,
    "rot90": _op_rot90,
    "transpose": _op_transpose,
    "shrink": _op_shrink,
    "grow": _op_grow,
    "add": _op_add,
    "mask": _op_mask,
    "invert": _op_invert,
    "duplicate": _op_duplicate,
    "scale": _op_scale,
    "tile": _op_tile,
    "tile_masked": _op_tile_masked,
    "crop": _op_crop,
    "shift": _op_shift,
    "replacecolor": _op_replacecolor,
    "swapcolors": _op_swapcolors,
    "outline": _op_outline,
    "majority": _op_majority,
    "minority": _op_minority,
    "threshold": _op_threshold,
    "largest": _op_largest,
    "fill_holes": _op_fill_holes,
    "keep_rings": _op_keep_rings,
    "remove_isolated": _op_remove_isolated,
    "keep_n_largest": _op_keep_n_largest,
    "keep_size_range": _op_keep_size_range,
    "resize": _op_resize,
    "stack": _op_stack,
    "subtract": _op_subtract,
    "phase_tile": _op_phase_tile,
    "phase_tile_row": _op_phase_tile_row,
    "phase_tile_col": _op_phase_tile_col,
}


# ============================================================================
# ============ Operation Order and Families (O2) ============
# ============================================================================

# --- Operation order and families (O2) ---
OP_ORDER_O2 = [
    # Tile family first (agent's best ordering)
    "tile", "tile_masked", "phase_tile_row", "phase_tile_col", "phase_tile", "resize",
    # Core geometry next
    "rot90", "mirror", "transpose", "shift", "crop", "grow", "shrink",
    # Composition / structural
    "stack", "add", "subtract", "largest", "keep_n_largest", "keep_size_range", "remove_isolated", "fill_holes", "keep_rings", "scale", "duplicate",
    # Color / palette last
    "mask", "replacecolor", "swapcolors", "invert", "fill", "outline", "threshold", "majority", "minority",
]

# Build a map for fast ordering
_OP_RANK = {name: i for i, name in enumerate(OP_ORDER_O2)}

# Whitelists for shape-then-color gating
SHAPE_OPS = {
    "tile", "tile_masked", "phase_tile_row", "phase_tile_col", "phase_tile", "resize",
    "rot90", "mirror", "transpose", "shift", "crop", "grow", "shrink",
    "stack", "add", "subtract", "largest", "keep_n_largest", "keep_size_range", "remove_isolated", "fill_holes", "keep_rings", "scale", "duplicate",
}
COLOR_OPS = {
    "mask", "replacecolor", "swapcolors", "invert", "fill", "outline", "threshold", "majority", "minority",
    # permit small finishing shifts too if desired:
    "shift",
}


# ============================================================================
# ============ dsl/program.py ============
# ============================================================================

@dataclass
class Step:
    """A single step in a program: (op_name, args)."""
    op: str
    args: Tuple[Any, ...]

    def __repr__(self):
        return f"Step({self.op}, {self.args})"


@dataclass
class Program:
    """A sequence of Steps."""
    steps: List[Step] = field(default_factory=list)

    def __repr__(self):
        return f"Program({len(self.steps)} steps)"

    def __len__(self):
        return len(self.steps)

    def copy(self):
        return Program([Step(s.op, s.args) for s in self.steps])

    def to_tuple(self):
        """Hashable representation."""
        return tuple((s.op, s.args) for s in self.steps)


def program_to_str(prog: Program) -> str:
    """Human-readable format."""
    lines = []
    for i, step in enumerate(prog.steps):
        lines.append(f"{i+1}. {step.op}{step.args}")
    return "\n".join(lines)


# ============================================================================
# ============ dsl/interpreter.py ============
# ============================================================================

def interpret_program(prog: Program, input_grid: np.ndarray) -> np.ndarray:
    """Execute a program on an input grid, returning the final grid."""
    state = input_grid.copy()
    states = [state.copy()]  # Keep history for register references
    
    for step in prog.steps:
        state = apply_step(state, step, states)
        states.append(state.copy())
    
    return state


def apply_step(grid: np.ndarray, step: Step, states=None) -> np.ndarray:
    """Apply a single step to a grid, with support for grid register references."""
    if states is None:
        states = [grid]
    
    op_name = step.op
    args = step.args

    if op_name not in OP_REGISTRY:
        raise ValueError(f"Unknown op: {op_name}")

    op_fn = OP_REGISTRY[op_name]

    # Handle multi-grid ops with register resolution
    try:
        if step.op in ("add", "subtract"):
            g2 = resolve_grid_arg(args[0], states)
            if g2 is None:
                return grid.copy()
            return op_fn(grid, g2)
        
        if step.op == "stack":
            g2 = resolve_grid_arg(args[0], states)
            if g2 is None:
                return grid.copy()
            axis = int(args[1])
            return op_fn(grid, g2, axis)
        
        # Regular ops
        result = op_fn(grid, *args)
    except Exception as e:
        # If error, return unchanged grid
        result = grid.copy()

    return result


# ============================================================================
# ============ perception/features.py ============
# ============================================================================

def task_features(train_pairs: List[Tuple[np.ndarray, np.ndarray]]) -> Dict[str, Any]:
    """Extract features from train pairs for OCO."""
    if not train_pairs:
        return {
            "n_examples": 0,
            "avg_in_size": (0, 0),
            "avg_out_size": (0, 0),
            "size_stable": False,
            "shape_stable": False,
            "palette_size_in": 0,
            "palette_size_out": 0,
            "complexity": 0.0,
            "aspect_ratio_in": 1.0,
            "aspect_ratio_out": 1.0,
        }

    n = len(train_pairs)
    sizes_in = []
    sizes_out = []
    palettes_in = []
    palettes_out = []

    for (x, y) in train_pairs:
        sizes_in.append(x.shape)
        sizes_out.append(y.shape)
        palettes_in.append(len(np.unique(x)))
        palettes_out.append(len(np.unique(y)))

    avg_in_size = (
        int(np.mean([s[0] for s in sizes_in])),
        int(np.mean([s[1] for s in sizes_in]))
    )
    avg_out_size = (
        int(np.mean([s[0] for s in sizes_out])),
        int(np.mean([s[1] for s in sizes_out]))
    )

    size_stable = all(s == sizes_in[0] for s in sizes_in)
    shape_stable = all(s == sizes_out[0] for s in sizes_out)

    palette_in = int(np.mean(palettes_in))
    palette_out = int(np.mean(palettes_out))

    complexity = (palette_in + palette_out) / 2.0

    aspect_in = avg_in_size[1] / max(avg_in_size[0], 1)
    aspect_out = avg_out_size[1] / max(avg_out_size[0], 1)

    return {
        "n_examples": n,
        "avg_in_size": avg_in_size,
        "avg_out_size": avg_out_size,
        "size_stable": size_stable,
        "shape_stable": shape_stable,
        "palette_size_in": palette_in,
        "palette_size_out": palette_out,
        "complexity": complexity,
        "aspect_ratio_in": aspect_in,
        "aspect_ratio_out": aspect_out,
    }


# ============================================================================
# ============ oco/octonion.py ============
# ============================================================================

def compute_phi(features: Dict[str, Any]) -> np.ndarray:
    """
    Compute 8D octonion embedding φ from task features.
    
    φ = [scale, objectness, palette, geometry, alignment, topology, pattern, composition]
    """
    n = features["n_examples"]
    size_stable = features["size_stable"]
    shape_stable = features["shape_stable"]
    palette_in = features["palette_size_in"]
    palette_out = features["palette_size_out"]
    complexity = features["complexity"]
    aspect_in = features["aspect_ratio_in"]
    aspect_out = features["aspect_ratio_out"]

    # Scale component
    in_h, in_w = features["avg_in_size"]
    out_h, out_w = features["avg_out_size"]
    scale = math.log(max(out_h * out_w, 1)) - math.log(max(in_h * in_w, 1))

    # Objectness (palette difference)
    objectness = palette_out - palette_in

    # Palette (color complexity)
    palette = complexity / 10.0

    # Geometry (aspect ratio change)
    geometry = abs(aspect_out - aspect_in)

    # Alignment (size stability)
    alignment = 1.0 if size_stable else 0.0

    # Topology (shape stability)
    topology = 1.0 if shape_stable else 0.0

    # Pattern (number of examples)
    pattern = n / 10.0

    # Composition (interaction term)
    composition = scale * objectness * 0.1

    phi = np.array([
        scale,
        objectness,
        palette,
        geometry,
        alignment,
        topology,
        pattern,
        composition
    ], dtype=np.float32)

    return phi


def phi_to_family(phi: np.ndarray) -> str:
    """Classify task family from φ."""
    if phi is None or len(phi) < 8:
        return "unknown"

    core = np.array(phi[:8], dtype=np.float32)
    abs_phi = np.abs(core)
    idx = np.argmax(abs_phi)

    families = [
        "scale",
        "objectness",
        "palette",
        "geometry",
        "alignment",
        "topology",
        "pattern",
        "composition"
    ]

    return families[idx]


# ============================================================================
# ============ oco/associator.py ============
# ============================================================================

def compute_program_tension(prog: Program, phi: np.ndarray) -> float:
    """
    Compute tension T_prog(φ) between program structure and task embedding.
    
    T_prog = Σ_i |op_i ⊗ φ|
    
    This measures how well the program's operations align with the task's
    octonion structure.
    """
    if phi is None or len(phi) != 8:
        return 0.0

    tension = 0.0
    for step in prog.steps:
        op_vec = op_to_vector(step.op)
        # Simple dot product as pseudo-octonion multiplication
        tension += abs(np.dot(op_vec, phi))

    return tension


def compute_slice_tension(state: np.ndarray, phi: np.ndarray) -> float:
    """
    Compute tension T_slice(φ) between current state and task embedding.
    
    T_slice = |state_features ⊗ φ|
    
    This measures how well the current state aligns with the task's
    expected structure.
    """
    if phi is None or len(phi) != 8:
        return 0.0

    state_vec = state_to_vector(state)
    tension = abs(np.dot(state_vec, phi))

    return tension


def op_to_vector(op_name: str) -> np.ndarray:
    """Map operation to 8D vector for tension computation."""
    # Phase-tiling family gets scale + pattern axes to avoid zero-tension free ride
    PHASE_TILE_VEC = np.array([1, 0, 0, 0, 0, 0, 1, 0], dtype=np.float32)
    
    op_map = {
        "fill": [0, 0, 1, 0, 0, 0, 0, 0],
        "mirror": [0, 0, 0, 1, 1, 0, 0, 0],
        "rot90": [0, 0, 0, 1, 0, 0, 0, 0],
        "transpose": [0, 0, 0, 1, 0, 0, 0, 0],
        "shrink": [1, 0, 0, 0, 0, 0, 0, 0],
        "grow": [1, 0, 0, 0, 0, 0, 0, 0],
        "add": [0, 1, 0, 0, 0, 0, 0, 1],
        "mask": [0, 0, 1, 0, 0, 0, 0, 0],
        "invert": [0, 0, 1, 0, 0, 0, 0, 0],
        "duplicate": [0, 0, 0, 0, 0, 0, 0, 0],
        "scale": [1, 0, 0, 0, 0, 0, 0, 0],
        "tile": PHASE_TILE_VEC.tolist(),
        "tile_masked": PHASE_TILE_VEC.tolist(),
        "phase_tile": PHASE_TILE_VEC.tolist(),
        "phase_tile_row": PHASE_TILE_VEC.tolist(),
        "phase_tile_col": PHASE_TILE_VEC.tolist(),
        "crop": [1, 0, 0, 0, 0, 0, 0, 0],
        "shift": [0, 0, 0, 0, 1, 0, 0, 0],
        "replacecolor": [0, 0, 1, 0, 0, 0, 0, 0],
        "swapcolors": [0, 0, 1, 0, 0, 0, 0, 0],
        "outline": [0, 1, 0, 0, 0, 1, 0, 0],
        "majority": [0, 0, 1, 0, 0, 0, 0, 0],
        "minority": [0, 0, 1, 0, 0, 0, 0, 0],
        "threshold": [0, 0, 1, 0, 0, 0, 0, 0],
        "largest": [0, 1, 0, 0, 0, 1, 0, 0],
        "fill_holes": [0, 0, 0, 0, 0, 1, 0, 0],
        "keep_rings": [0, 1, 0, 0, 0, 1, 0, 0],
        "keep_n_largest": [0, 1, 0, 0, 0, 1, 0, 0],
        "keep_size_range": [0, 1, 0, 0, 0, 1, 0, 0],
        "resize": [1, 0, 0, 0, 0, 0, 0, 0],
        "stack": [0, 0, 0, 0, 0, 0, 0, 1],
        "subtract": [0, 1, 0, 0, 0, 0, 0, 0],
    }

    vec = op_map.get(op_name, [0, 0, 0, 0, 0, 0, 0, 0])
    return np.array(vec, dtype=np.float32)


def state_to_vector(state: np.ndarray) -> np.ndarray:
    """Map grid state to 8D vector for tension computation."""
    h, w = state.shape
    size = math.log(max(h * w, 1))
    n_colors = len(np.unique(state))
    aspect = w / max(h, 1)
    density = np.count_nonzero(state) / max(state.size, 1)

    vec = np.array([
        size,
        n_colors,
        density,
        aspect,
        0.0,  # alignment (computed elsewhere)
        0.0,  # topology (computed elsewhere)
        0.0,  # pattern (computed elsewhere)
        0.0,  # composition (computed elsewhere)
    ], dtype=np.float32)

    return vec


# ============================================================================
# ============ oco/cost.py ============
# ============================================================================

@dataclass
class SearchSettings:
    """Configuration for beam search with OCO."""
    beam_width: int = 128
    max_depth: int = 10
    max_seconds: float = 3.0
    lambda_len: float = 0.20
    lambda1: float = 0.30  # program tension weight
    lambda2: float = 0.20  # slice tension weight
    slice_guard_thresh: float = 0.80
    allow_offslice_early: bool = False
    public_mode: bool = False
    log_every: int = 200
    seed: int = 1337
    _disable_rotation: bool = False
    max_train_pairs_for_beam: int = 2
    use_meta_controller: bool = False
    always_two_attempts: bool = False  # wrapper now manages alternates; keep default conservative
    test_palette_policy: str = "second_only_guarded"
    allow_finishers_on_masked: bool = False
    div_lambda: float = 0.20
    iou_cap: float = 0.97
    max_bounces: int = -1
    _hfp_rho: Optional[float] = None
    _hfp_rho_smoothed: Optional[float] = None
    _hfp_ready: bool = False


def compute_cost(
    prog: Program,
    input_grid: np.ndarray,
    target_grid: np.ndarray,
    phi: np.ndarray,
    settings: SearchSettings
) -> float:
    """
    OCO-augmented cost function:
    
    C = C_match + λ_len*L + λ1*T_prog + λ2*T_slice
    
    Where:
    - C_match: Pixel mismatch cost
    - L: Program length
    - T_prog: Program tension
    - T_slice: Slice tension
    
    Shape-stage bias: When shape matches, downweight both λ1 and λ2, add accuracy bonus.
    """
    # Execute program
    try:
        pred = interpret_program(prog, input_grid)
    except Exception:
        return 1e9

    # Match cost
    if pred.shape != target_grid.shape:
        c_match = 1.0
    else:
        diff = np.sum(pred != target_grid)
        c_match = diff / max(target_grid.size, 1)

    shape_same = (pred.shape == target_grid.shape)
    acc = (1.0 - c_match) if shape_same else 0.0

    # Keep original lambda1, lambda2 first
    lam1 = settings.lambda1
    lam2 = settings.lambda2

    # AFTER shape-match: reduce both tensions + small pixel-accuracy bonus
    if shape_same:
        lam1 *= 0.50   # downweight program tension when shape matches
        lam2 *= 0.30   # downweight slice tension when shape matches
        beta = 0.05    # pixel-accuracy bonus
    else:
        beta = 0.0

    # Length cost
    c_len = len(prog) * settings.lambda_len

    # OCO costs
    t_prog = compute_program_tension(prog, phi)
    t_slice = compute_slice_tension(pred, phi)

    c_oco = lam1 * t_prog + lam2 * t_slice

    total = c_match + c_len + c_oco - beta * acc

    # shape-stage: tiny prior for topology selection ops (mirrors tile_masked prior)
    if pred.shape == target_grid.shape and len(prog) > 0:
        last = prog.steps[-1].op
        if last in ("keep_rings", "fill_holes"):
            total -= 0.01

    return float(total)


# ============================================================================
# ============ search/beam.py ============
# ============================================================================

@dataclass
class Candidate:
    """A candidate program with its cost."""
    program: Program
    cost: float
    depth: int = 0

    def __lt__(self, other):
        return self.cost < other.cost


def _centroid_nonzero(a):
    import numpy as np
    ys, xs = np.where(a != 0)
    if ys.size == 0: return None
    return int(np.round(ys.mean())), int(np.round(xs.mean()))


def _quick_shape_candidates(input_grid, target_grid):
    """
    Try a small set of single-step shape transforms; return up to top-3 seeds
    (Program, acc) ranked by pixel accuracy (shape must match).
    Targets sparse tilings like border/cross and basic phase tilings.
    """
    H, W = input_grid.shape
    Ho, Wo = target_grid.shape
    cand = [Program([Step("resize", (Ho, Wo))])]

    if H and W and (Ho % H == 0) and (Wo % W == 0):
        ky, kx = Ho // H, Wo // W
        cand += [
            Program([Step("tile", (ky, kx))]),
            Program([Step("tile_masked", (ky, kx, 0))]),  # cross
            Program([Step("tile_masked", (ky, kx, 1))]),  # border
            Program([Step("phase_tile_row", (ky, kx, 0))]),
            Program([Step("phase_tile_row", (ky, kx, 1))]),
            Program([Step("phase_tile_row", (ky, kx, 2))]),
            Program([Step("phase_tile_col", (ky, kx, 0))]),
            Program([Step("phase_tile_col", (ky, kx, 1))]),
            Program([Step("phase_tile_col", (ky, kx, 2))]),
            Program([Step("phase_tile",     (ky, kx, 0))]),
            Program([Step("phase_tile",     (ky, kx, 1))]),
            Program([Step("phase_tile",     (ky, kx, 2))]),
        ]

    scored, seen = [], set()
    for prog in cand:
        try:
            pred = interpret_program(prog, input_grid)
        except Exception:
            continue
        if pred.shape != target_grid.shape:
            continue
        acc = (pred == target_grid).mean()
        sig = prog.to_tuple()
        if sig not in seen:
            scored.append((float(acc), prog)); seen.add(sig)
    scored.sort(key=lambda t: (-t[0], len(t[1])))
    return scored[:3]


def _consensus_one_step_candidate(train_pairs):
    """
    Quick shape-consensus sweep across train pairs.
    Returns (best_prog_or_None, best_mean_acc) for a small set of one-step programs
    evaluated on ALL train pairs. Only shape-matched predictions are scored.
    """
    try:
        x0, y0 = train_pairs[0]
        H, W = x0.shape
        Ho, Wo = y0.shape
    except Exception:
        return None, 0.0

    cands = [Program([Step("resize", (Ho, Wo))])]

    if H and W and (Ho % H == 0) and (Wo % W == 0):
        ky, kx = Ho // H, Wo // W
        cands += [
            Program([Step("tile",        (ky, kx))]),
            Program([Step("tile_masked", (ky, kx, 0))]),  # cross
            Program([Step("tile_masked", (ky, kx, 1))]),  # border
            Program([Step("phase_tile_row", (ky, kx, 0))]),
            Program([Step("phase_tile_row", (ky, kx, 1))]),
            Program([Step("phase_tile_row", (ky, kx, 2))]),
            Program([Step("phase_tile_col", (ky, kx, 0))]),
            Program([Step("phase_tile_col", (ky, kx, 1))]),
            Program([Step("phase_tile_col", (ky, kx, 2))]),
            Program([Step("phase_tile",     (ky, kx, 0))]),
            Program([Step("phase_tile",     (ky, kx, 1))]),
            Program([Step("phase_tile",     (ky, kx, 2))]),
        ]

    # Dedup by signature
    uniq, seen = [], set()
    for p in cands:
        sig = p.to_tuple()
        if sig not in seen:
            uniq.append(p); seen.add(sig)

    # Score each candidate on all train pairs (pixel acc if shapes match, else 0)
    best_prog, best_mean = None, 0.0
    for prog in uniq:
        accs = []
        for (xi, yi) in train_pairs:
            try:
                pred = interpret_program(prog, xi)
            except Exception:
                accs.append(0.0); continue
            if pred.shape != yi.shape:
                accs.append(0.0)
            else:
                accs.append(float((pred == yi).mean()))
        if accs:
            mean_acc = float(sum(accs) / len(accs))
            if mean_acc > best_mean:
                best_mean, best_prog = mean_acc, prog

    return best_prog, best_mean


def build_synth_context(input_grid, target_grid):
    import numpy as np
    Hin, Win = input_grid.shape
    Hout, Wout = target_grid.shape
    pal_in = sorted(np.unique(input_grid).tolist())
    pal_out = sorted(np.unique(target_grid).tolist())
    return {
        "in_shape": (Hin, Win),
        "out_shape": (Hout, Wout),
        "palette_in": pal_in,
        "palette_out": pal_out,
        "centroid_in": _centroid_nonzero(input_grid),
        "centroid_out": _centroid_nonzero(target_grid),
    }


def _int_space_for(op_name, idx, ctx):
    # idx: index among non-grid integer params
    if op_name == "rot90":
        return [1, 2, 3]
    if op_name == "mirror":
        return [0, 1]  # axis: 0=flipud, 1=fliplr
    if op_name in ("fill", "mask", "outline", "threshold"):
        vals = (ctx.get("palette_out") or []) + (ctx.get("palette_in") or [])
        vals = [v for v in dict.fromkeys(vals) if 0 <= v <= 9]
        return vals or list(range(10))
    if op_name == "resize":
        Hout, Wout = ctx["out_shape"]
        return [(Hout, Wout)]
    if op_name == "tile":
        Hin, Win = ctx["in_shape"]; Hout, Wout = ctx["out_shape"]
        pairs = []
        if Hin and Win and Hout % Hin == 0 and Wout % Win == 0:
            ky, kx = Hout // Hin, Wout // Win
            # ✅ Hard-ban identity tiling
            if ky > 1 or kx > 1:
                pairs.append((ky, kx))
        # Keep a single useful fallback (not identity)
        if not pairs:
            pairs = [(2, 2), (3, 3)]  # choose one or both; neither is (1,1)
        return pairs
    if op_name == "tile_masked":
        Hin, Win = ctx["in_shape"]; Hout, Wout = ctx["out_shape"]
        triples = []
        if Hin and Win and (Hout % Hin == 0) and (Wout % Win == 0):
            ky, kx = Hout // Hin, Wout // Win
            # never enumerate identity
            if not (ky == 1 and kx == 1):
                # v1: only cross(0) and border(1) to keep search tight
                for mode in (0, 1):
                    triples.append((ky, kx, mode))
                # leave diag modes out of enumeration for now
        return triples or []
    if op_name in ("phase_tile", "phase_tile_row", "phase_tile_col"):
        # Return (ky, kx, mode) triples
        Hin, Win = ctx["in_shape"]; Hout, Wout = ctx["out_shape"]
        triples = []
        if Hin and Win and Hout % Hin == 0 and Wout % Win == 0:
            ky, kx = Hout // Hin, Wout // Win
            # Try all three modes
            triples.extend([(ky, kx, 0), (ky, kx, 1), (ky, kx, 2)])
        if not triples:
            # Fallback
            triples = [(2, 2, 0), (3, 3, 0)]
        return triples
    if op_name == "shift":
        base = list(range(-3, 4))
        dy = dx = None
        ci, co = ctx.get("centroid_in"), ctx.get("centroid_out")
        if ci is not None and co is not None:
            dy = int(co[0] - ci[0]); dx = int(co[1] - ci[1])
        if idx == 0:
            return ([dy] + [v for v in base if v != dy]) if dy is not None else base
        if idx == 1:
            return ([dx] + [v for v in base if v != dx]) if dx is not None else base
        return base
    if op_name in ("replacecolor", "swapcolors"):
        pals = (ctx.get("palette_in") or []) + (ctx.get("palette_out") or [])
        pals = [c for c in dict.fromkeys(pals) if 0 <= c <= 9][:6]
        pairs = []
        for i, a in enumerate(pals):
            for b in pals[i+1:]:
                pairs.append((a, b))
        return pairs or [(1, 2), (2, 3), (3, 4)]
    if op_name in ("fill_holes", "keep_rings"):
        return [()]
    if op_name == "remove_isolated":
        return [(1,), (2,), (3,)]
    if op_name == "keep_n_largest":
        return [(1,), (2,), (3,)]
    if op_name == "keep_size_range":
        Hout, Wout = ctx["out_shape"]
        A = Hout * Wout if Hout and Wout else 0
        small  = max(1, A // 100)   # ~1%
        medium = max(2, A // 40)    # ~2.5%
        big    = max(3, A // 20)    # ~5%
        return [(small, medium), (medium, big)]
    return list(range(10))


def beam_search_one_pair(
    input_grid: np.ndarray,
    target_grid: np.ndarray,
    phi: np.ndarray,
    settings: SearchSettings,
    logger: Optional[Any] = None
) -> Optional[Program]:
    """
    OCO-guided beam search for a single train pair.
    
    Returns the best program found, or None if time/depth exceeded.
    """
    start_time = time.time()
    policy_prior = getattr(settings, "_policy_prior", _DEFAULT_POLICY_PRIOR) or _DEFAULT_POLICY_PRIOR
    trace_buffer = getattr(settings, "_trace_buffer", None) if getattr(settings, "_trace_ops", False) else None
    family_hint = phi_to_family(phi)
    divs_hint = _divisible_shape(input_grid.shape, target_grid.shape)

    # Early-abort tracking (no-shape)
    no_shape_seen = True           # flip to False once any successor matches target shape
    base_seconds = settings.max_seconds

    # Early-abort tracking (no-improvement)
    last_improve_t = time.time()
    best_cost_seen = 1e9

    # Build context for op-specific argument generation
    ctx = build_synth_context(input_grid, target_grid)

    # === Meta-seed discovery ===
    meta = _quick_shape_candidates(input_grid, target_grid)
    seeds = [prog for (acc, prog) in meta]

    # --- φ-aware topo gating for seeds / refiners ---
    dholes_g = dcomps_g = dholes_pc = dcomps_pc = 0.0
    # v2.8.7: read topology from side-channel hint (works for 8-D φ)
    topo_hint = getattr(settings, "_topo_hint", None)
    if topo_hint and len(topo_hint) == 4:
        dholes_g, dcomps_g, dholes_pc, dcomps_pc = map(float, topo_hint)

    try:
        if dholes_g > 0.0 or dholes_pc > 0.0:
            seeds.insert(0, Program([Step("keep_rings", ())]))
        if dholes_g < 0.0 or dholes_pc < 0.0:
            seeds.append(Program([Step("fill_holes", ())]))
    except Exception:
        pass

    # if shape is identical sizes and accuracy is high-ish, try denoising early
    try:
        if target_grid.size == input_grid.size:
            seeds.append(Program([Step("remove_isolated", (1,))]))
            seeds.append(Program([Step("remove_isolated", (2,))]))
    except Exception:
        pass

    # Early-latch: if any seed already strong, prefer it
    best_so_far, best_cost = None, 1e9
    for acc, prog in meta:
        if acc >= 0.80:  # strong shape match
            # try palette immediately; if exact, return
            mapping = _palette_map_from_train_pairs([(input_grid, target_grid)])
            if mapping:
                pred = interpret_program(prog, input_grid)
                p2 = np.array(_apply_palette_map_ll(pred.tolist(), mapping), dtype=np.int32)
                if p2.shape == target_grid.shape and np.array_equal(p2, target_grid):
                    return prog
            # not exact: make the beam expand this branch first
            best_so_far = prog
            best_cost   = 1.0 - acc
            break  # one is enough

    # Initialize beam with scored seeds (plus an empty program fallback)
    beam = []
    visited = set()

    def _seed_cost(p):
        try:
            return compute_cost(p, input_grid, target_grid, phi, settings)
        except Exception:
            return 1e9

    # add deduped seeds
    sig_seen = set()
    for prog in seeds:
        sig = prog.to_tuple()
        if sig in sig_seen: 
            continue
        sig_seen.add(sig)
        beam.append(Candidate(prog, _seed_cost(prog), depth=len(prog)))
        visited.add(sig)

    # always include empty program fallback
    empty_sig = Program([]).to_tuple()
    if empty_sig not in sig_seen:
        beam.append(Candidate(Program([]), cost=1e9, depth=0))
        visited.add(empty_sig)

    # if early-latch found a strong seed, bias its cost so it's explored first
    if best_so_far is not None:
        sig = best_so_far.to_tuple()
        if sig not in sig_seen:
            beam.append(Candidate(best_so_far, best_cost, depth=len(best_so_far)))
            visited.add(sig)

    # prune to beam width right away
    beam.sort()
    beam = beam[:settings.beam_width]

    # Track best solution found
    if best_so_far is None:
        best_so_far = None
        best_cost = 1e9

    # State cache for in-beam gating
    state_cache: Dict[Tuple, np.ndarray] = {}
    def _run(prog: Program) -> np.ndarray:
        sig = prog.to_tuple()
        if sig in state_cache:
            return state_cache[sig]
        out = interpret_program(prog, input_grid)
        state_cache[sig] = out
        return out

    iteration = 0

    while beam:
        iteration += 1

        if iteration >= 2 and not getattr(settings, "_hfp_ready", False):
            settings._hfp_ready = True

        stall_window, abort_after = _adjust_abort_windows(settings, base_seconds, not no_shape_seen)

        # Check timeout
        if time.time() - start_time > settings.max_seconds:
            if logger:
                logger.log_iteration(iteration, len(beam), best_cost)
            # Return best-so-far immediately on hard timeout
            if best_so_far is not None:
                return best_so_far
            # Fallback if nothing found
            return Program([Step("resize", (target_grid.shape[0], target_grid.shape[1]))])

        # No-improvement early-abort check
        if time.time() - last_improve_t > stall_window:
            if logger:
                print(f"[early-abort] No cost improvement for {stall_window:.1f}s, returning best")
            # Return best-so-far or resize fallback
            if best_so_far is not None:
                return best_so_far
            # Safe fallback: resize to target shape
            return Program([Step("resize", (target_grid.shape[0], target_grid.shape[1]))])

        # No-shape early-abort check: if ⅓ budget spent with no shape progress
        if time.time() - start_time > abort_after and no_shape_seen:
            if logger:
                print(f"[early-abort] No shape match after {abort_after:.1f}s, returning fallback")
            # Return best-so-far or resize fallback
            if best_so_far is not None:
                return best_so_far
            # Safe fallback: resize to target shape
            return Program([Step("resize", (target_grid.shape[0], target_grid.shape[1]))])

        # Get candidate with lowest cost
        current = beam.pop(0)

        # Track improvement for no-improvement abort
        if current.cost < best_cost_seen - 1e-6:
            best_cost_seen = current.cost
            last_improve_t = time.time()

        # Check if solved
        if current.cost < 0.01:
            best_so_far = current.program
            best_cost = current.cost
            if logger:
                logger.log_iteration(iteration, len(beam), best_cost)
            break

        # Track best
        if current.cost < best_cost:
            best_cost = current.cost
            best_so_far = current.program

        # Check depth limit
        if current.depth >= settings.max_depth:
            continue

        # Compute current state for gating
        try:
            cur_state = _run(current.program)
            # Check if we've seen shape match at this level
            if cur_state.shape == target_grid.shape:
                no_shape_seen = False
        except Exception:
            cur_state = input_grid

        # --- v2.8.3 Topology-aware refiner: keep_rings post-alignment ---
        dholes_g = dcomps_g = dholes_pc = dcomps_pc = 0.0
        # v2.8.7: read topology from side-channel hint (works for 8-D φ)
        topo_hint = getattr(settings, "_topo_hint", None)
        if topo_hint and len(topo_hint) == 4:
            dholes_g, dcomps_g, dholes_pc, dcomps_pc = map(float, topo_hint)

        acc_proxy = 0.0
        try:
            if cur_state.shape == target_grid.shape:
                acc_proxy = float((cur_state == target_grid).mean())
        except Exception:
            acc_proxy = 0.0

        if (
            cur_state.shape == target_grid.shape
            and (dholes_g > 0.0 or dholes_pc > 0.0)
            and acc_proxy >= 0.55
            and current.depth < 3
        ):
            try:
                ring_prog = current.program.copy()
                ring_prog.steps.append(Step("keep_rings", ()))
                sig = ring_prog.to_tuple()
                if sig not in visited:
                    cost_ring = compute_cost(ring_prog, input_grid, target_grid, phi, settings)
                    beam.append(Candidate(ring_prog, cost_ring, current.depth + 1))
                    visited.add(sig)
            except Exception:
                pass
        # --- end v2.8.3 insert ---

        # --- v2.8.4 Topology-aware refiner: fill_holes post-alignment ---
        if (
            cur_state.shape == target_grid.shape
            and (dholes_g < 0.0 or dholes_pc < 0.0)
            and acc_proxy >= 0.55
            and current.depth < 3
        ):
            try:
                fill_prog = current.program.copy()
                fill_prog.steps.append(Step("fill_holes", ()))
                sig = fill_prog.to_tuple()
                if sig not in visited:
                    cost_fill = compute_cost(fill_prog, input_grid, target_grid, phi, settings)
                    beam.append(Candidate(fill_prog, cost_fill, current.depth + 1))
                    visited.add(sig)
            except Exception:
                pass
        # --- end v2.8.4 insert ---

        # --- Conditional Smart Refiner (one extra post-shape step) ---
        try:
            if cur_state.shape == target_grid.shape and current.depth < 2:
                acc, _, _ = _current_acc_state(cur_state, target_grid)
                if acc >= 0.60:
                    # 1) alignment-guided shift (best (dy,dx) from existing helper)
                    try:
                        best = _propose_alignment_deltas(cur_state, target_grid, window=3)[:1]
                    except Exception:
                        best = []
                    for (dy, dx) in best:
                        if dy or dx:
                            prog_shift = current.program.copy()
                            prog_shift.steps.append(Step("shift", (int(dy), int(dx))))
                            sig = prog_shift.to_tuple()
                            if sig not in visited:
                                cost = compute_cost(prog_shift, input_grid, target_grid, phi, settings)
                                beam.append(Candidate(prog_shift, cost, current.depth + 1))
                                visited.add(sig)

                    # 2) alternate tile_masked mode (swap cross/border once)
                    if current.program.steps and current.program.steps[-1].op == "tile_masked":
                        ky, kx, m = map(int, current.program.steps[-1].args)
                        if m in (0, 1):
                            alt = 1 - m
                            prog_alt = current.program.copy()
                            prog_alt.steps.append(Step("tile_masked", (ky, kx, alt)))
                            sig = prog_alt.to_tuple()
                            if sig not in visited:
                                cost = compute_cost(prog_alt, input_grid, target_grid, phi, settings)
                                beam.append(Candidate(prog_alt, cost, current.depth + 1))
                                visited.add(sig)

                    # 3) optional mirror when fairly close
                    if acc >= 0.70 and current.program.steps:
                        axis = 1  # horizontal mirror default
                        prog_m = current.program.copy()
                        prog_m.steps.append(Step("mirror", (axis,)))
                        sig = prog_m.to_tuple()
                        if sig not in visited:
                            cost = compute_cost(prog_m, input_grid, target_grid, phi, settings)
                            beam.append(Candidate(prog_m, cost, current.depth + 1))
                            visited.add(sig)

                    # NOTE: we rely on beam pruning to cap to beam_width; refiners add ≤3 branches
        except Exception:
            pass

        # Choose allowed ops based on shape match
        if cur_state.shape == target_grid.shape:
            allowed = COLOR_OPS
        else:
            allowed = SHAPE_OPS

        # Generate successors with gating + O2 ordering
        successors = generate_successors(current.program, ctx, allowed=allowed)

        # Score each successor
        for succ_prog in successors:
            # Skip if visited
            prog_sig = succ_prog.to_tuple()
            if prog_sig in visited:
                continue
            visited.add(prog_sig)

            op_name = succ_prog.steps[-1].op if succ_prog.steps else ""

            # Compute cost
            cost = compute_cost(succ_prog, input_grid, target_grid, phi, settings)

            # Check successor output for tie-break, early exit, and shape tracking
            pred = None
            try:
                pred = interpret_program(succ_prog, input_grid)

                # Track shape match
                if pred.shape == target_grid.shape:
                    no_shape_seen = False

                # Tiny tie-break for tile_masked when shape matches
                if pred.shape == target_grid.shape:
                    last = succ_prog.steps[-1].op if succ_prog.steps else None
                    if last == "tile_masked":
                        cost -= 0.02

                    # Early exit if exact match
                    if np.array_equal(pred, target_grid):
                        return succ_prog
            except Exception:
                pass

            # Apply policy prior as an additive bonus in score space (subtract from cost)
            try:
                grid_shape = pred.shape if isinstance(pred, np.ndarray) else (None, None)
                ctx_prior = {
                    "phi": phi,
                    "grid": pred,
                    "grid_shape": grid_shape,
                    "divs": divs_hint if divs_hint is not None else (),
                    "family": family_hint,
                }
                prior_logit = policy_prior.op_logit(op_name, ctx_prior)
                cost -= 0.05 * prior_logit
            except Exception:
                pass

            if trace_buffer is not None and op_name:
                try:
                    delta_cost = float(cost - current.cost)
                except Exception:
                    delta_cost = float(cost)
                trace_buffer.append((op_name, delta_cost))

            # Add to beam
            new_cand = Candidate(succ_prog, cost, current.depth + 1)
            beam.append(new_cand)

        # Sort beam by cost
        beam.sort()

        # Prune to beam width
        beam = beam[:settings.beam_width]

        # Periodic logging
        if logger and iteration % settings.log_every == 0:
            logger.log_iteration(iteration, len(beam), best_cost)

    if logger:
        logger.log_iteration(iteration, len(beam), best_cost)

    return best_so_far


def generate_successors(prog: Program, ctx, allowed: Optional[Set[str]] = None) -> List[Program]:
    """Generate all valid single-step extensions of a program, filtered & ordered."""
    # Order operations by OP_ORDER_O2, and apply optional whitelist
    ordered = sorted(OP_NAMES_BASIC, key=lambda t: _OP_RANK.get(t[0], 10_000))
    successors = []
    for op_name, arity, param_types in ordered:
        if allowed is not None and op_name not in allowed:
            continue
        arg_combos = enumerate_args(op_name, param_types, ctx)
        for args in arg_combos:
            new_prog = prog.copy()
            new_prog.steps.append(Step(op_name, args))
            successors.append(new_prog)
    return successors


def enumerate_args(op_name: str, param_types: List[str], ctx) -> List[Tuple[Any, ...]]:
    """Enumerate concrete arguments for an operation with context awareness."""
    if not param_types:
        return [()]

    # Convert param_types: first 'grid' is implicit (current state), 
    # subsequent 'grid' become 'grid_ref' for register references
    filtered = []
    seen_grid = False
    for t in param_types:
        if t == "grid":
            if not seen_grid:
                seen_grid = True
                continue  # Skip first grid (current state)
            filtered.append("grid_ref")  # Subsequent grids are references
        else:
            filtered.append(t)
    
    if not filtered:
        return [()]

    # Bundled int-pair/triple ops
    if op_name == "resize":
        return [(h, w) for (h, w) in _int_space_for("resize", 0, ctx)]
    if op_name == "tile":
        return [(v, h) for (v, h) in _int_space_for("tile", 0, ctx)]
    if op_name == "tile_masked":
        return [(ky, kx, mode) for (ky, kx, mode) in _int_space_for("tile_masked", 0, ctx)]
    if op_name == "phase_tile":
        return [(ky, kx, mode) for (ky, kx, mode) in _int_space_for("phase_tile", 0, ctx)]
    if op_name == "phase_tile_row":
        return [(ky, kx, mode) for (ky, kx, mode) in _int_space_for("phase_tile_row", 0, ctx)]
    if op_name == "phase_tile_col":
        return [(ky, kx, mode) for (ky, kx, mode) in _int_space_for("phase_tile_col", 0, ctx)]
    if op_name in ("replacecolor", "swapcolors"):
        return [(a, b) for (a, b) in _int_space_for(op_name, 0, ctx)]
    if op_name == "keep_n_largest":
        return [(n,) for (n,) in _int_space_for("keep_n_largest", 0, ctx)]
    if op_name == "keep_size_range":
        return [(amin, amax) for (amin, amax) in _int_space_for("keep_size_range", 0, ctx)]

    # Generic cartesian product across per-position spaces
    spaces = []
    for i, t in enumerate(filtered):
        if t == "grid_ref":
            spaces.append([REG_PREV, REG_PREV2])
        elif t == "int":
            spaces.append(_int_space_for(op_name, i, ctx))
        else:
            spaces.append([0])

    combos = [()]
    for space in spaces:
        combos = [c + (v,) for c in combos for v in space]
    return combos


# ============================================================================
# ============ controller/modes.py ============
# ============================================================================

@dataclass
class ControllerState:
    """State of the OCO controller."""
    mode: str = "observer"  # observer, navigator, explorer
    rotation_count: int = 0
    last_rotation_cost: float = 1e9


# ============================================================================
# ============ controller/meta.py (opt-in meta-controller) ============
# ============================================================================


class _UCB:
    def __init__(self, n_arms: int):
        self.n = [0] * n_arms
        self.r = [0.0] * n_arms
        self.t = 0

    def select(self) -> int:
        self.t += 1
        for i, c in enumerate(self.n):
            if c == 0:
                return i
        import math

        avg = [self.r[i] / self.n[i] for i in range(len(self.n))]
        bonus = [math.sqrt(2 * math.log(self.t) / self.n[i]) for i in range(len(self.n))]
        ucb = [avg[i] + bonus[i] for i in range(len(self.n))]
        return int(np.argmax(ucb))

    def update(self, arm: int, reward: float):
        self.n[arm] += 1
        self.r[arm] += reward


class MetaControllerUCB:
    """
    Per-φ-family UCB over a few fixed bundles (beam, λ1, λ2, K).
    Reward is provided by caller (e.g., acc/sec).
    """

    def __init__(self):
        # bundles are minimal & stable; adjust offline if needed
        self.bundle_map = {
            "scale": [
                {"beam_width": 48, "lambda1": 0.25, "lambda2": 0.10, "max_train_pairs_for_beam": 1},
                {"beam_width": 96, "lambda1": 0.30, "lambda2": 0.20, "max_train_pairs_for_beam": 2},
                {"beam_width": 128, "lambda1": 0.30, "lambda2": 0.25, "max_train_pairs_for_beam": 2},
            ],
            "objectness": [
                {"beam_width": 48, "lambda1": 0.35, "lambda2": 0.15, "max_train_pairs_for_beam": 1},
                {"beam_width": 128, "lambda1": 0.20, "lambda2": 0.10, "max_train_pairs_for_beam": 2},
            ],
            "palette": [
                {"beam_width": 64, "lambda1": 0.20, "lambda2": 0.15, "max_train_pairs_for_beam": 2},
                {"beam_width": 96, "lambda1": 0.10, "lambda2": 0.25, "max_train_pairs_for_beam": 2},
            ],
            "alignment": [
                {"beam_width": 64, "lambda1": 0.25, "lambda2": 0.30, "max_train_pairs_for_beam": 1},
                {"beam_width": 64, "lambda1": 0.30, "lambda2": 0.25, "max_train_pairs_for_beam": 1},
            ],
            "geometry": [
                {"beam_width": 48, "lambda1": 0.25, "lambda2": 0.15, "max_train_pairs_for_beam": 1},
                {"beam_width": 128, "lambda1": 0.20, "lambda2": 0.10, "max_train_pairs_for_beam": 1},
            ],
            "pattern": [
                {"beam_width": 128, "lambda1": 0.25, "lambda2": 0.25, "max_train_pairs_for_beam": 2},
                {"beam_width": 128, "lambda1": 0.20, "lambda2": 0.10, "max_train_pairs_for_beam": 2},
            ],
            "default": [
                {"beam_width": 64, "lambda1": 0.30, "lambda2": 0.20, "max_train_pairs_for_beam": 2},
            ],
        }
        self.bandits = {}

    def _bundles_for(self, fam: str):
        return self.bundle_map.get(fam, self.bundle_map["default"])

    def select(self, fam: str):
        bundles = self._bundles_for(fam)
        if fam not in self.bandits:
            self.bandits[fam] = _UCB(len(bundles))
        arm = self.bandits[fam].select()
        return arm, bundles[arm]

    def update(self, fam: str, arm: int, reward: float):
        self.bandits[fam].update(arm, reward)


def should_rotate(state: ControllerState, current_cost: float, settings: SearchSettings) -> bool:
    """Decide if controller should rotate to next mode."""
    if settings._disable_rotation:
        return False

    # Rotate if cost isn't improving
    if current_cost >= state.last_rotation_cost * 0.98:
        return True

    # Rotate after 3 attempts in same mode
    if state.rotation_count >= 3:
        return True

    return False


def rotate_mode(state: ControllerState) -> str:
    """Rotate to next controller mode."""
    modes = ["observer", "navigator", "explorer"]
    idx = modes.index(state.mode)
    next_idx = (idx + 1) % len(modes)
    return modes[next_idx]


def apply_mode_bias(settings: SearchSettings, mode: str) -> SearchSettings:
    """Adjust search settings based on controller mode."""
    from dataclasses import replace

    if mode == "observer":
        # Conservative: low beam, high OCO
        return replace(settings, beam_width=64, lambda1=0.40, lambda2=0.30)
    elif mode == "navigator":
        # Balanced: default settings
        return settings
    elif mode == "explorer":
        # Aggressive: high beam, low OCO
        return replace(settings, beam_width=384, lambda1=0.05, lambda2=0.05)
    else:
        return settings


# ============================================================================
# ============ io/tasks.py ============
# ============================================================================

def load_tasks_from_dir(tasks_dir: str) -> List[Tuple[str, dict]]:
    """
    Load ARC tasks from a directory.

    Supports:
      (1) dict-of-tasks: {"id": {"train":[...], "test":[...]}, ...}
      (2) single dict:   {"train":[...], "test":[...]}
      (3) list of dicts: [{"train":...,"test":...}, ...]
      (4) optional wrappers: {"challenges":[...]}, {"training":[...]}, {"evaluation":[...]}, {"test":[...]}
    """
    import glob, json, os
    tasks: List[Tuple[str, dict]] = []

    def normalize(obj, base_id):
        out = []
        # (1) dict-of-tasks
        if isinstance(obj, dict) and not ("train" in obj and "test" in obj):
            ok = False
            for k, v in obj.items():
                if isinstance(v, dict) and "train" in v and "test" in v:
                    out.append((k, {"train": v["train"], "test": v["test"]}))
                    ok = True
            if ok:
                return out
            # wrapped lists
            for bucket in ("challenges", "training", "evaluation", "test"):
                if bucket in obj and isinstance(obj[bucket], list):
                    for i, e in enumerate(obj[bucket]):
                        if isinstance(e, dict) and "train" in e and "test" in e:
                            tid = e.get("task_id") or e.get("id") or f"{base_id}_{bucket}_{i:05d}"
                            out.append((tid, {"train": e["train"], "test": e["test"]}))
                    return out

        # (2) single dict
        if isinstance(obj, dict) and "train" in obj and "test" in obj:
            out.append((base_id, {"train": obj["train"], "test": obj["test"]}))
            return out

        # (3) list of dicts
        if isinstance(obj, list) and obj and isinstance(obj[0], dict):
            for i, e in enumerate(obj):
                if "train" in e and "test" in e:
                    tid = e.get("task_id") or e.get("id") or f"{base_id}_{i:05d}"
                    out.append((tid, {"train": e["train"], "test": e["test"]}))
            return out

        return out

    def load_json(path):
        with open(path, "r") as f:
            return json.load(f)

    # Prefer ARC Prize consolidated files if present
    got_any = False
    for name in ["arc-agi_training_challenges.json",
                 "arc-agi_evaluation_challenges.json",
                 "arc-agi_test_challenges.json"]:
        fp = os.path.join(tasks_dir, name)
        if os.path.exists(fp):
            got_any = True
            obj = load_json(fp)
            tasks.extend(normalize(obj, os.path.splitext(name)[0]))

    # Fallback: any *.json
    if not tasks:
        for fp in sorted(glob.glob(os.path.join(tasks_dir, "*.json"))):
            got_any = True
            try:
                tasks.extend(normalize(load_json(fp), os.path.splitext(os.path.basename(fp))[0]))
            except Exception:
                continue

    if not got_any:
        raise RuntimeError(f"No ARC tasks found under {tasks_dir}.")
    if not tasks:
        raise RuntimeError(f"Found JSON under {tasks_dir} but no (train/test) tasks parsed. Schema mismatch.")
    return tasks


def trains_from_task(task_json: Dict) -> List[Tuple[np.ndarray, np.ndarray]]:
    """Extract training pairs from a task JSON."""
    pairs = []
    for ex in task_json.get("train", []):
        x = np.array(ex["input"], dtype=np.int32)
        y = np.array(ex["output"], dtype=np.int32)
        pairs.append((x, y))
    return pairs


def tests_from_task(task_json: Dict) -> List[np.ndarray]:
    """Extract test inputs from a task JSON."""
    tests = []
    for ex in task_json.get("test", []):
        x = np.array(ex["input"], dtype=np.int32)
        tests.append(x)
    return tests


def write_submission_json(output_path: str, predictions: Dict[str, Any]):
    """Write predictions to submission JSON."""
    with open(output_path, 'w') as f:
        json.dump(predictions, f, indent=2)


# ============================================================================
# ============ io/telemetry.py ============
# ============================================================================

class StepLogger:
    """Logger for telemetry during search."""

    def __init__(self, public_mode: bool = False, jsonl_path: Optional[str] = None, log_every: int = 200):
        self.public_mode = public_mode
        self.jsonl_path = jsonl_path
        self.log_every = log_every
        self.jsonl_file = None

    def __enter__(self):
        if self.jsonl_path:
            self.jsonl_file = open(self.jsonl_path, 'w')
        return self

    def __exit__(self, *args):
        if self.jsonl_file:
            self.jsonl_file.close()

    def log_iteration(self, iteration: int, beam_size: int, best_cost: float):
        """Log iteration progress."""
        if iteration % self.log_every == 0:
            msg = f"Iter {iteration}: beam={beam_size}, cost={best_cost:.4f}"
            print(msg)

        if self.jsonl_file:
            record = {
                "iteration": iteration,
                "beam_size": beam_size,
                "best_cost": best_cost,
            }
            self.jsonl_file.write(json.dumps(record) + "\n")

    def log_task_start(self, task_id: str):
        """Log task start."""
        print(f"\n{'='*60}")
        print(f"Task: {task_id}")
        print(f"{'='*60}")

    def log_task_end(self, task_id: str, success: bool, elapsed: float):
        """Log task end."""
        status = "✓ SOLVED" if success else "✗ UNSOLVED"
        print(f"{status} ({elapsed:.1f}s)")


# ============================================================================
# ============ solver/task_solver.py ============
# ============================================================================


def _compute_train_px_err(best_program, train_pairs, phi, settings):
    if best_program is None or not train_pairs:
        return 1.0, 1.0, 0.0
    errs = []
    shape_hits = 0
    for (xi, yi) in train_pairs:
        try:
            pred = interpret_program(best_program, xi)
            if pred.shape == yi.shape:
                shape_hits += 1
                errs.append(float((pred != yi).mean()))
            else:
                errs.append(1.0)
        except Exception:
            errs.append(1.0)
    mean_err = float(sum(errs) / len(errs)) if errs else 1.0
    var_err = float(np.var(errs)) if errs else 1.0
    shape_rate = float(shape_hits / max(1, len(train_pairs)))
    return mean_err, var_err, shape_rate


def _update_rho(settings, best_program, train_pairs, phi, palette_is_safe):
    eps = 1e-3
    mean_err, var_err, shape_rate = _compute_train_px_err(best_program, train_pairs, phi, settings)
    t = max(eps, 1.0 - mean_err)
    r = 1.0 / (1.0 + max(0.0, min(1.0, var_err)))
    u_len = 1.0 / (1.0 + (len(best_program.steps) / 10.0)) if best_program else 0.5
    try:
        tens = compute_program_tension(best_program, phi) if best_program else 0.0
        u_ten = 1.0 / (1.0 + tens / 10.0)
    except Exception:
        u_ten = 0.7
    u = max(eps, 0.5 * u_len + 0.5 * u_ten)
    p = max(eps, shape_rate)
    a = 1.0 if palette_is_safe else 0.9
    C = max(eps, p * t * r * u * a)
    prevC = getattr(settings, "_hfp_prevC", None)
    rho = (C / prevC) if (prevC is not None and prevC > 0) else 1.0
    setattr(settings, "_hfp_prevC", float(C))
    setattr(settings, "_hfp_rho", float(rho))
    hist = list(getattr(settings, "_rho_samples", []))
    hist.append(float(rho))
    setattr(settings, "_rho_samples", hist)
    if len(hist) >= 2:
        setattr(settings, "_hfp_ready", True)
    return rho

def solve_task(
    task_id: str,
    task_json: Dict,
    settings: SearchSettings,
    logger: Optional[StepLogger] = None
) -> List[Any]:
    """
    Solve a single ARC task.
    
    Returns:
        List of predicted outputs for test inputs.
        - If best_program ends with tile_masked(ky,kx,m) where m in {0,1}:
          Returns list of dicts: [{"attempt_1": grid, "attempt_2": grid_alt}, ...]
          where attempt_2 uses the alternate mode (0↔1 swap)
        - Otherwise: Returns list of grids (np.ndarray format)
    """
    if logger:
        logger.log_task_start(task_id)

    start_time = time.time()

    # Extract training pairs and test inputs
    train_pairs = trains_from_task(task_json)
    test_inputs = tests_from_task(task_json)

    if not train_pairs or not test_inputs:
        if logger:
            logger.log_task_end(task_id, False, time.time() - start_time)
        return [np.array([[0]]) for _ in test_inputs]

    # Compute task features and φ
    features = task_features(train_pairs)
    phi = compute_phi(features)
    rho_samples = list(getattr(settings, "_rho_samples", []))

    if getattr(settings, "_policy_prior", None) is None:
        settings._policy_prior = _DEFAULT_POLICY_PRIOR

    trace = getattr(settings, "_trace_ops", False)
    settings._trace_buffer = []

    # --- Topology hint (side-channel): averages over train pairs ---
    dholes_g = dcomps_g = dholes_pc = dcomps_pc = 0.0
    for xi, yi in train_pairs:
        try:
            xg = np.asarray(xi)
            yg = np.asarray(yi)
            dholes_g += _holes_count(yg) - _holes_count(xg)
            dcomps_g += _component_count(yg) - _component_count(xg)
            dholes_pc += _holes_count(yg)
            dcomps_pc += _component_count(yg)
        except Exception:
            pass
    n = float(max(1, len(train_pairs)))
    settings._topo_hint = (dholes_g / n, dcomps_g / n, dholes_pc / n, dcomps_pc / n)

    # Meta-controller (opt-in)
    meta = None
    if settings.use_meta_controller:
        meta = getattr(solve_task, "_META", None)
        if meta is None:
            meta = MetaControllerUCB()
            solve_task._META = meta

    # --- Consensus one-step sweep (fast path) ---
    cons_prog, cons_mean = _consensus_one_step_candidate(train_pairs)

    # Learn program from training pairs
    best_program = None
    best_avg_cost = 1e9

    controller = ControllerState(mode="observer")
    selected_arm = None
    selected_bundle = None
    selected_fam = None

    # If consensus is strong, accept it and let finishers polish
    if cons_prog is not None and cons_mean >= 0.80:
        best_program = cons_prog
        best_avg_cost = 1.0 - cons_mean
    else:
        # Standard beam search on a subset of training pairs when consensus is weak
        K_eff = _maybe_escalate_K(cons_mean, rho_samples, settings, len(train_pairs))
        pairs_for_beam = train_pairs[:max(1, K_eff)]
        if logger:
            print(
                f"[beam] sampling {len(pairs_for_beam)}/{len(train_pairs)} pairs (consensus={cons_mean:.3f})"
            )
        selected_arm = None
        selected_bundle = None
        selected_fam = None
        if meta is not None:
            selected_fam = phi_to_family(phi)
            selected_arm, selected_bundle = meta.select(selected_fam)
            if logger:
                print(f"[meta] family={selected_fam} arm={selected_arm} bundle={selected_bundle}")
        for pair_idx, (x, y) in enumerate(pairs_for_beam):
            # Apply controller mode bias
            if meta is not None and selected_bundle is not None:
                mode_settings = replace(
                    settings,
                    beam_width=selected_bundle["beam_width"],
                    lambda1=selected_bundle["lambda1"],
                    lambda2=selected_bundle["lambda2"],
                    max_train_pairs_for_beam=selected_bundle["max_train_pairs_for_beam"],
                )
            else:
                mode_settings = apply_mode_bias(settings, controller.mode)

            topo_hint = getattr(settings, "_topo_hint", None)
            if topo_hint is not None:
                setattr(mode_settings, "_topo_hint", topo_hint)
            setattr(mode_settings, "_policy_prior", getattr(settings, "_policy_prior", _DEFAULT_POLICY_PRIOR))
            setattr(mode_settings, "_trace_ops", getattr(settings, "_trace_ops", False))
            if hasattr(settings, "_trace_buffer"):
                setattr(mode_settings, "_trace_buffer", getattr(settings, "_trace_buffer"))

            # Search for program
            prog = beam_search_one_pair(x, y, phi, mode_settings, logger)

            if prog is None:
                continue

            # Evaluate on all training pairs
            costs = []
            for (x_eval, y_eval) in train_pairs:
                cost = compute_cost(prog, x_eval, y_eval, phi, settings)
                costs.append(cost)

            avg_cost = np.mean(costs)

            if avg_cost < best_avg_cost:
                best_avg_cost = avg_cost
                best_program = prog

            # Check if should rotate controller
            if should_rotate(controller, avg_cost, settings):
                controller.mode = rotate_mode(controller)
                controller.rotation_count = 0
            else:
                controller.rotation_count += 1

            controller.last_rotation_cost = avg_cost

            # Early stop if perfect
            if best_avg_cost < 0.01:
                break

    # --- Micro-refinement for tile_masked consensus (optional shift nudge) ---
    if best_program is not None and len(best_program.steps) == 1:
        step = best_program.steps[0]
        if step.op == "tile_masked" and len(step.args) >= 3:
            # Try a single 'shift' around small deltas to catch off-by-one placement
            candidates = []
            for dy in (-1, 0, 1):
                for dx in (-1, 0, 1):
                    if dy == 0 and dx == 0: 
                        continue
                    prog_shift = Program(best_program.steps + [Step("shift", (dy, dx))])
                    candidates.append(prog_shift)

            # Evaluate train-average cost and keep if better
            def _avg_cost(p):
                cs = []
                for (xi, yi) in train_pairs:
                    try:
                        cs.append(compute_cost(p, xi, yi, phi, settings))
                    except Exception:
                        cs.append(1e9)
                return float(sum(cs) / len(cs)) if cs else 1e9

            if candidates:
                base_avg = _avg_cost(best_program)
                best_cand = min(candidates, key=_avg_cost, default=None)
                if best_cand is not None:
                    cand_avg = _avg_cost(best_cand)
                    if cand_avg + 1e-9 < base_avg:
                        best_program = best_cand
                        best_avg_cost = cand_avg

    # --- Alignment finisher (learn a single task-level shift) ---
    align = _learn_task_alignment(best_program, train_pairs, phi, settings) if best_program is not None else None
    if align is not None:
        dy, dx = align
        prog_shift = Program([Step(s.op, s.args) for s in best_program.steps] + [Step("shift", (dy, dx))])
        # accept shift if it improves average train cost
        avg_best  = float(np.mean([compute_cost(best_program, xi, yi, phi, settings) for (xi, yi) in train_pairs]))
        avg_shift = float(np.mean([compute_cost(prog_shift,   xi, yi, phi, settings) for (xi, yi) in train_pairs]))
        if avg_shift + 1e-9 < avg_best:
            best_program = prog_shift

    # --- Blockwise dominant-color finisher (sparse tiling) ---
    # Engage only when output is an integer multiple of input with small k
    learned_blockwise_proj = None
    try:
        x0, y0 = train_pairs[0]
        div = _divisible_shape(x0.shape, y0.shape) if (x0 is not None and y0 is not None) else None
    except Exception:
        div = None

    if div is not None and best_program is not None:
        ky, kx = div
        if ky in (3, 5) and kx in (3, 5):
            D = _learn_blockwise_projection(train_pairs, ky, kx)
            if D is not None:
                # Accept the projection only if it lowers or matches avg training cost when applied to predictions
                # (Compute average cost over train pairs with projection applied)
                def _avg_cost_with_projection(prog):
                    cs = []
                    for (xi, yi) in train_pairs:
                        try:
                            pi = interpret_program(prog, xi)
                            if pi.shape == yi.shape:
                                pi = _apply_blockwise_projection(pi, ky, kx, D)
                            # compute cost of fixed prediction vs target
                            cs.append(compute_cost(Program([]), pi, yi, phi, settings))
                        except Exception:
                            cs.append(1e9)
                    return float(np.mean(cs)) if cs else 1e9

                avg_cur = float(np.mean([compute_cost(best_program, xi, yi, phi, settings) for (xi, yi) in train_pairs]))
                avg_proj = _avg_cost_with_projection(best_program)
                if avg_proj + 1e-9 <= avg_cur:
                    learned_blockwise_proj = (ky, kx, D)

    # --- Block-mask finisher (sparse tiling) before palette ---
    # Learn a kxk mask only if all train shapes are consistent multiples of inputs
    try:
        # use the first train pair to infer divisibility
        x0, y0 = train_pairs[0]
        div = _divisible_shape(x0.shape, y0.shape) if (x0 is not None and y0 is not None) else None
    except Exception:
        div = None

    if div is not None and best_program is not None:
        ky, kx = div
        # small k only (keep it safe): 3 or 5
        if ky in (3,5) and kx in (3,5):
            M = _learn_block_mask(train_pairs, ky, kx)
            if M is not None:
                # Synthesize a program variant that applies the learned mask at prediction time
                # We do this as a *post-step* transform in training evaluation space:
                def _masked_eval_cost(prog):
                    # avg cost over train pairs with masking applied
                    cs = []
                    for (xi, yi) in train_pairs:
                        try:
                            pi = interpret_program(prog, xi)
                            # only mask if shapes are compatible
                            if pi.shape == yi.shape and _divisible_shape(xi.shape, yi.shape) == (ky, kx):
                                pi = _apply_block_mask(pi, ky, kx, M)
                            cs.append(compute_cost(Program([]), pi, yi, phi, settings))  # cost of fixed prediction vs target
                        except Exception:
                            cs.append(1e9)
                    return float(np.mean(cs)) if cs else 1e9

                avg_cur = _masked_eval_cost(best_program)  # masking applied to current program's outputs
                # Try appending an explicit mask step at inference time by wrapping predictions later;
                # since we don't have a DSL op for masking, we accept as a finisher iff it lowers cost.
                if avg_cur + 1e-9 < float(np.mean([compute_cost(best_program, xi, yi, phi, settings) for (xi, yi) in train_pairs])):
                    # Store learned mask for use on test predictions below
                    learned_block_mask = (ky, kx, M)
                else:
                    learned_block_mask = None
            else:
                learned_block_mask = None
        else:
            learned_block_mask = None
    else:
        learned_block_mask = None

    # Attach palette finisher - try appending palette remapping to improve accuracy
    if best_program is not None:
        mapping = _palette_map_from_train_pairs(train_pairs)
        if mapping:
            prog2 = Program([Step(s.op, s.args) for s in best_program.steps])
            for src, dst in mapping.items():
                prog2.steps.append(Step("replacecolor", (int(src), int(dst))))
            avg_best = np.mean([compute_cost(best_program, xi, yi, phi, settings) for (xi, yi) in train_pairs])
            avg_p2 = np.mean([compute_cost(prog2, xi, yi, phi, settings) for (xi, yi) in train_pairs])
            if avg_p2 < avg_best:
                best_program = prog2

    # ===== v2.9.1: Late auto-refiners (looser gate, extra candidate) =====
    if best_program is not None:
        def _avg_train_cost_pixels(prog: Program) -> float:
            errs = []
            for xi, yi in train_pairs:
                try:
                    pred = interpret_program(prog, xi)
                    tgt = np.asarray(yi)
                    if pred.shape != tgt.shape:
                        errs.append(1.0)
                    else:
                        errs.append(float((pred != tgt).mean()))
                except Exception:
                    errs.append(1.0)
            return float(np.mean(errs)) if errs else 1.0

        try:
            base_cost_px = _avg_train_cost_pixels(best_program)
            if base_cost_px <= 0.45:
                candidates = []
                base_steps = [Step(s.op, s.args) for s in best_program.steps]
                prog_keep1 = Program(base_steps + [Step("keep_n_largest", (1,))])
                prog_keep2 = Program(base_steps + [Step("keep_n_largest", (2,))])
                prog_fill = Program(base_steps + [Step("fill_holes", tuple())])
                candidates.append(("keep_n_largest(1)", prog_keep1, _avg_train_cost_pixels(prog_keep1)))
                candidates.append(("keep_n_largest(2)", prog_keep2, _avg_train_cost_pixels(prog_keep2)))
                candidates.append(("fill_holes", prog_fill, _avg_train_cost_pixels(prog_fill)))
                _, best_prog2, best_cost_px = min(candidates, key=lambda t: t[2])
                if best_cost_px + 1e-9 < base_cost_px:
                    best_program = best_prog2
                    best_avg_cost = np.mean(
                        [compute_cost(best_program, xi, yi, phi, settings) for (xi, yi) in train_pairs]
                    )
        except Exception:
            pass

    # Apply best program to test inputs
    # Learn palette map once from training pairs (used for TRAIN evaluation only)
    palette_map = _palette_map_from_train_pairs(train_pairs)
    
    # Validate palette safety on training pairs (for TRAIN-FIT logic)
    palette_is_safe = False
    if palette_map and best_program is not None:
        safe_count = 0
        for (xi, yi) in train_pairs:
            try:
                pred_train = interpret_program(best_program, xi)
                if pred_train.shape == yi.shape:
                    before = float((pred_train == yi).mean())
                    mapped = np.array(_apply_palette_map_ll(pred_train.tolist(), palette_map), dtype=np.int32)
                    after = float((mapped == yi).mean())
                    if after >= before:
                        safe_count += 1
            except Exception:
                continue
        # Palette is safe if it helps or maintains accuracy on all training pairs
        palette_is_safe = (safe_count == len(train_pairs))
    
    _update_rho(settings, best_program, train_pairs, phi, palette_is_safe)
    _smooth_rho(settings, getattr(settings, "_hfp_rho", None))
    prog_tension = float(compute_program_tension(best_program, phi)) if best_program is not None else 0.0

    # Check if we should generate two attempts using truncated shape-base
    # Truncate program at last tile_masked to get pure shape transformation
    base_steps, ky, kx, m = _truncate_at_last_tile_masked(best_program)
    emit_two_attempts = (base_steps is not None)
    
    # Debug telemetry
    if logger and emit_two_attempts:
        print(f"[two-attempts] tile_masked(ky={ky}, kx={kx}) found, using truncated shape-base (no finishers)")
    
    predictions = []
    
    # TWO-ATTEMPTS MODE: Use truncated shape-base programs, no finishers on TEST
    if emit_two_attempts:
        # Build truncated programs: attempt_1 = original mode, attempt_2 = swapped mode
        prog1 = Program(list(base_steps))  # keep original mode m
        
        # Build alternate with swapped mode
        alt_mode = 1 - m
        alt_steps = list(base_steps)
        alt_steps[-1] = Step("tile_masked", (ky, kx, alt_mode))
        prog2 = Program(alt_steps)
        
        for test_x in test_inputs:
            # Attempt 1: Original mode (raw output)
            try:
                pred1 = interpret_program(prog1, test_x)
            except Exception:
                pred1 = test_x.copy()
            
            # Attempt 2: Swapped mode (raw output)
            try:
                pred2 = interpret_program(prog2, test_x)
            except Exception:
                pred2 = pred1  # Fallback to attempt_1
            
            # Return two-attempts dict (raw outputs, no finishers applied)
            predictions.append({"attempt_1": pred1.tolist(), "attempt_2": pred2.tolist()})
    
    # SINGLE-ATTEMPT MODE: Apply finishers on TEST
    else:
        for test_x in test_inputs:
            if best_program is None:
                pred1 = test_x.copy()
            else:
                try:
                    pred1 = interpret_program(best_program, test_x)
                except Exception:
                    pred1 = test_x.copy()

            # Apply finishers to single-attempt predictions
            # Block-mask finisher
            if 'learned_block_mask' in locals() and learned_block_mask is not None:
                ky, kx, M = learned_block_mask
                div = _divisible_shape(test_x.shape, pred1.shape)
                if div == (ky, kx):
                    pred1 = _apply_block_mask(pred1, ky, kx, M)

            # Blockwise color projection finisher
            if 'learned_blockwise_proj' in locals() and learned_blockwise_proj is not None:
                ky, kx, D = learned_blockwise_proj
                div = _divisible_shape(test_x.shape, pred1.shape)
                if div == (ky, kx):
                    pred1 = _apply_blockwise_projection(pred1, ky, kx, D)

            # Single-output mode
            pred_np = np.asarray(pred1)
            meta = {
                "bg": 0,
                "prog_len": (len(best_program.steps) if best_program else 0),
                "tension": prog_tension,
                "align_dy_dx": align,
                "block_mask": (learned_block_mask[2] if ("learned_block_mask" in locals() and learned_block_mask is not None) else None),
                "block_proj": (lambda g, ky=learned_blockwise_proj[0], kx=learned_blockwise_proj[1], D=learned_blockwise_proj[2]: _apply_blockwise_projection(g, ky, kx, D)) if ("learned_blockwise_proj" in locals() and learned_blockwise_proj is not None) else None,
                "palette_safe": bool(palette_is_safe),
            }
            predictions.append({"grid": pred_np.tolist(), "meta": meta})

    # v2.8.7: expose last program for replay/debugging
    try:
        globals()["last_best_program"] = best_program
    except Exception:
        pass

    # v2.8.7: universal two-attempts for non-tiling programs
    if (
        settings.always_two_attempts
        and predictions
        and not (isinstance(predictions[0], dict) and ("attempt_1" in predictions[0] or "grid" in predictions[0]))
    ):
        enhanced = []
        for grid in predictions:
            try:
                g = np.asarray(grid)
                alt_cands = [
                    np.rot90(g, k=1),
                    np.rot90(g, k=2),
                    np.rot90(g, k=3),
                    np.fliplr(g),
                    np.flipud(g),
                ]
                attempt_2 = alt_cands[0]
                enhanced.append(
                    {
                        "attempt_1": g.tolist(),
                        "attempt_2": attempt_2.tolist(),
                    }
                )
            except Exception:
                fallback = grid.tolist() if hasattr(grid, "tolist") else grid
                enhanced.append({"attempt_1": fallback, "attempt_2": fallback})
        predictions = enhanced

    elapsed = time.time() - start_time
    success = best_avg_cost < 0.01

    # Meta-controller reward (opt-in)
    if meta is not None and selected_arm is not None and selected_fam is not None:
        try:
            reward = max(0.0, 1.0 - float(best_avg_cost)) / (0.05 + elapsed)
            meta.update(selected_fam, selected_arm, reward)
        except Exception:
            pass

    if trace and logger:
        print(f"[trace] collected {len(getattr(settings, '_trace_buffer', []))} op deltas")

    if logger:
        logger.log_task_end(task_id, success, elapsed)

    return predictions


# v2.8.7: progressive beam schedules
DEFAULT_SCHEDULES = [(24, 3), (48, 5), (64, 7), (96, 8)]


def solve_task_with_schedule(
    tid,
    task_json,
    settings,
    logger=None,
    schedules=None,
):
    """
    Try a tiered (beam_width, max_depth) schedule and return the best predictions and schedule.
    If settings._truths_provider is present (callable tid->list[np.ndarray]), use it to pick the best;
    otherwise returns the first successful predictions.
    """

    schedules = schedules or DEFAULT_SCHEDULES
    best_score, best_preds, best_cfg = -1.0, None, None
    last_preds = None

    truths = None
    try:
        provider = getattr(settings, "_truths_provider", None)
        if callable(provider):
            truths = provider(tid)
    except Exception:
        truths = None

    for beam_width, max_depth in schedules:
        sub = replace(settings, beam_width=beam_width, max_depth=max_depth)
        # propagate auxiliary hooks if present
        for attr in ("_truths_provider", "_policy_prior", "_trace_ops"):
            if hasattr(settings, attr):
                setattr(sub, attr, getattr(settings, attr))

        preds = solve_task(tid, task_json, sub, logger=logger)
        last_preds = preds
        score = -1.0
        if truths is not None and preds is not None and len(truths) == len(preds):
            try:
                score = float(
                    np.mean([_pixel_acc(preds[i], truths[i]) for i in range(len(preds))])
                )
            except Exception:
                score = -1.0

        if best_preds is None or score > best_score:
            best_score, best_preds, best_cfg = score, preds, (beam_width, max_depth)

        if score == 1.0:
            break

    return (best_preds if best_preds is not None else last_preds), best_cfg


# ============================================================================
# ============ solver/batch.py ============
# ============================================================================

def run_dir(
    tasks_dir: str,
    settings: SearchSettings,
    max_tasks: Optional[int] = None,
    logger: Optional[StepLogger] = None
) -> Dict[str, List[Any]]:
    tasks = list(load_tasks_from_dir(tasks_dir))
    tasks_dict = dict(tasks)
    panel_ids = list(tasks_dict.keys())
    if max_tasks:
        panel_ids = panel_ids[:max_tasks]
        tasks_dict = {tid: tasks_dict[tid] for tid in panel_ids}
    # _run_dir_staged expects loader to fetch tasks by id
    def _local_loader(_):
        return tasks_dict
    prev_loader = globals().get("_load_test_challenges")
    globals()["_load_test_challenges"] = _local_loader
    try:
        staged = _run_dir_staged(
            tasks_dir,
            seconds=getattr(settings, "max_seconds", None),
            panel_ids=panel_ids,
            logger=logger,
            settings_proto=settings,
        )
    finally:
        if prev_loader is not None:
            globals()["_load_test_challenges"] = prev_loader
        else:
            globals().pop("_load_test_challenges", None)
    results = {}
    for tid, preds in staged.items():
        if preds and isinstance(preds[0], dict):
            preds_ll = preds
        else:
            preds_ll = [pred.tolist() if hasattr(pred, "tolist") else pred for pred in preds]
        results[tid] = preds_ll
    return results


# ============================================================================
# ============ eval/metrics.py ============
# ============================================================================

def exact_match(pred: np.ndarray, truth: np.ndarray) -> bool:
    """Check if prediction exactly matches ground truth."""
    if pred.shape != truth.shape:
        return False
    return np.array_equal(pred, truth)


def pixel_accuracy(pred: np.ndarray, truth: np.ndarray) -> float:
    """Compute pixel-wise accuracy."""
    if pred.shape != truth.shape:
        return 0.0
    correct = np.sum(pred == truth)
    total = truth.size
    return correct / max(total, 1)


def solve_rate(preds: List[np.ndarray], truths: List[np.ndarray]) -> float:
    """Compute fraction of tasks solved."""
    if not preds or not truths:
        return 0.0
    solved = sum(exact_match(p, t) for p, t in zip(preds, truths))
    return solved / len(truths)


def evaluate_task(preds: List[np.ndarray], 
                 truths: List[np.ndarray]) -> Dict[str, float]:
    """Comprehensive evaluation metrics for one task."""
    if not preds or not truths:
        return {"exact_match": 0.0, "pixel_accuracy": 0.0, "solve_rate": 0.0}
    
    exact = all(exact_match(p, t) for p, t in zip(preds, truths))
    pixel_acc = np.mean([pixel_accuracy(p, t) for p, t in zip(preds, truths)])
    solve = solve_rate(preds, truths)
    
    return {
        "exact_match": float(exact),
        "pixel_accuracy": float(pixel_acc),
        "solve_rate": float(solve),
    }


# ============================================================================
# ============ eval/ablations.py ============
# ============================================================================

def without_oco(settings):
    """Disable OCO: no tension penalties."""
    from dataclasses import replace
    s = replace(settings)
    s.lambda1 = 0.0
    s.lambda2 = 0.0
    return s


def without_slice_guard(settings):
    """Disable slice gating."""
    from dataclasses import replace
    s = replace(settings)
    s.slice_guard_thresh = 1e9
    s.allow_offslice_early = True
    return s


def without_rotation(settings):
    """Disable controller rotations."""
    from dataclasses import replace
    s = replace(settings)
    s._disable_rotation = True
    return s


def get_ablation_config(name: str, base_settings):
    """Get settings for ablation experiment."""
    ablations = {
        "no_oco": without_oco,
        "no_slice": without_slice_guard,
        "no_rotation": without_rotation,
    }
    
    if name == "baseline":
        return base_settings
    elif name in ablations:
        return ablations[name](base_settings)
    else:
        raise ValueError(f"Unknown ablation: {name}")


# ============================================================================
# ============ Recursion-Safe Wrapper Pattern ============
# ============================================================================
# 
# CRITICAL: If you want to monkey-patch beam_search_one_pair or generate_successors
# with custom gating logic, use this pattern to avoid RecursionError.
#
# The problem: If wrapper A calls the patched function, which is now wrapper B,
# which calls wrapper A again → infinite recursion.
#
# The solution: Save the base function ONCE and always call the base.
#
# Example for beam_search_one_pair:
#
# import arc_one
#
# # Step 1: Save base exactly once (before any patching)
# if not hasattr(arc_one, "_BASE_BEAM"):
#     arc_one._BASE_BEAM = arc_one.beam_search_one_pair
#
# # Step 2: Define your wrapper
# def custom_beam_wrapper(input_grid, target_grid, phi, settings, logger):
#     """Your custom shape-then-color gating logic."""
#     # ... preprocessing ...
#     
#     # ALWAYS call the saved base, never the patched version
#     result = arc_one._BASE_BEAM(input_grid, target_grid, phi, settings, logger)
#     
#     # ... postprocessing ...
#     return result
#
# # Step 3: Guard against double-wrapping
# if getattr(arc_one.beam_search_one_pair, "__name__", "") != "custom_beam_wrapper":
#     arc_one.beam_search_one_pair = custom_beam_wrapper
#
# Same pattern for generate_successors:
# - Save as _BASE_GENERATE_SUCCESSORS
# - Always call _BASE_GENERATE_SUCCESSORS inside wrapper
# - Use try/finally to restore if needed
#
# Example with try/finally cleanup:
#
# if not hasattr(arc_one, "_BASE_GENERATE"):
#     arc_one._BASE_GENERATE = arc_one.generate_successors
#
# def custom_generate(prog, ctx):
#     # Filter or modify successor generation
#     successors = arc_one._BASE_GENERATE(prog, ctx)
#     return [s for s in successors if some_condition(s)]
#
# # Temporarily patch
# old_generate = arc_one.generate_successors
# arc_one.generate_successors = custom_generate
# try:
#     # ... run solver ...
#     results = run_dir(...)
# finally:
#     # Restore original
#     arc_one.generate_successors = old_generate
#
# ============================================================================


# ============================================================================
# ============ cli/main.py ============
# ============================================================================

def main():
    """Command-line interface for ARC-ONE solver."""
    parser = argparse.ArgumentParser(
        description="ARC-ONE: Octonionic Control Overlay Solver",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # OCO-guided two attempts (recommended)
  python arc_one.py --tasks_dir ./arc_tasks --two_attempts --out submission.json
  
  # Two attempts with manual strategy (e.g., horizontal flip)
  python arc_one.py --tasks_dir ./arc_tasks --two_attempts --attempt2_strategy flipH
  
  # Legacy single attempt (no change to file structure)
  python arc_one.py --tasks_dir ./arc_tasks --out submission.json
  
  # Quick test with validation
  python arc_one.py --tasks_dir ./arc_tasks --two_attempts --max_tasks 5
  
  # Ablation study
  python arc_one.py --tasks_dir ./arc_tasks --ablation no_oco --out no_oco.json
  
  # With telemetry logging
  python arc_one.py --tasks_dir ./arc_tasks --jsonl telemetry.jsonl --two_attempts
        """
    )
    
    # I/O
    parser.add_argument("--tasks_dir", required=True,
                       help="Directory containing task JSON files")
    parser.add_argument("--out", default="submission.json",
                       help="Output submission file")
    parser.add_argument("--two_attempts", action="store_true",
                       help="Output two attempts per test input (ARC 2025 schema).")
    parser.add_argument("--attempt2_strategy", type=str, default="oco_auto",
                       choices=["oco_auto","auto","rotate90","rot180","flipH","flipV","palette_swap","center","toward_input"],
                       help="How to generate attempt_2.")
    
    # Limits
    parser.add_argument("--max_tasks", type=int, default=None,
                       help="Max number of tasks to solve")
    
    # Search settings
    parser.add_argument("--beam", type=int, default=128,
                       help="Beam width (default: 128)")
    parser.add_argument("--depth", type=int, default=10,
                       help="Max program depth (default: 10)")
    parser.add_argument("--seconds", type=float, default=3.0,
                       help="Max seconds per task (default: 3.0)")
    
    # OCO settings
    parser.add_argument("--lambda_len", type=float, default=0.20,
                       help="Length penalty weight (default: 0.20)")
    parser.add_argument("--lambda1", type=float, default=0.30,
                       help="Program tension weight (default: 0.30)")
    parser.add_argument("--lambda2", type=float, default=0.20,
                       help="Slice tension weight (default: 0.20)")
    parser.add_argument("--div_lambda", type=float, default=0.20,
                        help="Diversity bonus weight for two-attempt selection (default: 0.20)")
    parser.add_argument("--iou_cap", type=float, default=0.97,
                        help="Maximum IoU allowed before alternates are rejected (default: 0.97)")
    parser.add_argument("--max_bounces", type=int, default=-1,
                        help="Hard cap on debate bounces (default: -1 for rho-driven)")
    
    # Ablations
    parser.add_argument("--ablation", type=str, default=None,
                       choices=["no_oco", "no_slice", "no_rotation"],
                       help="Run ablation experiment")
    
    # Logging
    parser.add_argument("--public_mode", action="store_true",
                       help="Use public-facing terminology in logs")
    parser.add_argument("--log_every", type=int, default=200,
                       help="Log every N iterations (default: 200)")
    parser.add_argument("--jsonl", type=str, default=None,
                       help="Path for JSONL telemetry log")
    
    # Determinism
    parser.add_argument("--seed", type=int, default=1337,
                       help="Random seed for determinism (default: 1337)")
    
    args = parser.parse_args()

    if not _NUMPY_AVAILABLE:
        print(f"ERROR: {_NUMPY_IMPORT_ERROR}", file=sys.stderr)
        sys.exit(1)

    # Banner
    print("=" * 80)
    print("ARC-ONE: Octonionic Control Overlay for Abstract Reasoning")
    print("=" * 80)
    print(f"Configuration:")
    print(f"  Beam width: {args.beam}")
    print(f"  Max depth: {args.depth}")
    print(f"  Max seconds: {args.seconds}")
    print(f"  OCO penalties: λ_len={args.lambda_len}, λ1={args.lambda1}, λ2={args.lambda2}")
    if args.two_attempts:
        print(f"  Two attempts mode: {args.attempt2_strategy}")
    if args.ablation:
        print(f"  Ablation: {args.ablation}")
    print("=" * 80)
    
    # Build settings
    settings = SearchSettings(
        beam_width=args.beam,
        max_depth=args.depth,
        max_seconds=args.seconds,
        lambda_len=args.lambda_len,
        lambda1=args.lambda1,
        lambda2=args.lambda2,
        public_mode=args.public_mode,
        log_every=args.log_every,
        seed=args.seed,
        div_lambda=args.div_lambda,
        iou_cap=args.iou_cap,
        max_bounces=args.max_bounces,
    )
    
    # Apply ablation if specified
    if args.ablation:
        print(f"\n⚠️  Running ablation: {args.ablation}\n")
        settings = get_ablation_config(args.ablation, settings)
    
    # Resolve tasks dir (robust against nested competition paths in Kaggle)
    resolved_tasks_dir = args.tasks_dir  # simplified: supports ARC Prize 2025 layout directly
    print(f"🔍 Using ARC tasks at: {resolved_tasks_dir}\n")
    
    # Run solver
    with StepLogger(args.public_mode, args.jsonl, args.log_every) as logger:
        results = run_dir(resolved_tasks_dir, settings, args.max_tasks, logger)
    
    # Build final predictions object (single- or two-attempts)
    if args.two_attempts:
        predictions = _two_attempts_from_results(
            results,
            tasks_dir=resolved_tasks_dir,
            strategy=args.attempt2_strategy,
            settings=settings,
        )
    else:
        predictions = results  # legacy single-output-per-test schema
    
    # Write submission (function dumps whatever dict we pass)
    write_submission_json(args.out, predictions)
    
    # Summary
    print("\n" + "=" * 80)
    print(f"✅ COMPLETE!")
    print("=" * 80)
    print(f"  Output: {args.out}")
    print(f"  Tasks: {len(results)}")
    if args.two_attempts:
        print(f"  Format: Two attempts ({args.attempt2_strategy})")
    if args.jsonl:
        print(f"  Telemetry: {args.jsonl}")
    print("=" * 80)


if __name__ == "__main__":
    # Check for test mode
    if os.environ.get("ARC_ONE_RUN_TESTS") == "1":
        print("Test mode not included in this artifact - run tests separately")
        print("To use the solver, run: python arc_one.py --tasks_dir <path>")
    else:
        main()
