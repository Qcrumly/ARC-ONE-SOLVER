from __future__ import annotations
# ==================================================================================
# ARC-ONE: Octonionic Control Overlay for Abstract Reasoning Corpus
# ==================================================================================
#
# Version: v2.10.5 — Shaped cost, two-lane beam, stronger A/B debate
#
# CHANGELOG
# v2.10.4 (2025-11-03)
# - Fix: stack enumeration filter now validates axis at args[1] and restores stack candidates;
#        optional ARC_DEBUG_STACK=1 prints a one-shot sample for inspection.
# - Fix: motif seeding falls back to the local parser when memory returns zero steps.
# - Fix: GOF R-thresholds calibrated to the kernel maximum so stretch/break gates are reachable.
# - Add: ONE controller face dwell (ARC_FACE_DWELL_STEPS) to stabilize Explorer/Navigator/Observer.
# - Add: Optional depth-0 size-op carve-out driven by scale_hard_thresh/ARC_SCALE_HARD_THRESH.
#
# v2.10.3-hotfix2 (2025-11-03)
# - Fix: stack enumeration filter checked the wrong arg index; no stack candidates were generated.
#        Now checks axis in args[1], matching executor (axis = int(args[1])).
# - Dev: Optional ARC_DEBUG_STACK=1 prints a sample of stack combos during enumeration.
# - (Optional, guarded) Add scale_hard_thresh setting with env override ARC_SCALE_HARD_THRESH.
#
# v2.10.3-hotfix1 (2025-11-03)
#   - Exact-match early exit is now robust: win logging is wrapped in try/except and task_id is resolved defensively, so a NameError can never block the return. The solver now always exits immediately on exact predictions.
#   - Pixel IoU guard: _pixel_iou now checks for zero-sized grids to avoid ZeroDivisionError. For empty-but-equal shapes, IoU returns 0.0 (conservative), matching prior semantics where union==0 yielded 0.0.
#
# v2.10.3 (2025-11-03)
#   - Strict two-attempts chooser: Attempt-B picked by score = conf - λ·IoU(first), with hard IoU cap.
#   - Optional CSV logs: exact-win attribution (which op closed the solution) and octonion-telemetry (x_oct + confounds).
#   - Robust seed parsing: prefer arc_memory.parse_op_tokens; normalize bare tokens to name() / name(1).
#   - SciPy-free fallback for connected components labeling.
#   - Vectorized pixel IoU; applied small telemetry size cap to prevent runaway logs.
#   - Kept everything Kaggle-safe and off by default unless flags are passed.
#
# v2.9.8
# - Block “identity/surrender” successors.
# - Gate early search steps by φ[scale] sign.
# - Keep palette-first blocked for the first few steps (configurable).
# - Telemetry exposes GOF-9000 constraint settings.
#
# v2.9.7 — Bounce-on-low-div-high-octo (default ON) + telemetry
# - Default-on policy: if skim diversity < lowdiv_thr and octonion z >= octo_z_min_for_bounce,
#   force up to bounce_max bounces in MAIN (usually 1).
# - New flags: --no_bounce, --lowdiv_thr, --octo_z_min_for_bounce, --bounce_max
# - Telemetry: forced_bounce (bool), lowdiv_skim (float), octo_z (float)
#
# v2.9.6
# - Stage controls: --no_polish to skip the third stage, and --stop_if_diversity <thr>
#   to stop after skim/main when A/B diversity (1 - IoU) >= thr (default 0.20).
# - Telemetry: per-stage timings t_skim, t_main, t_polish and skipped_polish flag.
# - Preserves v2.9.5 hard caps, octonion prior, two-attempt policy, and palette rules.
#
# v2.9.5
# - Enforce hard per-stage time caps: _solve_task_internal respects settings.max_seconds via a deadline check.
# - Task telemetry: budget_sec, elapsed_sec, hit_deadline (True if deadline reached).
# - Emit finalized program sequence for sidecar analysis: ops_tokens[], ops_families[].
# - Keeps v2.9.4 octonion palette8 prior and all previous policies unchanged.
#
# v2.9.4
# - Optional (default ON) palette8 octonion distance–based difficulty prior for training-stage time allocation.
#   * Scales Stage-2/3 seconds per task by a clamp of (1 + alpha * z), z = z-scored mean palette-distance over TRAIN pairs.
#   * Uses only TRAIN examples (Kaggle-safe); does not alter TEST packaging/policy or DSL semantics.
#   * Adds telemetry fields: octo_dist_mean, octo_z, octo_mult.
#   * Disable with --no_octo_prior; tune with --octo_alpha and --octo_clip.
#
# Enhancements:
# - v2.9.3: Real C/ρ tracker wired into settings (_hfp_prevC/_hfp_rho/_hfp_rho_smoothed/_hfp_ready),
#           enabling ρ-adaptive abort windows and bounded A↔B debate.
#           Belief pass-through for TEST predictions (align_dy_dx, block_mask, block_proj) so alternates are targeted.
#           Candidate-pool + IoU diversity guard for non-tilers (useful disagreement > 0).
#           Palette guard fixed (structure-safe mapping via _palette_map_from_train_pairs).
#           attempt2_strategy honored (seeded candidate in alternates list).
#           Staged runner now keeps best-so-far and early-exits when diversity achieved.
#           Duplicate topology-hint block removed; telemetry cleaned.
# - v2.9.2: Two-attempt debate with diversity guard, ρ-adaptive aborts, palette safety, and stage-aware scheduling.

__version__ = "v2.10.5"  # Shaped cost, two-lane beam, stronger A/B debate

last_best_program = None

_NUMPY_IMPORT_ERROR = (
    "ARC-ONE requires the 'numpy' package. Install it with `pip install numpy` "
    "or ensure it is available in your execution environment before running the solver."
)

try:
    import numpy as np  # type: ignore
    _NUMPY_AVAILABLE = True
except ModuleNotFoundError:  # pragma: no cover - depends on environment
    _NUMPY_AVAILABLE = False

    class _MissingNumpy:
        """Proxy that surfaces a helpful error message when numpy is absent."""

        class ndarray:  # minimal stand-in for isinstance checks
            pass

        def __getattr__(self, name):
            raise ModuleNotFoundError(_NUMPY_IMPORT_ERROR)

    np = _MissingNumpy()  # type: ignore
from dataclasses import dataclass, field, replace
from typing import Dict, List, Tuple, Optional, Any, Set, Iterable, Sequence
from collections import Counter, deque, defaultdict
from functools import lru_cache
import csv, time, random
import itertools
import math
import statistics as _stats
import json
import os
import glob
import sys
import argparse
import tempfile
import shutil
import hashlib
import re


def _envfloat(name, default):
    try:
        return float(os.getenv(name, str(default)))
    except Exception:
        return default


def _envint(name, default):
    try:
        return int(os.getenv(name, str(default)))
    except Exception:
        return default


def _envbool(name, default):
    """Read boolean from env using permissive parsing (0/1, true/false)."""
    default_str = "1" if default else "0"
    raw = os.getenv(name, default_str)
    try:
        return bool(int(raw))
    except Exception:
        if isinstance(raw, str):
            val = raw.strip().lower()
            if val in {"true", "t", "yes", "y", "on"}:
                return True
            if val in {"false", "f", "no", "n", "off"}:
                return False
        return default
try:
    from scipy.ndimage import label as _cc_label
except Exception:  # pragma: no cover - optional SciPy dependency

    def _cc_label(arr, structure=None):
        import numpy as np  # local import keeps fallback self-contained

        arr = (np.asarray(arr) != 0).astype(np.uint8)
        h, w = arr.shape
        labels = np.zeros_like(arr, dtype=np.int32)
        current = 0
        for y in range(h):
            for x in range(w):
                if arr[y, x] and labels[y, x] == 0:
                    current += 1
                    labels[y, x] = current
                    dq = deque([(y, x)])
                    while dq:
                        yy, xx = dq.popleft()
                        for dy, dx in ((1, 0), (-1, 0), (0, 1), (0, -1)):
                            ny, nx = yy + dy, xx + dx
                            if 0 <= ny < h and 0 <= nx < w and arr[ny, nx] and labels[ny, nx] == 0:
                                labels[ny, nx] = current
                                dq.append((ny, nx))
        return labels, int(current)


def _keep_n_largest_ll(grid_ll, n=1):
    g = np.asarray(grid_ll)
    if g.size == 0:
        return g.tolist()
    bg = 0
    mask = (g != bg).astype(np.uint8)
    labels, k = _cc_label(mask)
    if k <= n:
        return g.tolist()
    areas = [(i, int((labels == i).sum())) for i in range(1, k + 1)]
    areas.sort(key=lambda t: t[1], reverse=True)
    keep = {i for i, _ in areas[: max(1, n)]}
    out = g.copy()
    out[~np.isin(labels, list(keep))] = bg
    return out.tolist()


def _structure_tuple(arr):
    a = np.asarray(arr)
    h, w = a.shape
    return (h, w, int((a != 0).sum()))


def _structure_area(arr, bg=0):
    a = np.asarray(arr)
    if a.ndim != 2:
        return 0
    return int((a != bg).sum())


# --- Train-aware single-object heuristics (no test target required) ---


def _nonbg_area(grid, bg=0):
    try:
        arr = np.asarray(grid)
        return int((arr != bg).sum())
    except Exception:
        return 0


def _modal_components_and_area(train_pairs, bg=0):
    """
    From training outputs, estimate typical component count and area.
    Returns (modal_components, median_area) or (None, None) if unknown.
    """

    comps: List[int] = []
    areas: List[int] = []
    try:
        for (_x, y) in (train_pairs or []):
            arr = np.asarray(y)
            labels, k = _cc_label((arr != bg).astype(np.uint8))
            comps.append(int(k))
            areas.append(int((arr != bg).sum()))
        if not comps:
            return (None, None)
        vals, freqs = np.unique(np.array(comps), return_counts=True)
        modal_k = int(vals[int(np.argmax(freqs))])
        med_area = int(np.median(np.array(areas))) if areas else None
        return (modal_k, med_area)
    except Exception:
        return (None, None)


def _keep1_guard(candidate, train_pairs, belief, loosen=0.40):
    """
    If training suggests a single object, keep the single largest component.
    Only apply if candidate area is 'reasonable' vs typical train area.
    """

    bg = (belief.get("bg", 0) if isinstance(belief, dict) else 0)
    modal_k, med_area = _modal_components_and_area(train_pairs, bg=bg)
    if modal_k != 1:
        return candidate, False

    cand_area = _nonbg_area(candidate, bg)
    if med_area is None or cand_area == 0:
        return candidate, False

    lo = int((1.0 - loosen) * med_area)
    hi = int((1.0 + loosen) * med_area)
    if lo <= cand_area <= hi:
        try:
            try:
                out = _keep_n_largest_ll(candidate, 1)
            except Exception:
                out = _largest_component_only(candidate, bg=bg)
            return out, True
        except Exception:
            return candidate, False
    return candidate, False


def _fill_holes_ll(grid_ll):
    arr = np.asarray(grid_ll)
    if arr.ndim != 2:
        return arr.tolist() if hasattr(arr, "tolist") else grid_ll
    try:
        filled = _op_fill_holes(arr)
    except Exception:
        return arr.tolist()
    filled_arr = np.asarray(filled)
    return filled_arr.tolist() if hasattr(filled_arr, "tolist") else filled


def _apply_morph_pivot(grid_ll):
    pivot = _keep_n_largest_ll(grid_ll, 1)
    return _fill_holes_ll(pivot)


# === Determinism & banner ===
try:  # Prefer real numpy but fall back to the shim if unavailable
    import numpy as _np
except ModuleNotFoundError:  # pragma: no cover - aligns with optional numpy install
    _np = np  # type: ignore

try:  # Memory helpers are optional
    import arc_memory as MEM
except Exception:  # pragma: no cover - memory module optional
    MEM = None


DIVERSITY_MIN_THRESHOLD = 0.15
SHAPE_MATCH_BONUS = 0.02
RHO_HEALTHY = 0.80
MAX_TELEMETRY_ITEMS = 50
SEED = 0

_CLI_ARGS: Optional[Any] = None


# ---- Memory motif token parsing & safe telemetry --------------------------------
_OP_TOKEN_RE = re.compile(r"^\s*([A-Za-z_]\w*)\s*\((.*)\)\s*$")


def _cap_telemetry(telemetry: Optional[Dict[str, Any]]) -> None:
    if isinstance(telemetry, dict) and len(telemetry) > MAX_TELEMETRY_ITEMS:
        for key in list(telemetry.keys())[MAX_TELEMETRY_ITEMS:]:
            telemetry.pop(key, None)


def rvd_R(rho: float, eps: float = 1e-12) -> float:
    """Anti-friction metric R(ρ)=ρ⁷·|ln ρ| for ρ∈(0,1]."""
    rho = max(eps, min(1.0, float(rho)))
    return (rho ** 7) * abs(math.log(rho))


def _telemetry_note_safe(carrier, **kv):
    """Silently attach telemetry if helper is available."""
    try:
        _telemetry_note  # type: ignore[name-defined]
    except Exception:
        return
    try:
        _telemetry_note(carrier, **kv)  # type: ignore[name-defined]
    except Exception:
        pass
    target = None
    try:
        if isinstance(carrier, dict):
            target = carrier.get("_telemetry") if "_telemetry" in carrier else carrier
        else:
            target = getattr(carrier, "_telemetry", None)
    except Exception:
        target = None
    _cap_telemetry(target)


def _parse_seed_tokens_to_program(
    tokens: Optional[List[str]],
    StepCls: Optional[Any] = None,
    ProgramCls: Optional[Any] = None,
):
    """Return (Program or None, number_of_steps) for a motif token list."""
    if not tokens:
        return None, 0
    # Prefer arc_memory's permissive parser when available
    try:
        import arc_memory as MEM

        if hasattr(MEM, "parse_op_tokens"):
            steps = MEM.parse_op_tokens(list(tokens))
            ProgType = ProgramCls or globals().get("Program")
            if steps and ProgType is not None:
                return ProgType(list(steps)), len(steps)
    except Exception:
        pass
    # Conservative fallback: normalize tokens via the solver's operator regex if present
    steps = []
    _re = globals().get("_OP_TOKEN_RE", None)
    for raw in tokens:
        tok = (raw or "").strip()
        if not tok:
            continue
        if _re:
            m = _re.match(tok)
            if m:
                name, args = m.group(1), (m.group(2) or "").strip()
            else:
                name, args = tok, ""
        else:
            name = tok.split("(")[0].strip()
            args = tok[tok.find("(") + 1 : tok.rfind(")")] if "(" in tok and ")" in tok else ""
        arg_tuple = (
            tuple([a.strip() for a in args.split(",") if a.strip()]) if args != "" else tuple()
        )
        StepType = StepCls or globals().get("Step")
        if StepType is not None:
            steps.append(StepType(name=name, args=arg_tuple))
    ProgType = ProgramCls or globals().get("Program")
    if steps and ProgType is not None:
        return ProgType(steps), len(steps)
    return None, 0


def _enqueue_seed_programs(
    frontier: List[Any],
    input_grid: Any,
    target_grid: Any,
    seed_tokens_list: Optional[List[List[str]]],
    interpret_program_fn,
    compute_cost_fn,
    step_cls: Optional[Any] = None,
    prog_cls: Optional[Any] = None,
    telem_carrier: Optional[Any] = None,
) -> int:
    """Append parsed motif programs to the frontier. Returns count appended."""
    if not seed_tokens_list:
        return 0

    pushed = 0
    for toks in seed_tokens_list:
        prog, nops = _parse_seed_tokens_to_program(toks, StepCls=step_cls, ProgramCls=prog_cls)
        if prog is None or not getattr(prog, "steps", None):
            continue
        try:
            pred = interpret_program_fn(prog, input_grid)
            cost = compute_cost_fn(prog, input_grid, target_grid)
        except Exception:
            continue

        try:
            if "BeamEntry" in globals():
                entry = BeamEntry(program=prog, pred=pred, cost=cost)  # type: ignore[name-defined]
            else:
                entry = {"program": prog, "pred": pred, "cost": float(cost)}
            frontier.append(entry)
            _telemetry_note_safe(
                entry,
                memory_seeded_motif=list(toks or []),
                memory_seed_ops=int(nops),
                memory_seed_cost=float(cost),
            )
            if telem_carrier is not None:
                _telemetry_note_safe(
                    telem_carrier,
                    memory_seeded_motif=list(toks or []),
                    memory_seed_ops=int(nops),
                    memory_seed_cost=float(cost),
                )
            pushed += 1
        except Exception:
            continue

    return pushed


def _collect_memory_motif_candidates(
    task_json: Dict[str, Any],
    memo: Dict[str, Any],
    motif_topk: int,
) -> List[Tuple[List[str], str, float, Program]]:
    if MEM is None or motif_topk <= 0:
        return []
    try:
        motifs_store = memo.get("motifs", {}) if isinstance(memo, dict) else {}
    except Exception:
        motifs_store = {}
    if not motifs_store:
        return []
    train_entries = list(task_json.get("train", []))
    families: List[str] = []
    for ex in train_entries:
        try:
            fp = MEM.fingerprint_layout({"train": [ex]})
            fam = MEM.detect_family(fp)
        except Exception:
            fam = None
        if fam and fam not in families:
            families.append(fam)
    if not families:
        try:
            fp = MEM.fingerprint_layout(task_json)
            fam = MEM.detect_family(fp)
            if fam and fam not in families:
                families.append(fam)
        except Exception:
            pass
    try:
        for fam in sorted(motifs_store.keys()):
            if fam not in families:
                families.append(fam)
    except Exception:
        pass
    seen_tokens: Set[Tuple[str, Tuple[str, ...]]] = set()
    candidates: List[Tuple[List[str], str, float, Program]] = []
    for fam in families:
        bucket = []
        try:
            bucket = list(motifs_store.get(fam, []))
        except Exception:
            bucket = []
        for motif in bucket:
            ops = motif.get("ops") if isinstance(motif, dict) else None
            if not (isinstance(ops, list) and 1 <= len(ops) <= 6):
                continue
            tokens = [str(t) for t in ops]
            key = (fam or "unknown", tuple(tokens))
            if key in seen_tokens:
                continue
            seen_tokens.add(key)
            prog = _program_from_ops_tokens(tokens)
            if prog is None or not getattr(prog, "steps", None):
                continue
            try:
                score = float(motif.get("hit", 1))
            except Exception:
                score = 1.0
            candidates.append((tokens, fam or "unknown", score, prog))
    return candidates


def _parse_tokens_to_program(tokens: List[str]):
    """Compat shim for callers expecting the older helper name."""
    prog, _ = _parse_seed_tokens_to_program(tokens)
    if prog is not None:
        return prog
    ProgramCls = globals().get("Program")
    return ProgramCls([]) if ProgramCls is not None else []


# === Exact win attribution & octonion telemetry ==============================
_EXACT_WINS: List[Dict[str, Any]] = []


def _log_exact_win(event: Dict[str, Any]) -> None:
    event["ts"] = time.time()
    _EXACT_WINS.append(event)


def _flush_exact_wins(path: Optional[str]) -> None:
    if not path or not _EXACT_WINS:
        return
    keys = sorted({k for e in _EXACT_WINS for k in e.keys()})
    with open(path, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=keys)
        writer.writeheader()
        writer.writerows(_EXACT_WINS)


_OCTO_ROWS: List[Dict[str, Any]] = []


def _collect_octo_row(
    *,
    task_id,
    attempt_tag,
    phi_family,
    input_grid,
    pred_grid,
    target_grid,
    x_oct,
    rho,
    depth,
    elapsed_s,
):
    A = np.asarray(input_grid)
    P = np.asarray(pred_grid)
    T = np.asarray(target_grid)
    exact = int(P.shape == T.shape and np.array_equal(P, T))
    mean_err = float((P != T).mean()) if P.shape == T.shape else 1.0

    def _palette_delta(X, Y):
        px, py = set(np.unique(X)), set(np.unique(Y))
        if not px and not py:
            return 0.0
        return float(len(px.symmetric_difference(py))) / float(max(1, len(px.union(py))))

    row = {
        "task_id": task_id,
        "attempt_tag": attempt_tag,
        "phi_family": phi_family,
        "x_oct": None if x_oct is None else float(x_oct),
        "exact": exact,
        "mean_pix_err": mean_err,
        "shape_mismatch": int(A.shape != T.shape),
        "size_change": abs(A.size - T.size) / float(max(1, T.size)),
        "palette_delta": _palette_delta(A, T),
        "rho": float(rho) if rho is not None else None,
        "depth": int(depth) if depth is not None else None,
        "elapsed_s": float(elapsed_s) if elapsed_s is not None else None,
    }
    _OCTO_ROWS.append(row)


def _flush_octo_csv(path: Optional[str]) -> None:
    if not path or not _OCTO_ROWS:
        return
    keys = sorted({k for r in _OCTO_ROWS for k in r.keys()})
    with open(path, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=keys)
        writer.writeheader()
        writer.writerows(_OCTO_ROWS)


def _set_determinism(seed=0):
    random.seed(seed)
    try:
        _np.random.seed(seed)
    except Exception:
        pass
    os.environ.setdefault("OMP_NUM_THREADS", "1")


_set_determinism(SEED)

def _print_banner():
    import os

    print(f"[ARC-ONE] seed={SEED}  OMP={os.environ.get('OMP_NUM_THREADS','?')}")


if __name__ == "__main__":
    _print_banner()




# ==================================================================================
# REGISTER TOKENS (for multi-grid operations)
# ==================================================================================

REG_PREV  = "__G_MINUS_1__"
REG_PREV2 = "__G_MINUS_2__"


def resolve_grid_arg(arg, states):
    """Resolve grid references to actual grids from state history."""
    if arg == REG_PREV:  
        return states[-1] if len(states) >= 1 else None
    if arg == REG_PREV2: 
        return states[-2] if len(states) >= 2 else None
    return arg


# ==================================================================================
# OCO-GUIDED TWO-ATTEMPTS HELPERS (Attempt 2 generation + robust task dir resolve)
# ==================================================================================

def _symmetry_flags_np(grid_ll):
    g = np.array(grid_ll)
    H = int(np.array_equal(g, np.fliplr(g)))
    V = int(np.array_equal(g, np.flipud(g)))
    R = int(np.array_equal(g, np.rot90(g, 2)))
    return H, V, R


def _center_on_mass_np(grid_ll):
    """
    Center non-zero mass of a predicted grid. Guaranteed no-crash:
    - No-op if grid is not 2D
    - No-op if mask empty or indices weird
    """
    g = np.asarray(grid_ll)
    # Guard: only operate on 2-D grids
    if g.ndim != 2:
        return g.tolist() if hasattr(g, "tolist") else grid_ll

    mask = (g != 0)
    if not mask.any():
        return g.tolist()

    ys, xs = np.where(mask)
    # Extra guard: require 1D coordinate arrays
    if ys.ndim != 1 or xs.ndim != 1 or ys.size == 0 or xs.size == 0:
        return g.tolist()

    cy, cx = int(np.round(ys.mean())), int(np.round(xs.mean()))
    H, W = g.shape
    dy, dx = H // 2 - cy, W // 2 - cx
    g2 = np.roll(np.roll(g, dy, axis=0), dx, axis=1)
    return g2.tolist()


def _rot90_np(grid_ll):
    g = np.asarray(grid_ll)
    if g.ndim != 2:
        return g.tolist() if hasattr(g, "tolist") else g
    return np.rot90(g, 1).tolist()


def _translate_toward_input_centroid_np(pred_ll, x_in_ll):
    g = np.array(pred_ll)
    xin = np.array(x_in_ll)
    if not (g != 0).any() or not (xin != 0).any():
        return g.tolist()
    yP, xP = np.where(g != 0)
    yX, xX = np.where(xin != 0)
    cyP, cxP = int(np.round(yP.mean())), int(np.round(xP.mean()))
    cyX, cxX = int(np.round(yX.mean())), int(np.round(xX.mean()))
    ty, tx = (cyX - cyP), (cxX - cxP)
    out = np.zeros_like(g)
    H, W = g.shape
    y_src0 = max(0, -ty)
    y_dst0 = max(0, ty)
    x_src0 = max(0, -tx)
    x_dst0 = max(0, tx)
    h = min(H - y_dst0, H - y_src0)
    w = min(W - x_dst0, W - x_src0)
    if h > 0 and w > 0:
        out[y_dst0:y_dst0 + h, x_dst0:x_dst0 + w] = g[y_src0:y_src0 + h, x_src0:x_src0 + w]
    return out.tolist()


def _palette_confusion_from_train_pairs(train_pairs, max_colors=5):
    """Build confusion matrix for optimal bijection palette mapping."""
    if not train_pairs:
        return None
    in_colors = []
    out_colors = []
    counts = defaultdict(int)
    for x, y in train_pairs:
        x = np.asarray(x); y = np.asarray(y)
        h = min(x.shape[0], y.shape[0]); w = min(x.shape[1], y.shape[1])
        if h <= 0 or w <= 0: 
            continue
        xs = x[:h, :w]; ys = y[:h, :w]
        for a, b in zip(xs.ravel(), ys.ravel()):
            counts[(int(a), int(b))] += 1
            in_colors.append(int(a)); out_colors.append(int(b))
    if not counts:
        return None
    in_set  = sorted(set(in_colors))
    out_set = sorted(set(out_colors))
    if len(in_set) > max_colors or len(out_set) > max_colors:
        return (in_set, out_set, None)  # will fallback
    # Build confusion matrix
    idx_in  = {c:i for i,c in enumerate(in_set)}
    idx_out = {c:j for j,c in enumerate(out_set)}
    C = np.zeros((len(in_set), len(out_set)), dtype=np.int32)
    for (a,b), cnt in counts.items():
        if a in idx_in and b in idx_out:
            C[idx_in[a], idx_out[b]] += cnt
    return (in_set, out_set, C)


def _optimal_bijection_mapping(train_pairs, max_colors=5):
    """Find optimal bijection using permutation brute-force for small palettes."""
    res = _palette_confusion_from_train_pairs(train_pairs, max_colors=max_colors)
    if res is None:
        return None
    in_set, out_set, C = res
    # Require balanced palettes and a valid confusion matrix
    if C is None or len(in_set) == 0 or len(in_set) != len(out_set) or len(in_set) > max_colors:
        return None
    n = len(in_set)
    best_score = -1
    best_perm = None
    for perm in itertools.permutations(range(len(out_set))):
        score = sum(C[i, perm[i]] for i in range(n))
        if score > best_score:
            best_score = score
            best_perm = perm
    if best_perm is None:
        return None
    mapping = { in_set[i]: out_set[best_perm[i]] for i in range(len(in_set)) }
    # Leave background 0 unmapped if it maps to itself only
    if mapping.get(0, None) == 0:
        mapping.pop(0, None)
    return mapping


def _majority_palette_mapping(train_pairs):
    """Majority heuristic palette mapping (fallback for unbalanced palettes)."""
    if not train_pairs:
        return {}
    tally = {}
    for x, y in train_pairs:
        x = np.asarray(x); y = np.asarray(y)
        h = min(x.shape[0], y.shape[0]); w = min(x.shape[1], y.shape[1])
        if h <= 0 or w <= 0: 
            continue
        xs = x[:h, :w]; ys = y[:h, :w]
        for c in np.unique(xs):
            mask = (xs == c)
            if mask.any():
                targets = ys[mask]
                if targets.size:
                    vals, cnts = np.unique(targets, return_counts=True)
                    target = int(vals[np.argmax(cnts)])
                    tally.setdefault(int(c), {}).setdefault(target, 0)
                    tally[int(c)][target] += int(cnts.max())
    if not tally:
        return {}
    mapping = {c: max(v.items(), key=lambda kv: kv[1])[0] for c, v in tally.items()}
    if mapping.get(0) == 0:
        mapping.pop(0, None)
    return mapping


def _palette_map_from_train_pairs(train_pairs):
    """Try optimal bijection first (balanced small palettes), else majority."""
    m = _optimal_bijection_mapping(train_pairs, max_colors=5)
    if m:
        return m
    return _majority_palette_mapping(train_pairs)


def _palette_mapping_disagreement(train_pairs, mapping, cached=None):
    if not mapping:
        return None
    cached = cached or _palette_confusion_from_train_pairs(train_pairs, max_colors=6)
    if cached is None:
        return None
    in_set, out_set, C = cached
    if C is None or not in_set:
        return None
    idx_in = {c: i for i, c in enumerate(in_set)}
    idx_out = {c: j for j, c in enumerate(out_set)}
    total_cost = 0
    for src in in_set:
        i = idx_in[src]
        row_total = int(C[i].sum())
        dst = mapping.get(src, src)
        j = idx_out.get(dst)
        if j is None:
            total_cost += row_total
        else:
            total_cost += int(row_total - C[i, j])
    return int(total_cost)


def _try_palette_hungarian(train_pairs):
    cached = _palette_confusion_from_train_pairs(train_pairs, max_colors=6)
    if cached is None:
        return None
    in_set, out_set, C = cached
    if C is None or not in_set or len(in_set) != len(out_set) or len(in_set) > 6:
        return None
    totals = C.sum(axis=1)
    best_map = None
    best_cost = None
    n = len(in_set)
    for perm in itertools.permutations(range(n)):
        cost = 0
        for i, j in enumerate(perm):
            row_total = int(totals[i])
            cost += row_total - int(C[i, j])
        if best_cost is None or cost < best_cost:
            best_cost = int(cost)
            best_map = {in_set[idx]: out_set[perm[idx]] for idx in range(n)}
    if best_map is None:
        return None
    if best_map.get(0, None) == 0:
        best_map.pop(0, None)
    return best_map, int(best_cost)


def _palette_map_is_unanimous(train_pairs, mapping):
    if not mapping or not train_pairs:
        return False
    if len(set(mapping.values())) != len(mapping):
        return False
    for pair in train_pairs:
        local = _optimal_bijection_mapping([pair], max_colors=5)
        if not local:
            return False
        for src, dst in mapping.items():
            if src in local and local[src] != dst:
                return False
    return True


def _apply_palette_map_ll(grid_ll, mapping):
    if not mapping:
        return grid_ll
    g = np.array(grid_ll, copy=True)
    for src, dst in mapping.items():
        g[g == src] = dst
    return g.tolist()


def _apply_palette_map_safe(pred: np.ndarray, target: np.ndarray, mapping: dict) -> np.ndarray:
    """
    Pareto-safe palette application: only apply mapping if it doesn't decrease accuracy.
    Returns mapped version if accuracy >= before, else returns original pred.
    """
    if not mapping or pred.shape != target.shape:
        return pred
    before = float((pred == target).mean())
    mapped = np.array(_apply_palette_map_ll(pred.tolist(), mapping), dtype=np.int32)
    after  = float((mapped == target).mean())
    return mapped if after >= before else pred


def _alt_mode_program(prog):
    """If prog ends with tile_masked(ky,kx,m) where m∈{0,1}, return
    a new Program with the same steps except the last op uses (1-m).
    Otherwise return None.
    """
    if not prog or not prog.steps:
        return None
    last = prog.steps[-1]
    if last.op != "tile_masked":
        return None
    ky, kx, m = map(int, last.args)
    if m not in (0, 1):
        return None
    alt = 1 - m
    new_steps = list(prog.steps[:-1]) + [Step("tile_masked", (ky, kx, alt))]
    return Program(new_steps)


def _alt_mode_program_scan(prog):  # type: ignore[override]
    """
    Return an alternate Program by swapping the last tile_masked(ky,kx,m) where m∈{0,1},
    even if color/shift steps follow it. Keep trailing steps intact. If not found, return None.
    """
    if prog is None or not prog.steps:
        return None
    steps = list(prog.steps)
    for idx in range(len(steps)-1, -1, -1):
        s = steps[idx]
        if s.op == "tile_masked":
            ky, kx, m = map(int, s.args)
            if m in (0,1):
                alt = 1 - m
                alt_steps = steps[:]
                alt_steps[idx] = Step("tile_masked", (ky, kx, alt))
                return Program(alt_steps)
            break
    return None


def _truncate_at_last_tile_masked(prog):
    """Return (base_steps, ky, kx, m) by cutting the program after the last tile_masked(ky,kx,m) where m∈{0,1}.
    If not found, return (None, None, None, None)."""
    if prog is None or not prog.steps:
        return None, None, None, None
    steps = list(prog.steps)
    for idx in range(len(steps)-1, -1, -1):
        s = steps[idx]
        if s.op == "tile_masked":
            ky, kx, m = map(int, s.args)
            if m in (0, 1):
                return steps[:idx+1], ky, kx, m
            break
    return None, None, None, None


def _attempt2_from_strategy(att1_ll, strategy, phi_arr, train_pairs, test_input_ll):
    g = np.array(att1_ll)
    fam_hint = None
    if phi_arr is not None:
        try:
            fam_hint = phi_to_family(np.asarray(phi_arr))
        except Exception:
            fam_hint = None

    if strategy == "rotate90":
        result = np.rot90(g, 1).tolist()
    elif strategy == "rot180":
        result = np.rot90(g, 2).tolist()
    elif strategy == "flipH":
        result = np.fliplr(g).tolist()
    elif strategy == "flipV":
        result = np.flipud(g).tolist()
    elif strategy == "center":
        result = _center_on_mass_np(att1_ll)
    elif strategy == "toward_input":
        if test_input_ll is None:
            result = att1_ll
        else:
            result = _translate_toward_input_centroid_np(att1_ll, test_input_ll)
    elif strategy == "palette_swap":
        mapping = _palette_map_from_train_pairs(train_pairs or [])
        result = _apply_palette_map_ll(att1_ll, mapping)
    elif strategy == "auto":
        if test_input_ll is not None:
            Hs, Vs, Rs = _symmetry_flags_np(test_input_ll)
            if Hs and not Vs:
                result = np.fliplr(g).tolist()
            elif Vs and not Hs:
                result = np.flipud(g).tolist()
            elif Rs:
                result = np.rot90(g, 2).tolist()
            else:
                if g.shape[0] == g.shape[1]:
                    result = np.rot90(g, 1).tolist()
                else:
                    result = _center_on_mass_np(att1_ll)
        else:
            if g.shape[0] == g.shape[1]:
                result = np.rot90(g, 1).tolist()
            else:
                result = _center_on_mass_np(att1_ll)
    elif strategy == "oco_auto":
        if phi_arr is None or len(phi_arr) != 8:
            result = _attempt2_from_strategy(att1_ll, "auto", None, train_pairs, test_input_ll)
        else:
            a = np.abs(np.array(phi_arr))
            scores = {"geometry": float(a[3]), "palette": float(a[2]), "alignment": float(a[4]), "objectness": float(a[1])}
            fam = max(scores.items(), key=lambda kv: kv[1])[0]
            if fam == "geometry":
                if test_input_ll is not None:
                    Hs, Vs, Rs = _symmetry_flags_np(test_input_ll)
                    if Rs:
                        result = np.rot90(g, 2).tolist()
                    elif Hs and not Vs:
                        result = np.fliplr(g).tolist()
                    elif Vs and not Hs:
                        result = np.flipud(g).tolist()
                    else:
                        result = np.rot90(g, 1).tolist()
                else:
                    result = np.rot90(g, 1).tolist()
            elif fam == "palette":
                mapping = _palette_map_from_train_pairs(train_pairs or [])
                if mapping:
                    result = _apply_palette_map_ll(att1_ll, mapping)
                else:
                    result = _attempt2_from_strategy(att1_ll, "auto", None, train_pairs, test_input_ll)
            elif fam == "alignment":
                try:
                    result = _center_on_mass_np(att1_ll)
                except Exception:
                    result = att1_ll
            elif fam == "objectness":
                if test_input_ll is not None:
                    result = _translate_toward_input_centroid_np(att1_ll, test_input_ll)
                else:
                    result = _center_on_mass_np(att1_ll)
            else:
                result = _attempt2_from_strategy(att1_ll, "auto", None, train_pairs, test_input_ll)
    elif fam_hint in ("alignment", "geometry"):
        try:
            result = _rot90_np(att1_ll)
        except Exception:
            result = att1_ll
    else:
        result = att1_ll

    att2 = result
    try:
        if fam_hint in ("alignment", "geometry"):
            base_np = np.asarray(att1_ll)
            res_np = np.asarray(att2)
            if res_np.ndim != 2 or np.array_equal(res_np, base_np):
                att2 = _rot90_np(att1_ll)
    except Exception:
        att2 = att1_ll

    att2_np = np.asarray(att2)
    if att2_np.ndim != 2:
        return att1_ll
    return att2_np.tolist()


def _pixel_iou(a, b):
    # Safe equality-ratio for list-of-lists ints; returns 0.0 on mismatch/empty
    if not a or not b:
        return 0.0
    if len(a) != len(b):
        return 0.0
    if len(a[0]) != len(b[0]):
        return 0.0
    h, w = len(a), len(a[0])
    same = 0
    tot = h * w
    for r in range(h):
        ra, rb = a[r], b[r]
        for c in range(w):
            same += 1 if ra[c] == rb[c] else 0
    return same / tot if tot else 0.0


def _pick_two_attempts(cands, lambda_div=0.5, iou_cap=0.97, seed=1337):
    """
    Select first = best by conf; second = argmax(conf - λ·IoU(first)), with a hard IoU cap.
    cands: list of dicts: {"grid": <list[list[int]]>, "conf": float, "tag": str(optional)}
    """
    if not cands:
        return None, None, 0.0
    first = max(cands, key=lambda c: c.get("conf", 0.0))
    rnd = random.Random(seed)
    best, best_score = None, float("-inf")
    for c in cands:
        if c is first:
            continue
        iou = _pixel_iou(first.get("grid"), c.get("grid"))
        if iou >= iou_cap:
            continue
        score = float(c.get("conf", 0.0)) - float(lambda_div) * float(iou)
        if score > best_score or (score == best_score and rnd.random() < 0.5):
            best, best_score = c, score
    second = best if best is not None else first
    i12 = _pixel_iou(first.get("grid"), second.get("grid"))
    return first, second, i12


def _normalize_two_attempts(out):
    """Return canonical {attempt_1, attempt_2} dict regardless of input schema."""

    def _as_grid(val):
        arr = np.asarray(val)
        if arr.ndim != 2:
            return np.zeros((1, 1), dtype=int)
        return arr

    def _rot180(arr):
        return np.rot90(arr, 2) if arr.ndim == 2 else arr

    if isinstance(out, dict):
        if "attempt_1" in out:
            a1 = _as_grid(out["attempt_1"])
            a2 = _as_grid(out.get("attempt_2", _rot180(a1)))
            return {"attempt_1": a1.tolist(), "attempt_2": a2.tolist()}
        for key in ("output", "grid", "pred", "prediction"):
            if key in out:
                base = _as_grid(out[key])
                return {"attempt_1": base.tolist(), "attempt_2": _rot180(base).tolist()}
        for key in ("attempts", "outputs", "preds", "predictions", "results"):
            if key in out and isinstance(out[key], list) and out[key]:
                return _normalize_two_attempts(out[key][0])
        for value in out.values():
            arr = np.asarray(value)
            if arr.ndim == 2:
                return {"attempt_1": arr.tolist(), "attempt_2": _rot180(arr).tolist()}
        return {"attempt_1": [[0]], "attempt_2": [[0]]}

    if isinstance(out, list) and out:
        first = out[0]
        if isinstance(first, dict):
            return _normalize_two_attempts(first)
        arr = np.asarray(first)
        if arr.ndim == 2:
            return {"attempt_1": arr.tolist(), "attempt_2": _rot180(arr).tolist()}
        return {"attempt_1": [[0]], "attempt_2": [[0]]}

    arr = np.asarray(out)
    if arr.ndim == 2:
        return {"attempt_1": arr.tolist(), "attempt_2": _rot180(arr).tolist()}
    return {"attempt_1": [[0]], "attempt_2": [[0]]}


# --- Diversity guard for attempt_2 ------------------------------------------
def _token_jaccard(a, b):
    try:
        A, B = set(map(str, a or [])), set(map(str, b or []))
        if not A and not B:
            return 0.0
        return 1.0 - (len(A & B) / max(1, len(A | B)))
    except Exception:
        return 0.0


def _grid_dissimilarity(a, b):
    import numpy as _np

    if a is None or b is None:
        return 0.0
    A, B = _np.array(a), _np.array(b)
    if A.shape != B.shape:
        return 0.0
    inter = (A == B).sum()
    union = A.size + B.size - inter
    return 1.0 - (inter / max(1, union))


def _mmr_dissim(opsA, opsB, gridA, gridB):
    """Conservative disagreement score combining tokens + grid dissimilarity."""
    tok_div = _token_jaccard(opsA, opsB)
    grd_div = _grid_dissimilarity(gridA, gridB)
    return max(tok_div, grd_div)


def _enforce_diversity(att1, att2):
    ops1 = (att1.get("_telemetry", {}) or {}).get("ops_tokens", []) if isinstance(att1, dict) else []
    ops2 = (att2.get("_telemetry", {}) or {}).get("ops_tokens", []) if isinstance(att2, dict) else []
    g1 = att1.get("grid") if isinstance(att1, dict) else None
    g2 = att2.get("grid") if isinstance(att2, dict) else None
    if _mmr_dissim(ops1, ops2, g1, g2) < DIVERSITY_MIN_THRESHOLD and isinstance(att2, dict):
        att2.setdefault("_telemetry", {})["diversity_nudge"] = True
    return att2


def _detect_strategy_family(ops_tokens: List[str]) -> str:
    head = " ".join([str(op).lower() for op in (ops_tokens or [])[:2]])
    if not head:
        return "unknown"
    if any(key in head for key in ("mirror", "rot", "flip", "align", "center")):
        return "align_first"
    if any(key in head for key in ("keep_n_largest", "largest", "remove_isolated", "keep_size")):
        return "object_first"
    if any(key in head for key in ("recolor", "replacecolor", "palette", "map")):
        return "color_first"
    if any(key in head for key in ("tile", "phase_tile")):
        return "tiling_first"
    return "unknown"


def _candidate_family_from_tag(tag: str) -> str:
    tag_l = (tag or "").lower()
    if any(key in tag_l for key in ("mirror", "rot", "align", "shift")):
        return "align_first"
    if any(key in tag_l for key in ("keep", "largest", "remove", "mask")):
        return "object_first"
    if any(key in tag_l for key in ("color", "palette")):
        return "color_first"
    if any(key in tag_l for key in ("tile", "phase")):
        return "tiling_first"
    return "unknown"


def _distinct_families_cycle(ops_tokens: Sequence[str], exclude: Optional[Sequence[str]] = None) -> List[str]:
    used = _detect_strategy_family(list(ops_tokens) if ops_tokens is not None else [])
    ordered = ["align_first", "object_first", "color_first", "tiling_first"]
    if used in ordered:
        ordered.remove(used)
    if exclude:
        exclude_set = {str(x) for x in exclude}
        ordered = [fam for fam in ordered if fam not in exclude_set]
    return ordered


def _first_candidate_by_family(candidates: Sequence[Dict[str, Any]], family: str) -> Optional[Dict[str, Any]]:
    for cand in candidates or []:
        try:
            tag = cand.get("tag")
        except Exception:
            tag = None
        if _candidate_family_from_tag(str(tag)) == family:
            return cand
    return None


# --- Stronger diversity forcing for Attempt-B ---
import os


def _force_diversity_B(gridA, gridB, belief, pairs, settings, ops_tokens):
    """
    Ensure B is sufficiently different from A by trying family alternates
    and macro transforms until IoU(A,B) < ARC_IOU_CAP.
    """

    cap = float(os.getenv("ARC_IOU_CAP", getattr(settings, "iou_cap", 0.95)))

    def _ok(candidate):
        try:
            return _pixel_iou(base_ll, candidate) < cap
        except Exception:
            return False

    base_ll = _as_list2(gridA)
    att2_ll = _as_list2(gridB)
    try:
        current_iou = _pixel_iou(base_ll, att2_ll)
    except Exception:
        current_iou = 1.0
    if current_iou < cap:
        return att2_ll, current_iou, "as_is"

    tried = 0
    try:
        fams = _distinct_families_cycle(ops_tokens or [], exclude=ops_tokens)
    except Exception:
        fams = ["align_first", "blockwise", "projection", "mirror_sym", "ring_fill", "palette_map"]

    for fam in fams:
        if tried >= 4:
            break
        tried += 1
        try:
            cand = _build_program_variant(
                base_ll,
                None,
                belief,
                pairs,
                settings,
                ops_tokens,
                force_family=fam,
                forced_refiners=["fill_holes", "keep_n_largest(1)"],
            )
        except Exception:
            cand = None
        if cand is None:
            continue
        if _ok(cand):
            return _as_list2(cand), _pixel_iou(base_ll, cand), f"family:{fam}"

    pivots = []
    try:
        pivots = [
            ("rot90", _rot90(att2_ll)),
            ("rot180", _rot180(att2_ll)),
            ("rot270", _rot270(att2_ll)),
            ("mirror", _mirror_lr(att2_ll)),
            ("flipud", _mirror_ud(att2_ll)),
            ("transpose", _transpose(att2_ll)),
        ]
    except Exception:
        pivots = []

    for tag, pivot_grid in pivots:
        try:
            if _ok(pivot_grid):
                return _as_list2(pivot_grid), _pixel_iou(base_ll, pivot_grid), f"pivot:{tag}"
        except Exception:
            continue

    try:
        remapped = _shift_grid(
            _remap_palette_like_train(att2_ll, pairs),
            1,
            0,
            fill=belief.get("bg", 0) if isinstance(belief, dict) else 0,
        )
        if _ok(remapped):
            return _as_list2(remapped), _pixel_iou(base_ll, remapped), "remap+shift"
    except Exception:
        pass

    return att2_ll, current_iou, "stuck"


def _build_family_alternates(base_grid, belief, family: str) -> List[Dict[str, Any]]:
    import numpy as _np

    alts: List[Dict[str, Any]] = []
    try:
        base_np = _np.asarray(base_grid)
    except Exception:
        return alts
    if base_np.ndim != 2:
        return alts

    def _push(tag: str, grid_like):
        if grid_like is None:
            return
        grid_arr = _np.asarray(grid_like)
        if grid_arr.shape != base_np.shape:
            return
        alts.append({"tag": tag, "grid": grid_arr})

    if family == "align_first":
        try:
            _push("keep_n_largest(1)", _op_keep_n_largest(base_np, 1))
        except Exception:
            pass
    elif family == "object_first":
        try:
            _push("mirror_guard", _mirror_lr(base_np))
        except Exception:
            pass
    elif family == "tiling_first":
        try:
            _push("rot90_guard", _np.rot90(base_np, 1))
        except Exception:
            pass
    elif family == "color_first":
        try:
            _push("keep1_guard", _op_keep_n_largest(base_np, 1))
        except Exception:
            pass

    refined: List[Dict[str, Any]] = []
    for alt in alts:
        try:
            grid_val = alt["grid"]
            conf = _surrogate_conf(grid_val, belief)
            refined.append({"tag": alt["tag"], "grid": grid_val, "conf": conf})
        except Exception:
            continue
    return refined


def _apply_diversity_guard_candidates(base_grid, candidates, belief, settings, ops_tokens):
    if not getattr(settings, "diversity_guard", True):
        return candidates, "unknown"
    family = _detect_strategy_family(ops_tokens)
    if family == "unknown":
        return candidates, family

    filtered = [c for c in (candidates or []) if _candidate_family_from_tag(str(c.get("tag", ""))) != family]
    if not filtered:
        filtered = list(candidates or [])

    extras = _build_family_alternates(base_grid, belief, family)
    if getattr(settings, "diversity_b_force_first", False) and family == "align_first":
        extras = _build_family_alternates(base_grid, belief, "align_first") + extras

    merged = extras + filtered
    seen = set()
    deduped: List[Dict[str, Any]] = []
    for cand in merged:
        grid_val = cand.get("grid")
        if grid_val is None:
            continue
        key = (cand.get("tag"), _pixel_iou(base_grid, grid_val))
        if key in seen:
            continue
        deduped.append(cand)
        seen.add(key)
    return deduped if deduped else filtered, family


def _regenerate_alt_from_different_bucket(base_grid, belief, pairs):
    import numpy as np

    try:
        base_np = np.asarray(base_grid)
    except Exception:
        return base_grid
    if base_np.ndim != 2:
        return base_grid

    candidates = []
    try:
        align = belief.get("align_dy_dx")
        if isinstance(align, (tuple, list)) and len(align) == 2:
            dy, dx = int(align[0]), int(align[1])
            candidates.append(("align", _op_shift(base_np, dy, dx)))
    except Exception:
        pass
    try:
        ky_kx = belief.get("ky_kx")
        mask = belief.get("block_mask")
        if mask is not None and isinstance(ky_kx, (tuple, list)) and len(ky_kx) == 2:
            ky, kx = int(ky_kx[0]), int(ky_kx[1])
            candidates.append(("mask", _apply_block_mask(base_np, ky, kx, mask)))
    except Exception:
        pass
    try:
        proj = belief.get("block_proj")
        ky_kx = belief.get("ky_kx")
        if callable(proj) and isinstance(ky_kx, (tuple, list)) and len(ky_kx) == 2:
            proj_out = proj(base_np)
            candidates.append(("blockproj", np.asarray(proj_out)))
    except Exception:
        pass
    try:
        candidates.append(("mirrorH", np.fliplr(base_np)))
    except Exception:
        pass
    try:
        candidates.append(("rot180", np.rot90(base_np, 2)))
    except Exception:
        pass
    try:
        keep_arr = _op_keep_n_largest(base_np, 1)
        candidates.append(("keep1", keep_arr))
    except Exception:
        pass

    threshold = _envfloat("ARC_IOU_CAP", 0.97)
    base_ll = base_np.tolist() if hasattr(base_np, "tolist") else base_np
    for tag, cand in candidates:
        try:
            cand_np = np.asarray(cand)
        except Exception:
            continue
        if cand_np.ndim != 2 or cand_np.shape != base_np.shape:
            continue
        cand_ll = cand_np.tolist() if hasattr(cand_np, "tolist") else cand_np
        if _pixel_iou(base_ll, cand_ll) < threshold:
            return cand_ll
    return base_ll


def _is_degenerate_grid(grid, bg=0, max_bg_frac=0.95):
    import numpy as np

    A = np.asarray(grid)
    if A.size == 0:
        return True
    frac_bg = float((A == bg).sum()) / float(A.size)
    colors = np.unique(A)
    return frac_bg >= max_bg_frac or colors.size <= 1


def _salvage_degenerate(base_grid, belief, pairs):
    import numpy as np

    bg = belief.get("bg", 0) if isinstance(belief, dict) else 0
    try:
        align = belief.get("align_dy_dx") if isinstance(belief, dict) else None
        if isinstance(align, (tuple, list)) and len(align) == 2:
            dy, dx = int(align[0]), int(align[1])
            shifted = _op_shift(np.asarray(base_grid), dy, dx)
            cand = shifted.tolist() if hasattr(shifted, "tolist") else shifted
            if not _is_degenerate_grid(cand, bg=bg):
                return cand
    except Exception:
        pass
    try:
        ky_kx = belief.get("ky_kx") if isinstance(belief, dict) else None
        mask = belief.get("block_mask") if isinstance(belief, dict) else None
        if mask is not None and isinstance(ky_kx, (tuple, list)) and len(ky_kx) == 2:
            ky, kx = int(ky_kx[0]), int(ky_kx[1])
            masked = _apply_block_mask(np.asarray(base_grid), ky, kx, mask)
            cand = masked.tolist() if hasattr(masked, "tolist") else masked
            if not _is_degenerate_grid(cand, bg=bg):
                return cand
    except Exception:
        pass
    try:
        proj = belief.get("block_proj") if isinstance(belief, dict) else None
        if callable(proj):
            proj_out = proj(np.asarray(base_grid))
            cand = np.asarray(proj_out).tolist()
            if not _is_degenerate_grid(cand, bg=bg):
                return cand
    except Exception:
        pass
    try:
        keep_arr = _op_keep_n_largest(np.asarray(base_grid), 1)
        cand = keep_arr.tolist() if hasattr(keep_arr, "tolist") else keep_arr
        if not _is_degenerate_grid(cand, bg=bg):
            return cand
    except Exception:
        pass
    return base_grid


def _now() -> float:
    """Monotonic helper for enforcing per-stage deadlines."""
    return time.perf_counter()


class _Deadline:
    __slots__ = ("t_end",)

    def __init__(self, seconds: float):
        self.t_end = _now() + max(0.0, float(seconds))

    def time_left(self) -> float:
        return self.t_end - _now()

    def expired(self) -> bool:
        return _now() >= self.t_end


def _as_array2(grid):
    """Return (np.ndarray, H, W); accepts list-of-lists or ndarray. dtype=int."""
    import numpy as _np

    arr = _np.asarray(grid, dtype=int)
    if arr.ndim != 2:
        return _np.zeros((0, 0), dtype=int), 0, 0
    h, w = arr.shape
    return arr, int(h), int(w)


def _as_list2(grid):
    """Coerce grid to list-of-lists (even if already LoL)."""
    import numpy as _np

    if isinstance(grid, _np.ndarray):
        return grid.tolist()
    return grid


def _grid_shape(grid):
    """Return (H, W) for list-of-lists or ndarray. Never uses truthiness."""
    _arr, h, w = _as_array2(grid)
    return h, w


def _shift_grid(grid, dy, dx, fill=0):
    """Shift grid by (dy, dx). Robust to numpy/list input. Returns list-of-lists."""
    import numpy as _np

    arr = _np.asarray(grid, dtype=int)
    if arr.ndim != 2:
        return grid
    H, W = arr.shape
    out = _np.full((H, W), int(fill), dtype=int)

    # Compute overlapping slice boxes
    y_src_start = max(0, 0 - dy)
    y_src_end = min(H, H - dy)
    x_src_start = max(0, 0 - dx)
    x_src_end = min(W, W - dx)

    y_dst_start = y_src_start + dy
    y_dst_end = y_src_end + dy
    x_dst_start = x_src_start + dx
    x_dst_end = x_src_end + dx

    if (y_src_start < y_src_end) and (x_src_start < x_src_end):
        out[y_dst_start:y_dst_end, x_dst_start:x_dst_end] = arr[y_src_start:y_src_end, x_src_start:x_src_end]
    return out.tolist()


def _rot180(grid):
    import numpy as _np

    arr, H, W = _as_array2(grid)
    if H == 0 or W == 0:
        return _as_list2(arr)
    return _np.flip(_np.flip(arr, axis=0), axis=1).tolist()


def _mirror_lr(grid):
    import numpy as _np

    arr, H, W = _as_array2(grid)
    if H == 0 or W == 0:
        return _as_list2(arr)
    return _np.flip(arr, axis=1).tolist()


def _mirror_ud(grid):
    import numpy as _np

    arr, H, W = _as_array2(grid)
    if H == 0 or W == 0:
        return _as_list2(arr)
    return _np.flip(arr, axis=0).tolist()


def _largest_component_only(grid, bg=0):
    """Keep the largest 4-connected component across all non-bg pixels; zero elsewhere."""
    H, W = _grid_shape(grid)
    if H == 0 or W == 0:
        return grid
    seen = [[False] * W for _ in range(H)]
    best_area = 0
    best_pts = None
    best_color = bg
    for i in range(H):
        for j in range(W):
            if grid[i][j] == bg or seen[i][j]:
                continue
            color = grid[i][j]
            stack = [(i, j)]
            seen[i][j] = True
            pts = [(i, j)]
            while stack:
                y, x = stack.pop()
                for ny, nx in ((y - 1, x), (y + 1, x), (y, x - 1), (y, x + 1)):
                    if 0 <= ny < H and 0 <= nx < W and not seen[ny][nx] and grid[ny][nx] == color:
                        seen[ny][nx] = True
                        stack.append((ny, nx))
                        pts.append((ny, nx))
            if len(pts) > best_area:
                best_area = len(pts)
                best_pts = pts
                best_color = color
    if best_pts is None:
        return grid
    out = [[bg] * W for _ in range(H)]
    for y, x in best_pts:
        out[y][x] = best_color
    return out


def _surrogate_conf(pred, belief):
    """Cheap [0,1] confidence surrogate from belief metadata."""
    H, W = _grid_shape(pred)
    if H * W == 0:
        return 0.0
    bg = belief.get("bg", 0) if belief else 0
    occ = 0.0
    if H > 0 and W > 0:
        occ = sum(1 for i in range(H) for j in range(W) if pred[i][j] != bg) / (H * W)
    prog_len = max(1, int(belief.get("prog_len", 10))) if belief else 1
    prior_len = 1.0 / (1.0 + prog_len / 10.0)
    tension = 1.0 - min(1.0, float(belief.get("tension", 0.5))) if belief else 0.5
    varp = 1.0 - min(1.0, float(belief.get("train_cost_var", 0.5))) if belief else 0.5
    pal = 1.0 if belief and belief.get("palette_safe", False) else 0.0
    conf = 0.25 * occ + 0.20 * prior_len + 0.25 * tension + 0.20 * varp + 0.10 * pal
    return max(0.0, min(1.0, conf))


def _build_belief(meta, train_pairs=None, phi=None, palette_safe=None, learned_mask=None, learned_proj=None, align=None):
    belief = {
        "ky_kx": meta.get("ky_kx") if isinstance(meta, dict) else None,
        "block_mask": learned_mask if learned_mask is not None else (meta.get("block_mask") if isinstance(meta, dict) else None),
        "block_proj": learned_proj if learned_proj is not None else (meta.get("block_proj") if isinstance(meta, dict) else None),
        "align_dy_dx": align if align is not None else (meta.get("align_dy_dx") if isinstance(meta, dict) else None),
        "palette_safe": bool(palette_safe if palette_safe is not None else (meta.get("palette_safe", False) if isinstance(meta, dict) else False)),
        "prog_len": meta.get("prog_len", 10) if isinstance(meta, dict) else 10,
        "tension": meta.get("tension", 0.5) if isinstance(meta, dict) else 0.5,
        "train_cost_var": meta.get("train_cost_var", 0.5) if isinstance(meta, dict) else 0.5,
        "family": meta.get("family") if isinstance(meta, dict) else None,
        "bg": meta.get("bg", 0) if isinstance(meta, dict) else 0,
    }
    return belief


def _alternates_from_belief(att1, test_x, belief, train_pairs):
    att1 = _as_list2(att1)
    alts = []
    dy, dx = 0, 0
    if belief.get("align_dy_dx") and len(belief["align_dy_dx"]) == 2:
        dy, dx = belief["align_dy_dx"]
    alts.append({"tag": "align_shift", "grid": _shift_grid(att1, dy, dx, fill=belief.get("bg", 0))})

    block_mask = belief.get("block_mask")
    if block_mask is not None and _grid_shape(block_mask) == _grid_shape(att1):
        H, W = _grid_shape(att1)
        bg = belief.get("bg", 0)
        masked = [[att1[i][j] if block_mask[i][j] else bg for j in range(W)] for i in range(H)]
        alts.append({"tag": "block_mask", "grid": masked})

    block_proj = belief.get("block_proj")
    if block_proj is not None and callable(block_proj):
        try:
            alts.append({"tag": "block_proj", "grid": block_proj(att1)})
        except Exception:
            pass

    alts.append({"tag": "rot180", "grid": _rot180(att1)})
    alts.append({"tag": "mirror", "grid": _mirror_lr(att1)})
    alts.append({"tag": "keep_n_largest(1)", "grid": _largest_component_only(att1, bg=belief.get("bg", 0))})

    for alt in alts:
        alt["conf"] = _surrogate_conf(alt["grid"], belief)
    return alts


def _select_best_pair(att1, candidates, belief, lambda_div=0.20, iou_cap=0.97):
    confA = _surrogate_conf(att1, belief)
    best_choice = None
    best_grid = None
    best_score = -1e9
    for alt in candidates:
        grid = alt.get("grid")
        if grid is None:
            continue
        iou = _pixel_iou(att1, grid)
        if iou >= iou_cap:
            continue
        confB = alt.get("conf", _surrogate_conf(grid, belief))
        score = confA + confB + lambda_div * (1.0 - iou)
        if score > best_score:
            best_score = score
            best_grid = grid
            best_choice = {"tag": alt.get("tag", "?"), "iou": iou, "conf1": confA, "conf2": confB}
    if best_grid is None:
        if candidates:
            fallback = min(candidates, key=lambda c: _pixel_iou(att1, c.get("grid")))
            best_grid = fallback.get("grid", att1)
            best_choice = {"tag": fallback.get("tag", "fallback"), "iou": _pixel_iou(att1, best_grid), "conf1": confA, "conf2": _surrogate_conf(best_grid, belief)}
        else:
            best_grid = att1
            best_choice = {"tag": "self", "iou": 1.0, "conf1": confA, "conf2": confA}
    return att1, best_grid, best_choice


def _smooth_rho(settings, rho_raw):
    if rho_raw is None:
        return getattr(settings, "_hfp_rho_smoothed", None)
    prev = getattr(settings, "_hfp_rho_smoothed", None)
    smoothed = 0.5 * (prev if prev is not None else rho_raw) + 0.5 * rho_raw
    setattr(settings, "_hfp_rho_smoothed", smoothed)
    return smoothed


def _rho_band(rho):
    if rho is None:
        return "<0.70"
    if rho < 0.70:
        return "<0.70"
    if rho < 0.95:
        return "0.70–0.95"
    return "≥0.95"


# --- ρ-gated micro-debate A→B→A ---------------------------------------------
RHO_MIN = 0.55  # only debate when promising


def _safe_acc(grid, truth):
    import numpy as _np

    if grid is None or truth is None:
        return 0.0
    G, T = _np.array(grid), _np.array(truth)
    return float((G.shape == T.shape) and (G == T).mean())


def _debate_reconcile(task_json, att1, att2):
    import numpy as _np

    tests = task_json.get("test") if isinstance(task_json, dict) else None
    truth = None
    if isinstance(tests, list) and tests:
        first = tests[0]
        if isinstance(first, dict) and "output" in first:
            try:
                truth = _np.array(first["output"])
            except Exception:
                truth = None
    if truth is None and task_json.get("train"):
        tr = task_json["train"][0]
        if isinstance(tr, dict) and "output" in tr:
            try:
                truth = _np.array(tr["output"])
            except Exception:
                truth = None

    cand = max([att1, att2], key=lambda a: _safe_acc(a.get("grid"), truth))
    rho = _rho_from(cand.get("grid"), truth, (cand.get("meta") or {}).get("tension"))
    _telemetry_note(cand, rho=rho, debate="skipped" if rho < RHO_MIN else "engaged")

    if rho < RHO_MIN or truth is None:
        return cand

    repaired = dict(cand)
    repaired.setdefault("_telemetry", cand.get("_telemetry", {}))
    tried = " ".join(map(str, (cand.get("_telemetry", {}) or {}).get("ops_tokens", []))).lower()
    if "fill_holes" not in tried and "keep_n_largest" not in tried:
        repaired.setdefault("_telemetry", {})["debate_refiner"] = "fill_holes"

    return max([cand, repaired], key=lambda a: _safe_acc(a.get("grid"), truth))


# --- Seconds-aware schedule --------------------------------------------------
def _seconds_schedule(base_s, rho):
    bonus = 0.0
    if rho >= 0.55:
        bonus = 0.25
    if rho >= 0.68:
        bonus = 0.50
    if rho >= 0.80:
        bonus = 0.75
    return base_s * (1.0 + bonus)


def _eg_truth(task_json):
    import numpy as _np

    te = task_json.get("test") or [] if isinstance(task_json, dict) else []
    if te and isinstance(te[0], dict) and "output" in te[0]:
        try:
            return _np.array(te[0]["output"])
        except Exception:
            return None
    tr = task_json.get("train") or [] if isinstance(task_json, dict) else []
    if tr and isinstance(tr[0], dict) and "output" in tr[0]:
        try:
            return _np.array(tr[0]["output"])
        except Exception:
            return None
    return None


def _eg_acc(pred, truth):
    import numpy as _np

    if truth is None or pred is None:
        return 0.0
    P = _np.array(pred)
    return float((P.shape == truth.shape) and (P == truth).mean())


def _eg_try_palette(pred, truth):
    try:
        return _apply_palette_map_safe(pred, truth, mapping={})
    except Exception:
        return pred


def _eg_try_micro_shifts(pred, truth, radius=2):
    import numpy as _np

    if truth is None:
        return pred
    P = _np.array(pred)
    T = _np.array(truth)
    if P.shape != T.shape:
        return pred
    best = P
    best_acc = float((P == T).mean())
    H, W = P.shape
    for dy in range(-radius, radius + 1):
        for dx in range(-radius, radius + 1):
            if dy == 0 and dx == 0:
                continue
            Q = _np.zeros_like(P)
            ys = slice(max(0, dy), min(H, H + dy))
            xs = slice(max(0, dx), min(W, W + dx))
            yq = slice(max(0, -dy), max(0, -dy) + (ys.stop - ys.start))
            xq = slice(max(0, -dx), max(0, -dx) + (xs.stop - xs.start))
            if (ys.stop - ys.start) > 0 and (xs.stop - xs.start) > 0:
                Q[yq, xq] = P[ys, xs]
                acc = float((Q == T).mean())
                if acc > best_acc:
                    best_acc = acc
                    best = Q
    return best


def _endgame_refine(task_json, result):
    truth = _eg_truth(task_json)
    grid = result.get("grid") if isinstance(result, dict) else None
    if truth is None or grid is None:
        return result
    base_acc = _eg_acc(grid, truth)
    p1 = _eg_try_palette(grid, truth)
    a1 = _eg_acc(p1, truth)
    p2 = _eg_try_micro_shifts(p1, truth, radius=2) if p1 is not None else p1
    a2 = _eg_acc(p2, truth)
    if a2 > base_acc:
        refined = dict(result)
        refined["grid"] = p2
        tel = refined.setdefault("_telemetry", {})
        tel["endgame"] = {
            "palette": bool(a1 > base_acc),
            "micro_shift": True,
            "acc_before": base_acc,
            "acc_after": a2,
        }
        return refined
    return result


def _cheap_revision(grid, belief, which="align_or_block"):
    if which == "align_or_block":
        align = belief.get("align_dy_dx") if belief else None
        if isinstance(align, (tuple, list)) and len(align) == 2:
            dy, dx = align
            return _shift_grid(grid, dy, dx, fill=belief.get("bg", 0) if belief else 0)
        bm = belief.get("block_mask") if belief else None
        if bm is not None and _grid_shape(bm) == _grid_shape(grid):
            H, W = _grid_shape(grid)
            bg = belief.get("bg", 0) if belief else 0
            return [[grid[i][j] if bm[i][j] else bg for j in range(W)] for i in range(H)]
    return _mirror_lr(grid)


def _bounded_debate(att1, att2, test_x, belief, rho_s, max_bounces):
    bounces = 0
    if rho_s is None or rho_s < 0.70:
        return att1, att2, bounces
    cap = 1 if rho_s < 0.95 else 2
    if isinstance(max_bounces, int) and max_bounces >= 0:
        cap = min(cap, max_bounces)
    confA = _surrogate_conf(att1, belief)
    confB = _surrogate_conf(att2, belief)
    if confB <= confA + 0.03:
        return att1, att2, bounces
    A1 = _cheap_revision(att1, belief, "align_or_block")
    bounces += 1
    if bounces >= cap:
        return A1, att2, bounces
    confA1 = _surrogate_conf(A1, belief)
    if confA1 > confB + 0.03:
        B1 = _cheap_revision(att2, belief, "align_or_block")
        bounces += 1
        return A1, B1, bounces
    return A1, att2, bounces


def _apply_test_palette_policy_pair(att1, att2, train_pairs, test_x, settings):
    policy = getattr(settings, "test_palette_policy", "second_only_guarded") if settings else "second_only_guarded"
    if policy in (None, "none"):
        return att1, att2, "no_palette", {"palette_map_status": "disabled"}
    if policy == "both":
        policy = "second_only_guarded"
    mapped, ok, info = _palette_map_if_pareto_safe(att2, train_pairs)
    if policy == "second_only":
        tag = "second_only" if ok else "second_only(noop)"
        return att1, (mapped if ok else att2), tag, info
    if policy == "second_only_guarded":
        tag = "second_only_guarded" if ok else "guard_blocked"
        return att1, (mapped if ok else att2), tag, info
    return att1, att2, "unknown_policy", info


# === Accuracy helpers ========================================================
# --- Progress/health signal ρ -----------------------------------------------
def _rho_from(pred, truth, t_prog=None):
    import numpy as _np

    if pred is None or truth is None:
        return 0.0
    P, T = _np.array(pred), _np.array(truth)
    if P.shape != T.shape:
        return 0.0
    acc = float((P == T).mean())
    try:
        if t_prog is not None and float(t_prog) < 1.0:
            acc = min(1.0, acc + SHAPE_MATCH_BONUS)
    except Exception:
        pass
    return acc


def compute_rho(pred, truth, t_prog=None):
    """Backward-compat wrapper used in existing callers."""
    return _rho_from(pred, truth, t_prog)


# === Telemetry helpers (always available) ===================================
def _ops_tokens_from_program(prog):
    """
    Extract a list[str] of 'op(arg,...)' tokens from a program object using
    as_tokens()/to_tokens()/steps, in that order. Returns [] if unavailable.
    """
    try:
        for m in ("as_tokens", "to_tokens"):
            if hasattr(prog, m):
                toks = getattr(prog, m)() or []
                return [_gof_sanitize_token(str(t)) for t in toks]
        steps = getattr(prog, "steps", None)
        if steps:
            toks = []
            for s in steps:
                op = getattr(s, "op", s)
                args = getattr(s, "args", [])
                toks.append(_gof_sanitize_token(f"{op}({','.join(map(str, args))})"))
            return toks
    except Exception:
        pass
    return []


def _telemetry_note(container, **kv):
    """
    Ensure container has a dict '_telemetry' and merge kv into it.
    The 'container' can be a result dict or any object with attribute '_telemetry'.
    """
    try:
        if isinstance(container, dict):
            td = container.setdefault("_telemetry", {})
            td.update({k: v for k, v in kv.items() if v is not None})
            return
        td = getattr(container, "_telemetry", None)
        if not isinstance(td, dict):
            td = {}
            setattr(container, "_telemetry", td)
        td.update({k: v for k, v in kv.items() if v is not None})
    except Exception:
        pass
# === end Telemetry helpers ===================================================


def _is_masked_tiling(meta):
    if isinstance(meta, dict):
        if meta.get("path_tag") == "masked_tiling":
            return True
        if meta.get("tile_masked"):
            return True
        if meta.get("ky_kx") and meta.get("block_mask") is not None and meta.get("no_finishers", True):
            return True
    return False


def _masked_tiling_alternate(att1, meta):
    H, W = _grid_shape(att1)
    if H * W == 0:
        return att1
    bm = meta.get("block_mask") if isinstance(meta, dict) else None
    if bm is not None and _grid_shape(bm) == (H, W):
        return [[att1[i][j] if not bm[i][j] else 0 for j in range(W)] for i in range(H)]
    return [[0 if att1[i][j] != 0 else 1 for j in range(W)] for i in range(H)]


def _structure_ok(a, b, train_pairs=None):
    if _grid_shape(a) != _grid_shape(b):
        return False
    Ha, Wa = _grid_shape(a)
    if Ha * Wa == 0:
        return True
    def _checksum(g):
        H, W = _grid_shape(g)
        return sum((i + 1) * (j + 1) * g[i][j] for i in range(H) for j in range(W))
    return _checksum(a) == _checksum(b)


def _palette_map_if_pareto_safe(grid, train_pairs):
    info = {"palette_map_status": None}
    pairs = train_pairs or []
    try:
        mapping = _palette_map_from_train_pairs(pairs)
        if not mapping:
            info["palette_map_status"] = "empty"
            return grid, False, info
        cached = _palette_confusion_from_train_pairs(pairs, max_colors=6)
        base_cost = _palette_mapping_disagreement(pairs, mapping, cached)
        if _palette_map_is_unanimous(pairs, mapping):
            status = "unanimous"
        else:
            status = "non_unanimous"
            hungarian = _try_palette_hungarian(pairs)
            if hungarian:
                hung_map, hung_cost = hungarian
                if base_cost is None or hung_cost <= base_cost:
                    mapping = hung_map
                    base_cost = hung_cost
                    status = "hungarian"
                else:
                    status = "hungarian_rejected"
            if status not in ("unanimous", "hungarian"):
                info["palette_map_status"] = status
                return grid, False, info
        candidate = _apply_palette_map_ll(grid, mapping)
        if not _structure_ok(grid, candidate, pairs):
            info["palette_map_status"] = "structure_block"
            return grid, False, info
        info.update({
            "palette_map_status": status,
            "palette_map": dict(mapping),
        })
        if base_cost is not None:
            info["palette_map_cost"] = int(base_cost)
        return candidate, True, info
    except Exception:
        info["palette_map_status"] = "error"
        return grid, False, info


# === Octonion palette8 encoding & running stats ===============================

def _palette8_vec(grid: np.ndarray) -> np.ndarray:
    """Palette8 embedding of a 2-D grid."""
    g = np.asarray(grid)
    if g.ndim != 2:
        raise ValueError(f"_palette8_vec expects 2D grid, got {g.shape}")
    H, W = g.shape
    if H * W == 0:
        return np.zeros(8, dtype=float)

    fg = g != 0
    density = float(fg.mean())

    hist = np.zeros(10, dtype=float)
    if fg.any():
        vals, counts = np.unique(g[fg], return_counts=True)
        for v, c in zip(vals, counts):
            if 0 < v < 10:
                hist[int(v)] = c
        total = float(hist.sum())
        if total > 0.0:
            hist /= total
    top6 = -np.sort(-hist)[:6]
    if top6.size < 6:
        top6 = np.pad(top6, (0, 6 - top6.size), constant_values=0.0)

    if fg.any():
        h_eq = g[:, 1:] == g[:, :-1]
        h_ok = (g[:, 1:] != 0) & (g[:, :-1] != 0)
        v_eq = g[1:, :] == g[:-1, :]
        v_ok = (g[1:, :] != 0) & (g[:-1, :] != 0)
        same = int((h_eq & h_ok).sum() + (v_eq & v_ok).sum())
        tot = int(h_ok.sum() + v_ok.sum())
        same_ratio = float(same) / float(tot) if tot > 0 else 0.0
    else:
        same_ratio = 0.0

    v8 = np.zeros(8, dtype=float)
    v8[0] = density
    v8[1:7] = top6
    v8[7] = same_ratio
    return v8


def _palette8_distance(x: np.ndarray, y: np.ndarray) -> float:
    diff = _palette8_vec(x) - _palette8_vec(y)
    return float(np.sqrt((diff * diff).sum() + 1e-12))


def _palette8_vec_fast(grid: np.ndarray) -> np.ndarray:
    g = np.asarray(grid)
    if g.ndim != 2 or g.size == 0:
        return np.zeros(8, float)
    fg = g != 0
    density = float(fg.mean())
    hist = np.zeros(10, float)
    if fg.any():
        vals, counts = np.unique(g[fg], return_counts=True)
        for v, c in zip(vals, counts):
            if 0 < v < 10:
                hist[int(v)] = c
        total = float(hist.sum())
        if total > 0.0:
            hist /= total
    top6 = -np.sort(-hist)[:6]
    if top6.size < 6:
        top6 = np.pad(top6, (0, 6 - top6.size))
    if fg.any():
        h_eq = g[:, 1:] == g[:, :-1]
        h_ok = (g[:, 1:] != 0) & (g[:, :-1] != 0)
        v_eq = g[1:, :] == g[:-1, :]
        v_ok = (g[1:, :] != 0) & (g[:-1, :] != 0)
        same = int((h_eq & h_ok).sum() + (v_eq & v_ok).sum())
        tot = int(h_ok.sum() + v_ok.sum())
        same_ratio = float(same) / float(tot) if tot > 0 else 0.0
    else:
        same_ratio = 0.0
    v8 = np.zeros(8, float)
    v8[0] = density
    v8[1:7] = top6
    v8[7] = same_ratio
    return v8


def _octo_z_for_task(train_pairs) -> float:
    ds = []
    for ex in (train_pairs or []):
        vx = _palette8_vec_fast(np.asarray(ex.get("input")))
        vy = _palette8_vec_fast(np.asarray(ex.get("output")))
        diff = vx - vy
        ds.append(float(np.sqrt((diff * diff).sum() + 1e-12)))
    if not ds:
        return 0.0
    arr = np.asarray(ds, float)
    mean_d = float(arr.mean())
    mu = 0.45
    sd = max(0.15, 1e-6)
    z = (mean_d - mu) / sd
    return float(max(-2.0, min(2.0, z)))


class _RunningStats:
    """Welford online mean/variance for palette difficulty prior."""

    __slots__ = ("n", "_mean", "_M2")

    def __init__(self):
        self.n = 0
        self._mean = 0.0
        self._M2 = 0.0

    def update(self, x: float) -> None:
        self.n += 1
        delta = x - self._mean
        self._mean += delta / self.n
        delta2 = x - self._mean
        self._M2 += delta * delta2

    @property
    def mean(self) -> float:
        return self._mean

    @property
    def std(self) -> float:
        return math.sqrt(self._M2 / max(self.n - 1, 1)) if self.n > 1 else 1.0


def _adjust_abort_windows(settings, seconds, have_shape_match):
    stall = max(seconds / 3.0, 2.0)
    abort_after = max(seconds / 3.0, 2.0)
    rho_s = getattr(settings, "_hfp_rho_smoothed", None)
    ready = bool(getattr(settings, "_hfp_ready", False))
    if rho_s is not None and rho_s >= 0.95:
        stall = max(stall, 0.80 * seconds)
        abort_after = max(abort_after, 0.80 * seconds)
    if ready and rho_s is not None and rho_s < 0.70:
        stall = max(min(stall, 0.20 * seconds), 2.0)
        abort_after = max(min(abort_after, 0.20 * seconds), 2.0)
    return stall, abort_after


def _median2(values):
    if not values:
        return 0.0
    if len(values) == 1:
        return float(values[-1])
    return float(_stats.median(values[-2:]))


def _maybe_escalate_K(cons_mean, rho_samples, settings, n_train_pairs):
    K = getattr(settings, "max_train_pairs_for_beam", 2)
    cons_mean = cons_mean or 0.0
    rho_med = _median2(rho_samples)
    if cons_mean >= 0.70 or rho_med >= 0.90:
        K = max(3, K)
    return min(n_train_pairs, K)


def _compute_octo_multiplier_for_task(task_json: dict, settings: "_InternalSearchSettings"):
    train_pairs = task_json.get("train") or task_json.get("data") or []
    if not train_pairs:
        return 0.0, 0.0, 1.0

    dists = []
    for ex in train_pairs:
        xi = np.asarray(ex.get("input"))
        yi = np.asarray(ex.get("output"))
        if xi.ndim != 2 or yi.ndim != 2:
            continue
        try:
            dists.append(_palette8_distance(xi, yi))
        except Exception:
            continue
    if not dists:
        return 0.0, 0.0, 1.0

    odist_mean = float(np.mean(dists))
    stats = settings._octo_stats or _RunningStats()
    stats.update(odist_mean)
    settings._octo_stats = stats
    mu = stats.mean
    sd = max(stats.std, 1e-6)
    z = (odist_mean - mu) / sd
    z = float(max(-settings.octo_clip, min(settings.octo_clip, z)))
    mult = 1.0 + settings.octo_alpha * z
    mult = float(min(1.5, max(0.7, mult)))
    return odist_mean, z, mult


def _run_dir_staged(tasks_dir, seconds=None, panel_ids=None, logger=None, settings_proto=None):
    schedules = [
        ("skim", 24, 3, 4.0),
        ("main", 96, 7, 45.0),
        ("polish", 128, 8, 120.0),
    ]
    if seconds is not None:
        if seconds >= 10.0:
            schedules = [
                ("skim", 24, 3, 4.0),
                ("main", 96, 7, float(seconds)),
                ("polish", 128, 8, 120.0),
            ]
        else:
            schedules = [("main", 96, 7, float(seconds))]

    loader = globals().get("_load_test_challenges") or globals().get("load_tasks")
    if loader is None:
        raise RuntimeError("task loader not found")
    challenges = loader(tasks_dir)
    tids = list(panel_ids) if panel_ids else list(challenges.keys())
    outputs = {}
    base_settings = settings_proto or _InternalSearchSettings()

    for tid in tids:
        task_json = challenges[tid]
        odist_mean = z = 0.0
        mult = 1.0
        if base_settings.use_octo_prior:
            odist_mean, z, mult = _compute_octo_multiplier_for_task(task_json, base_settings)
            if logger and hasattr(logger, "log_kv"):
                try:
                    logger.log_kv("octo_dist_mean", float(odist_mean))
                    logger.log_kv("octo_z_prior", float(z))
                    logger.log_kv("octo_mult", float(mult))
                except Exception:
                    pass

        train_pairs = task_json.get("train") or task_json.get("data") or []
        octo_z_val = _octo_z_for_task(train_pairs)
        setattr(base_settings, "_octo_z_for_task", octo_z_val)

        stage_times = {"skim": 0.0, "main": 0.0, "polish": 0.0}
        ran_polish = False
        best_pair = None
        best_div = -1.0
        best_stage = None
        lowdiv_skim = None
        forced_bounce = False

        for name, beam, depth, base_sec in schedules:
            if name == "polish":
                if getattr(base_settings, "no_polish", False):
                    continue
                ran_polish = True

            if name == "skim":
                stage_seconds = base_sec
            else:
                stage_seconds = base_sec * (mult if base_settings.use_octo_prior else 1.0)

            base_stage_settings = replace(
                base_settings,
                beam_width=beam,
                max_depth=depth,
                max_seconds=float(stage_seconds),
                use_meta_controller=True,
            )
            stage_settings = base_stage_settings
            if (
                name == "main"
                and getattr(base_settings, "bounce_if_lowdiv", False)
                and lowdiv_skim is not None
                and lowdiv_skim < float(getattr(base_settings, "lowdiv_thr", 0.05))
                and octo_z_val >= float(getattr(base_settings, "octo_z_min_for_bounce", 1.20))
            ):
                if hasattr(base_stage_settings, "max_bounces"):
                    stage_settings = replace(
                        base_stage_settings,
                        max_bounces=int(getattr(base_settings, "bounce_max", 1)),
                    )
                    forced_bounce = True
            for attr in ("_truths_provider", "_policy_prior", "_trace_ops"):
                if hasattr(base_settings, attr):
                    setattr(stage_settings, attr, getattr(base_settings, attr))
            setattr(stage_settings, "_octo_z_for_task", getattr(base_settings, "_octo_z_for_task", None))

            start = time.perf_counter()
            try:
                stage_output = _solve_task_internal(tid, task_json, stage_settings, logger=logger)
            except TypeError:
                stage_output = _solve_task_internal(tid, task_json, stage_settings)
            stage_times[name] = float(round(time.perf_counter() - start, 3))

            if logger and hasattr(logger, "log"):
                try:
                    monitor = getattr(stage_settings, "_monitor", None)
                    rho_sm = getattr(monitor, "rho_sm", None) if monitor else None
                    R_last = getattr(monitor, "R_last", None) if monitor else None
                    health = getattr(monitor, "health", None) if monitor else None
                    should_break = bool(monitor.should_health_break()) if monitor else False
                except Exception:
                    monitor = None
                    rho_sm = None
                    R_last = None
                    health = None
                    should_break = False
                attempt_forced = bool(getattr(stage_settings, "_attemptB_forced_alternate", False))
                palette_unanimous = bool(getattr(stage_settings, "_palette_unanimous", False))
                palette_mapping = getattr(stage_settings, "_palette_mapping", None)
                tel = {
                    "rho_sm": rho_sm,
                    "R_last": R_last,
                    "health": health,
                    "should_break": should_break,
                    "break_reason": "low-R" if should_break else None,
                    "attemptB_forced_alternate": attempt_forced,
                    "palette_unanimous": palette_unanimous,
                    "palette_mapping": palette_mapping if palette_mapping else None,
                    "t_skim": float(stage_times.get("skim", 0.0)),
                    "t_main": float(stage_times.get("main", 0.0)),
                    "t_polish": float(stage_times.get("polish", 0.0)),
                    "stage_time": float(stage_times.get(name, 0.0)),
                }
                try:
                    logger.log(tid, name, tel)
                except Exception:
                    pass

            pair = _normalize_two_attempts(stage_output)
            diversity = 1.0 - _pixel_iou(pair["attempt_1"], pair["attempt_2"])

            if name == "skim":
                lowdiv_skim = diversity

            if best_pair is None or diversity > best_div:
                best_pair = pair
                best_div = diversity
                best_stage = name
                outputs[tid] = [best_pair]

            threshold = float(getattr(base_settings, "stop_if_diversity", 0.20))
            if diversity >= threshold and name != "polish":
                break

        if best_pair is None:
            best_pair = _normalize_two_attempts([[0]])
            best_div = 0.0
            best_stage = "skim"
            outputs[tid] = [best_pair]

        skipped_polish = not ran_polish or stage_times.get("polish", 0.0) == 0.0
        lowdiv_value = float(lowdiv_skim if lowdiv_skim is not None else 0.0)

        if logger and hasattr(logger, "log_kv"):
            try:
                logger.log_kv("t_skim", float(stage_times.get("skim", 0.0)))
                logger.log_kv("t_main", float(stage_times.get("main", 0.0)))
                logger.log_kv("t_polish", float(stage_times.get("polish", 0.0)))
                logger.log_kv("skipped_polish", bool(skipped_polish))
                logger.log_kv("chosen_stage", best_stage)
                logger.log_kv("diversity", float(best_div))
                logger.log_kv("forced_bounce", bool(forced_bounce))
                logger.log_kv("lowdiv_skim", lowdiv_value)
                logger.log_kv("octo_z", float(octo_z_val))
                logger.log_kv("block_identity", bool(getattr(base_settings, "block_identity", True)))
                logger.log_kv("rails_scale_hard", bool(getattr(base_settings, "rails_scale_hard", True)))
                logger.log_kv("scale_hard_thresh", float(getattr(base_settings, "scale_hard_thresh", 0.60)))
                logger.log_kv("scale_hard_steps", int(getattr(base_settings, "scale_hard_steps", 3)))
                logger.log_kv(
                    "early_palette_block_steps",
                    int(getattr(base_settings, "early_palette_block_steps", 3)),
                )
            except Exception:
                pass

        _telemetry_note(
            best_pair,
            t_skim=float(stage_times.get("skim", 0.0)),
            t_main=float(stage_times.get("main", 0.0)),
            t_polish=float(stage_times.get("polish", 0.0)),
            skipped_polish=bool(skipped_polish),
            chosen_stage=best_stage,
            diversity=float(best_div),
            octo_dist_mean=float(odist_mean),
            octo_z=float(octo_z_val),
            octo_mult=float(mult),
            forced_bounce=bool(forced_bounce),
            lowdiv_skim=lowdiv_value,
            block_identity=bool(getattr(base_settings, "block_identity", True)),
            rails_scale_hard=bool(getattr(base_settings, "rails_scale_hard", True)),
            scale_hard_thresh=float(getattr(base_settings, "scale_hard_thresh", 0.60)),
            scale_hard_steps=int(getattr(base_settings, "scale_hard_steps", 3)),
            early_palette_block_steps=int(getattr(base_settings, "early_palette_block_steps", 3)),
        )

    return outputs

def _build_train_test_lookups(tasks_dir):
    train_lookup = {}
    test_lookup = {}
    for task_id, task_json in load_tasks_from_dir(tasks_dir):
        pairs = trains_from_task(task_json)
        tests = tests_from_task(task_json)
        train_lookup[task_id] = pairs
        test_lookup[task_id] = tests
    return train_lookup, test_lookup


def _two_attempts_from_results(results, tasks_dir, strategy="oco_auto", settings=None):
    train_lookup, test_lookup = _build_train_test_lookups(tasks_dir)
    out = {}
    for tid, pred_ll in results.items():
        base_obj = pred_ll
        if isinstance(base_obj, dict) and "attempt_1" in base_obj and "attempt_2" in base_obj:
            entry = {
                "attempt_1": base_obj["attempt_1"],
                "attempt_2": base_obj["attempt_2"],
                "grid": base_obj.get("grid", base_obj["attempt_1"]),
                "_telemetry": base_obj.get("_telemetry", {}),
            }
            out[tid] = [entry]
            continue
        if isinstance(base_obj, list) and len(base_obj) == 1 and isinstance(base_obj[0], dict) and "attempt_1" in base_obj[0] and "attempt_2" in base_obj[0]:
            first = base_obj[0]
            entry = {
                "attempt_1": first["attempt_1"],
                "attempt_2": first["attempt_2"],
                "grid": first.get("grid", first["attempt_1"]),
                "_telemetry": first.get("_telemetry", {}),
            }
            out[tid] = [entry]
            continue

        pairs = train_lookup.get(tid, [])
        phi = None
        if pairs:
            try:
                phi = compute_phi(task_features(pairs))
            except Exception:
                phi = None
        tests = test_lookup.get(tid, [])
        outs = base_obj if isinstance(base_obj, list) else [base_obj]
        out_two = []
        for idx, pred in enumerate(outs):
            if isinstance(pred, dict) and "attempt_1" in pred:
                base = pred["attempt_1"]
                meta = pred
            elif isinstance(pred, dict) and "grid" in pred:
                base = pred["grid"]
                meta = pred.get("meta", pred)
            else:
                base = pred
                meta = pred if isinstance(pred, dict) else {}
            base_np = np.asarray(base)
            if base_np.ndim != 2:
                base_list = base_np.tolist() if hasattr(base_np, "tolist") else base
                entry = {
                    "attempt_1": base_list,
                    "attempt_2": base_list,
                    "grid": base_list,
                    "_telemetry": {},
                }
                _telemetry_note(
                    entry,
                    seconds_after_schedule=float(getattr(settings, "max_seconds", 0.0) if settings else 0.0),
                )
                out_two.append(entry)
                continue

            test_in = tests[idx] if tests and idx < len(tests) else None
            meta_dict = meta if isinstance(meta, dict) else {}
            belief = _build_belief(meta_dict, train_pairs=pairs)

            palette_info = None
            choice = None
            forced_alternate = False
            target = None
            if isinstance(test_in, dict):
                target = test_in.get("output")
            i12 = None
            if _is_masked_tiling(meta_dict):
                att2 = _masked_tiling_alternate(base, meta)
                policy_tag = "masked_raw"
                bounces = 0
                try:
                    _telemetry_note(
                        meta_dict,
                        iou=_pixel_iou(base, att2),
                        alt_tag="masked_raw_pair",
                        conf1=None,
                        conf2=None,
                        bounces=bounces,
                        rho_band=_rho_band(None),
                        attempt_policy=policy_tag,
                        block_identity=bool(getattr(settings, "block_identity", True)) if settings else True,
                        rails_scale_hard=bool(getattr(settings, "rails_scale_hard", True)) if settings else True,
                        scale_hard_thresh=float(getattr(settings, "scale_hard_thresh", 0.60)) if settings else 0.60,
                        scale_hard_steps=int(getattr(settings, "scale_hard_steps", 3)) if settings else 3,
                        early_palette_block_steps=int(getattr(settings, "early_palette_block_steps", 3)) if settings else 3,
                    )
                except Exception:
                    pass
            else:
                candidates = []
                if strategy and strategy != "oco_auto":
                    try:
                        forced = _attempt2_from_strategy(base, strategy, phi, pairs, test_in)
                        candidates.append({"tag": f"forced:{strategy}", "grid": forced, "conf": _surrogate_conf(forced, belief)})
                        forced_alternate = True
                    except Exception:
                        pass
                base = _as_list2(base)
                candidates.extend(_alternates_from_belief(base, test_in, belief, pairs))
                ops_tokens = (meta_dict.get("_telemetry", {}) or {}).get("ops_tokens", [])
                candidates, family_tag = _apply_diversity_guard_candidates(base, candidates, belief, settings, ops_tokens)
                lambda_div = getattr(settings, "div_lambda", 0.50) if settings else 0.50
                lambda_div *= float(getattr(settings, "attemptB_beam_scale", 1.0) if settings else 1.0)
                iou_cap = getattr(settings, "iou_cap", 0.97) if settings else 0.97
                att1_sel, att2_sel, choice = _select_best_pair(base, candidates, belief, lambda_div=lambda_div, iou_cap=iou_cap)
                base = att1_sel
                att2 = att2_sel
                rho_raw = getattr(settings, "_hfp_rho", None) if settings else None
                rho_sm = _smooth_rho(settings, rho_raw) if settings else rho_raw
                max_b = getattr(settings, "max_bounces", -1) if settings else -1
                if max_b and max_b > 0 and settings is not None:
                    scale = float(getattr(settings, "attemptB_time_scale", 1.0))
                    max_b = max(1, int(round(max_b * max(scale, 0.1))))
                att1_deb, att2_deb, bounces = _bounded_debate(base, att2, test_in, belief, rho_sm, max_b)
                base = att1_deb
                att2 = att2_deb
                base, att2, policy_tag, palette_info = _apply_test_palette_policy_pair(
                    base, att2, pairs, test_in, settings or _InternalSearchSettings()
                )

                telemetry_kwargs = {
                    "alt_tag": choice.get("tag") if choice else None,
                    "conf1": choice.get("conf1") if choice else None,
                    "conf2": choice.get("conf2") if choice else None,
                    "bounces": bounces,
                    "rho_band": _rho_band(rho_sm),
                    "attempt_policy": policy_tag,
                    "diversity_family": family_tag,
                }
                if palette_info:
                    status = palette_info.get("palette_map_status")
                    if status is not None:
                        telemetry_kwargs["palette_map_status"] = status
                    if palette_info.get("palette_map"):
                        telemetry_kwargs["palette_map"] = palette_info["palette_map"]
                    if "palette_map_cost" in palette_info and palette_info["palette_map_cost"] is not None:
                        telemetry_kwargs["palette_map_cost"] = palette_info["palette_map_cost"]

                bg_val = belief.get("bg", 0)
                for key_name, grid_val in (("attempt_1", base), ("attempt_2", att2)):
                    try:
                        if _is_degenerate_grid(grid_val, bg=bg_val):
                            salvaged = _salvage_degenerate(grid_val, belief, pairs)
                            if key_name == "attempt_1":
                                base = salvaged
                            else:
                                att2 = salvaged
                    except Exception:
                        pass

                forced_div_tag = "as_is"
                try:
                    att2_variant, i12_forced, forced_div_tag = _force_diversity_B(
                        base,
                        att2,
                        belief,
                        pairs,
                        settings,
                        ops_tokens,
                    )
                    att2 = att2_variant
                    i12 = i12_forced
                    forced_alternate = True
                except Exception:
                    forced_div_tag = "error"
                    try:
                        att2 = _mirror_lr(att2)
                        i12 = _pixel_iou(base, att2)
                        forced_alternate = True
                        forced_div_tag = "mirror"
                    except Exception:
                        i12 = i12 if i12 is not None else None
                telemetry_kwargs["b_forced"] = forced_div_tag

                cap_default_env = float(getattr(settings, "iou_cap", 0.95)) if settings else 0.95
                cap_env = float(os.getenv("ARC_IOU_CAP", str(cap_default_env)))
                telemetry_kwargs["ab_diversity_cap"] = cap_env
                if i12 is None:
                    try:
                        i12 = _pixel_iou(base, att2)
                    except Exception:
                        i12 = 1.0
                telemetry_kwargs["ab_iou12"] = float(i12)

                cand_list: List[Dict[str, Any]] = []
                for tag_name, grid_val in (("attemptA", base), ("attemptB", att2)):
                    grid_list = grid_val.tolist() if hasattr(grid_val, "tolist") else grid_val
                    cand_list.append(
                        {
                            "tag": tag_name,
                            "grid": grid_list,
                            "conf": float(_surrogate_conf(grid_list, belief)),
                        }
                    )

                lam_default = float(getattr(settings, "div_lambda", 0.50)) if settings else 0.50
                lam_final = _envfloat("ARC_DIV_LAMBDA", lam_default)
                lam_final *= float(getattr(settings, "attemptB_beam_scale", 1.0) if settings else 1.0)
                cap_default = float(getattr(settings, "iou_cap", 0.97)) if settings else 0.97
                cap_final = _envfloat("ARC_IOU_CAP", cap_default)
                seed_val = int(getattr(settings, "seed", 1337)) if settings and hasattr(settings, "seed") else 1337
                first_sel, second_sel, i12 = _pick_two_attempts(
                    cand_list,
                    lambda_div=lam_final,
                    iou_cap=cap_final,
                    seed=seed_val,
                )
                if first_sel is None and cand_list:
                    first_sel = cand_list[0]
                if second_sel is None:
                    second_sel = first_sel
                if first_sel is None:
                    first_sel = {"grid": base, "conf": _surrogate_conf(base, belief), "tag": "attemptA"}
                if second_sel is None:
                    second_sel = first_sel

                choice = {
                    "tag": second_sel.get("tag") if isinstance(second_sel, dict) else None,
                    "conf1": first_sel.get("conf") if isinstance(first_sel, dict) else None,
                    "conf2": second_sel.get("conf") if isinstance(second_sel, dict) else None,
                    "iou": i12,
                }

                base = first_sel.get("grid") if isinstance(first_sel, dict) else first_sel
                att2 = second_sel.get("grid") if isinstance(second_sel, dict) else second_sel

                forced_flag = forced_alternate or (
                    choice and isinstance(choice.get("tag"), str) and choice["tag"].startswith("forced:")
                )
                if forced_flag and not getattr(settings, "_attemptB_forced_alternate", False):
                    setattr(settings, "_attemptB_forced_alternate", True)

                palette_status = None
                palette_map_payload = None
                if palette_info:
                    palette_status = palette_info.get("palette_map_status")
                    if palette_info.get("palette_map"):
                        try:
                            palette_map_payload = dict(palette_info["palette_map"])
                        except Exception:
                            palette_map_payload = None
                setattr(settings, "_palette_unanimous", bool(palette_status == "unanimous"))
                if palette_map_payload is not None:
                    setattr(settings, "_palette_mapping", palette_map_payload)
                elif palette_status is not None:
                    setattr(settings, "_palette_mapping", None)

                gof_payload = dict(telemetry_kwargs)
                gof_payload.update(
                    {
                        "block_identity": bool(getattr(settings, "block_identity", True)) if settings else True,
                        "rails_scale_hard": bool(getattr(settings, "rails_scale_hard", True)) if settings else True,
                        "scale_hard_thresh": float(getattr(settings, "scale_hard_thresh", 0.60)) if settings else 0.60,
                        "scale_hard_steps": int(getattr(settings, "scale_hard_steps", 3)) if settings else 3,
                        "early_palette_block_steps": int(getattr(settings, "early_palette_block_steps", 3)) if settings else 3,
                    }
                )

                try:
                    _telemetry_note(
                        meta_dict,
                        iou=_pixel_iou(base, att2),
                        iou12=i12,
                        **gof_payload,
                    )
                except Exception:
                    pass

            base_np = np.asarray(base)
            att2_np = np.asarray(att2)
            if att2_np.ndim != 2 or att2_np.shape != base_np.shape:
                att2_np = base_np
            telemetry = {}
            _cap_telemetry(telemetry)
            att1_entry = {"grid": base_np.tolist(), "_telemetry": telemetry, "meta": meta_dict}
            att2_entry = {"grid": att2_np.tolist(), "_telemetry": telemetry, "meta": meta_dict}
            att2_entry = _enforce_diversity(att1_entry, att2_entry)
            _cap_telemetry(telemetry)

            telemetry["b_forced"] = telemetry_kwargs.get("b_forced", "as_is")

            gA, keepA = _keep1_guard(att1_entry["grid"], pairs, belief)
            gB, keepB = _keep1_guard(att2_entry["grid"], pairs, belief)
            att1_entry["grid"] = gA
            att2_entry["grid"] = gB
            telemetry["keep1_A"] = bool(keepA)
            telemetry["keep1_B"] = bool(keepB)
            telemetry["keep1_applied"] = bool(keepA or keepB)

            cap_default_env = float(getattr(settings, "iou_cap", 0.95)) if settings else 0.95
            cap_env = float(os.getenv("ARC_IOU_CAP", str(cap_default_env)))
            try:
                i12 = _pixel_iou(att1_entry["grid"], att2_entry["grid"])
            except Exception:
                if i12 is None:
                    i12 = 1.0
            i12_float = float(i12 if i12 is not None else 0.0)
            telemetry["ab_diversity_cap"] = cap_env
            telemetry["ab_cap"] = cap_env
            telemetry["ab_iou12"] = float(i12_float)

            if settings and getattr(settings, "attemptB_auto_boost", False):
                stuck = int(getattr(settings, "_ab_stuck", 0))
                if float(i12_float) >= float(cap_env) - 1e-9:
                    stuck += 1
                    if stuck >= 2:
                        try:
                            boost_beam = float(getattr(settings, "attemptB_boost_beam", 1.5))
                            boost_time = float(getattr(settings, "attemptB_boost_time", 1.3))
                            settings.attemptB_beam_scale = max(
                                float(getattr(settings, "attemptB_beam_scale", 1.0)), boost_beam
                            )
                            settings.attemptB_time_scale = max(
                                float(getattr(settings, "attemptB_time_scale", 1.0)), boost_time
                            )
                        except Exception:
                            pass
                else:
                    stuck = 0
                setattr(settings, "_ab_stuck", stuck)

            conf1 = (choice or {}).get("conf1") if isinstance(choice, dict) else None
            conf2 = (choice or {}).get("conf2") if isinstance(choice, dict) else None
            try:
                conf1 = float(conf1) if conf1 is not None else _surrogate_conf(att1_entry["grid"], belief)
            except Exception:
                conf1 = _surrogate_conf(att1_entry["grid"], belief)
            try:
                conf2 = float(conf2) if conf2 is not None else _surrogate_conf(att2_entry["grid"], belief)
            except Exception:
                conf2 = _surrogate_conf(att2_entry["grid"], belief)

            diversity = 1.0 - float(i12_float)
            lam_default = float(getattr(settings, "div_lambda", 0.30)) if settings else 0.30
            try:
                div_lambda = float(os.getenv("ARC_DIV_LAMBDA", str(lam_default)))
            except Exception:
                div_lambda = lam_default

            def _calib(x):
                try:
                    return 1.0 / (1.0 + math.exp(-3.0 * (float(x) - 0.5)))
                except Exception:
                    try:
                        return float(x)
                    except Exception:
                        return 0.0

            cA = _calib(conf1)
            cB = _calib(conf2)

            mmrA = cA + div_lambda * diversity
            mmrB = cB + div_lambda * diversity
            pickB = mmrB > mmrA

            gap = abs(mmrA - mmrB)
            strong_div = diversity >= 0.55
            if (not pickB) and strong_div and (gap <= 0.05):
                pickB = True

            telemetry.update(
                {
                    "diversity": float(diversity),
                    "div_lambda": float(div_lambda),
                    "keep1_applied": bool(telemetry.get("keep1_applied", False)),
                    "keep1_A": bool(telemetry.get("keep1_A", False)),
                    "keep1_B": bool(telemetry.get("keep1_B", False)),
                    "b_forced": telemetry.get("b_forced", "as_is"),
                    "conf1": float(conf1),
                    "conf2": float(conf2),
                    "conf1_cal": float(cA),
                    "conf2_cal": float(cB),
                    "mmrA": float(mmrA),
                    "mmrB": float(mmrB),
                    "gap": float(gap),
                }
            )

            telemetry["chosen_attempt"] = "B" if pickB else "A"

            final_grid = gB if pickB else gA

            entry = {
                "attempt_1": gA,
                "attempt_2": gB,
                "grid": final_grid,
                "_telemetry": {
                    **telemetry,
                    "chosen_attempt": "B" if pickB else "A",
                    "ab_diversity_cap": float(telemetry.get("ab_diversity_cap", cap_final)),
                    "ab_iou12": float(telemetry.get("ab_iou12", i12_float)),
                },
                "iou12": i12 if i12 is not None else i12_float,
            }

            lam_val = entry["_telemetry"].get("div_lambda")
            lam_str = f"{lam_val:.3f}" if isinstance(lam_val, (int, float)) else str(lam_val)
            div_val = entry["_telemetry"].get("diversity", 0.0)
            iou_val = entry["_telemetry"].get("ab_iou12", -1.0)
            keepA = entry["_telemetry"].get("keep1_A", False)
            keepB = entry["_telemetry"].get("keep1_B", False)
            chosen = entry["_telemetry"].get("chosen_attempt", "?")
            gap_val = entry["_telemetry"].get("gap", 0.0)
            forced_tag = entry["_telemetry"].get("b_forced", "as_is")
            print(
                f"[A/B] iou12={float(iou_val):.3f} div={float(div_val):.3f} λ={lam_str} "
                f"chosen={chosen} gap={float(gap_val):.3f} keepA={keepA} keepB={keepB} forced={forced_tag}"
            )

            try:
                _telemetry_note(
                    entry,
                    chosen="B" if pickB else "A",
                    conf1=float(conf1),
                    conf2=float(conf2),
                    dis=float(diversity),
                    lam=float(div_lambda),
                )
            except Exception:
                pass

            telemetry = entry["_telemetry"]
            _cap_telemetry(telemetry)
            att1_entry["_telemetry"] = telemetry
            att2_entry["_telemetry"] = telemetry

            _telemetry_note(
                entry,
                seconds_after_schedule=float(getattr(settings, "max_seconds", 0.0) if settings else 0.0),
            )
            _telemetry_note(
                entry,
                mmrA=float(mmrA),
                mmrB=float(mmrB),
            )
            cli_args = globals().get("_CLI_ARGS")
            if getattr(cli_args, "octo_stats_csv", None):
                try:
                    input_grid = None
                    target_grid = None
                    if isinstance(test_in, dict):
                        input_grid = test_in.get("input")
                        target_grid = test_in.get("output")
                    if input_grid is None and pairs:
                        input_grid = pairs[0][0]
                    if target_grid is None and pairs:
                        target_grid = pairs[0][1]
                    _collect_octo_row(
                        task_id=tid,
                        attempt_tag="A",
                        phi_family=str(getattr(settings, "_phi_family", None)),
                        input_grid=input_grid,
                        pred_grid=entry["attempt_1"],
                        target_grid=target_grid,
                        x_oct=getattr(settings, "_octo_z_for_task", None),
                        rho=getattr(settings, "_hfp_rho_smoothed", None),
                        depth=None,
                        elapsed_s=getattr(settings, "max_seconds", None),
                    )
                    _collect_octo_row(
                        task_id=tid,
                        attempt_tag="B",
                        phi_family=str(getattr(settings, "_phi_family", None)),
                        input_grid=input_grid,
                        pred_grid=entry["attempt_2"],
                        target_grid=target_grid,
                        x_oct=getattr(settings, "_octo_z_for_task", None),
                        rho=getattr(settings, "_hfp_rho_smoothed", None),
                        depth=None,
                        elapsed_s=getattr(settings, "max_seconds", None),
                    )
                except Exception:
                    pass
            out_two.append(entry)
        out[tid] = out_two
    return out


def _find_arc_tasks_dir_fallback(requested_dir):
    if os.path.isdir(requested_dir):
        files = glob.glob(os.path.join(requested_dir, "*.json"))
        if files:
            return requested_dir
    parent = os.path.dirname(requested_dir)
    if parent:
        alt = os.path.join(parent, "arc-agi_test_challenges.json")
        if os.path.isfile(alt):
            test_dir = os.path.join(parent, "test")
            if os.path.isdir(test_dir):
                return test_dir
    return requested_dir


# ============================================================================
# ============ Alignment Finisher Utilities ============
# ============================================================================

def _binary_mask(a):
    import numpy as np
    return (np.asarray(a) != 0).astype(np.int32)

def _bbox_top_left(a):
    import numpy as np
    g = np.asarray(a)
    mask = (g != 0)
    if not mask.any():
        return None
    ys, xs = np.where(mask)
    return int(ys.min()), int(xs.min())

def _overlap_score(maskA, maskB, dy, dx):
    import numpy as np
    A = maskA; B = maskB
    H, W = B.shape
    # place A on B with shift (dy,dx), count overlap of 1s
    y_ps = max(0,  dy);  x_ps = max(0,  dx)
    y_bs = max(0, -dy);  x_bs = max(0, -dx)
    h = H - abs(dy);     w = W - abs(dx)
    if h <= 0 or w <= 0:
        return 0
    return int((A[y_ps:y_ps+h, x_ps:x_ps+w] & B[y_bs:y_bs+h, x_bs:x_bs+w]).sum())

def _propose_alignment_deltas(pred, truth, window=3):
    """
    Return a small candidate set of (dy,dx) using bbox + cross-corr (overlap)
    plus neighbors; clip to ±window.
    """
    import numpy as np
    pm = _binary_mask(pred); tm = _binary_mask(truth)
    # bbox proposal
    bbox = []
    bpp = _bbox_top_left(pred)
    btt = _bbox_top_left(truth)
    if bpp and btt:
        bbox = [(int(btt[0]-bpp[0]), int(btt[1]-bpp[1]))]

    # cross-corr (maximize overlap in a small window)
    best = (0, 0, -1)
    for dy in range(-window, window+1):
        for dx in range(-window, window+1):
            s = _overlap_score(pm, tm, dy, dx)
            if s > best[2]:
                best = (dy, dx, s)

    cand = [(0,0)]
    if bbox:
        by, bx = bbox[0]
        cand += [(by, bx), (by+1, bx), (by-1, bx), (by, bx+1), (by, bx-1)]
    cy, cx = best[0], best[1]
    cand += [(cy, cx), (cy+1, cx), (cy-1, cx), (cy, cx+1), (cy, cx-1)]

    # clip & dedup
    uniq = []
    seen = set()
    for (dy,dx) in cand:
        dy = int(np.clip(dy, -window, window))
        dx = int(np.clip(dx, -window, window))
        if (dy,dx) not in seen:
            uniq.append((dy,dx)); seen.add((dy,dx))
    return uniq

def _current_acc_state(pred: np.ndarray, target: np.ndarray):
    """Return (acc, (cy_p,cx_p), (cy_t,cx_t)) or (0.0, None, None) if shape mismatch."""
    if pred.shape != target.shape: 
        return 0.0, None, None
    acc = float((pred == target).mean())
    def _centroid(g):
        ys, xs = np.where(g != 0)
        return (float(ys.mean()), float(xs.mean())) if ys.size else (None, None)
    return acc, _centroid(pred), _centroid(target)

def _learn_task_alignment(program, train_pairs, phi, settings):
    """
    For each training pair, pick the best (dy,dx) from a small candidate set
    (bbox + xcorr proposals); return the median (dy,dx) over pairs.
    """
    import numpy as np
    dy_list, dx_list = [], []
    for (x, y) in train_pairs:
        try:
            pred = interpret_program(program, x)
        except Exception:
            continue
        # only consider alignment if shapes match
        if pred.shape != y.shape:
            continue
        cand = _propose_alignment_deltas(pred, y, window=3)
        if not cand:
            continue
        # choose by pixel accuracy
        best_acc, best_dy, best_dx = -1.0, 0, 0
        H, W = pred.shape
        for (dy,dx) in cand:
            shifted = np.zeros_like(pred)
            y0 = max(0,  dy);  x0 = max(0,  dx)
            ys = max(0, -dy);  xs = max(0, -dx)
            h = H - abs(dy);    w = W - abs(dx)
            if h <= 0 or w <= 0:
                continue
            shifted[y0:y0+h, x0:x0+w] = pred[ys:ys+h, xs:xs+w]
            acc = (shifted == y).mean()
            if acc > best_acc:
                best_acc, best_dy, best_dx = acc, dy, dx
        dy_list.append(best_dy); dx_list.append(best_dx)
    if not dy_list:
        return None
    return (int(np.median(dy_list)), int(np.median(dx_list)))


# ============================================================================
# ============ Block-Mask Finisher Utilities ============
# ============================================================================

def _divisible_shape(in_shape, out_shape):
    Hin, Win = in_shape
    Hout, Wout = out_shape
    if Hin <= 0 or Win <= 0: 
        return None
    if Hout % Hin != 0 or Wout % Win != 0:
        return None
    return (Hout // Hin, Wout // Win)

def _downsample_block_presence(y, ky, kx):
    """
    For a target grid y with shape (Hout, Wout) and block (Hin, Win) = (Hout/ky, Wout/kx),
    compute a boolean mask M[ky,kx] that marks whether each block is *meaningfully nonzero*.
    """
    import numpy as np
    y = np.asarray(y)
    Hout, Wout = y.shape
    Hin, Win   = Hout // ky, Wout // kx
    M = np.zeros((ky, kx), dtype=bool)
    for by in range(ky):
        for bx in range(kx):
            y0, x0 = by*Hin, bx*Win
            block = y[y0:y0+Hin, x0:x0+Win]
            # "meaningfully nonzero": at least one non-zero AND not trivially dense background (heuristic)
            if np.any(block != 0):
                M[by, bx] = True
    return M

def _learn_block_mask(train_pairs, ky, kx):
    """
    Aggregate block-presence across all training targets; return a majority-vote mask M[ky,kx].
    If ambiguity is high, fall back to all-True (no masking).
    """
    import numpy as np
    votes = np.zeros((ky, kx), dtype=np.int32)
    total = 0
    for (_x, y) in train_pairs:
        y = np.asarray(y)
        Hout, Wout = y.shape
        if Hout % ky != 0 or Wout % kx != 0:
            continue
        M = _downsample_block_presence(y, ky, kx)
        votes += M.astype(np.int32)
        total += 1
    if total == 0:
        return None
    # majority vote (>= half)
    thresh = (total + 1) // 2
    Mmaj = votes >= thresh
    # avoid degenerate all-False; if nearly full, keep all-True (no mask)
    if not Mmaj.any():
        return None
    return Mmaj

def _apply_block_mask(grid, ky, kx, M):
    """
    Zero out blocks where M[by,bx] == False. Assumes grid shape (Hout,Wout) divisible by ky,kx.
    """
    import numpy as np
    g = np.asarray(grid)
    Hout, Wout = g.shape
    Hin, Win   = Hout // ky, Wout // kx
    out = g.copy()
    for by in range(ky):
        for bx in range(kx):
            if not M[by, bx]:
                y0, x0 = by*Hin, bx*Win
                out[y0:y0+Hin, x0:x0+Win] = 0
    return out


# ============================================================================
# ============ Blockwise Color Finisher Utilities ============
# ============================================================================

def _block_shapes(out_shape, ky, kx):
    Hout, Wout = out_shape
    if Hout % ky != 0 or Wout % kx != 0:
        return None
    Hin, Win = Hout // ky, Wout // kx
    return Hin, Win

def _blockwise_dominant_colors(y, ky, kx):
    """
    For a target grid y (Hout,Wout) divisible by ky,kx, return an array D[ky,kx]
    with the dominant (majority) color per block.
    """
    import numpy as np
    y = np.asarray(y)
    Hout, Wout = y.shape
    Hin, Win = _block_shapes((Hout, Wout), ky, kx)
    if Hin is None:
        return None
    D = np.zeros((ky, kx), dtype=np.int32)
    for by in range(ky):
        for bx in range(kx):
            y0, x0 = by*Hin, bx*Win
            block = y[y0:y0+Hin, x0:x0+Win]
            vals, cnts = np.unique(block, return_counts=True)
            D[by, bx] = int(vals[np.argmax(cnts)])
    return D

def _learn_blockwise_projection(train_pairs, ky, kx):
    """
    Learn a blockwise dominant-color projection from training targets.
    Majority-vote the dominant color per block across all pairs.
    Returns D[ky,kx] or None if shapes are inconsistent.
    """
    import numpy as np
    votes = None
    total = 0
    for (_x, y) in train_pairs:
        y = np.asarray(y)
        HinWin = _block_shapes(y.shape, ky, kx)
        if HinWin is None:
            continue
        D = _blockwise_dominant_colors(y, ky, kx)
        if D is None:
            continue
        if votes is None:
            # store as dict of counters per block
            votes = [[{} for _ in range(kx)] for _ in range(ky)]
        for by in range(ky):
            for bx in range(kx):
                c = int(D[by, bx])
                votes[by][bx][c] = votes[by][bx].get(c, 0) + 1
        total += 1

    if votes is None or total == 0:
        return None

    Dmaj = [[0 for _ in range(kx)] for _ in range(ky)]
    for by in range(ky):
        for bx in range(kx):
            cnts = votes[by][bx]
            if not cnts:
                Dmaj[by][bx] = 0
            else:
                # majority color per block
                Dmaj[by][bx] = max(cnts.items(), key=lambda kv: kv[1])[0]
    import numpy as np
    return np.array(Dmaj, dtype=np.int32)

def _apply_blockwise_projection(pred, ky, kx, D):
    """
    Project each block of pred to the learned dominant target color D[by,bx].
    For now, we set all non-zero pixels in the block to D[by,bx] (zero stays zero).
    """
    import numpy as np
    g = np.asarray(pred).copy()
    Hout, Wout = g.shape
    Hin, Win = _block_shapes((Hout, Wout), ky, kx)
    if Hin is None or D is None:
        return g
    for by in range(ky):
        for bx in range(kx):
            y0, x0 = by*Hin, bx*Win
            blk = g[y0:y0+Hin, x0:x0+Win]
            tgt = int(D[by, bx])
            mask = (blk != 0)
            blk[mask] = tgt
            g[y0:y0+Hin, x0:x0+Win] = blk
    return g



# ============================================================================
# ============ dsl/ops.py ============
# ============================================================================

def _crop_border(arr, margin=1):
    """Remove `margin` rows/cols from all 4 sides (for op=shrink)."""
    if arr.shape[0] <= 2*margin or arr.shape[1] <= 2*margin:
        return arr
    return arr[margin:-margin, margin:-margin].copy()

def _pad_expand(arr, amount=1, fillval=0):
    """Add `amount` rows/cols on all 4 sides."""
    return np.pad(arr, amount, mode='constant', constant_values=fillval)

def _isolate_largest_region(arr):
    """Zero everything except the largest connected component (4-neighbor)."""
    from collections import deque
    H, W = arr.shape
    visited = np.zeros((H, W), dtype=bool)
    components = []
    for y0 in range(H):
        for x0 in range(W):
            if not visited[y0, x0] and arr[y0, x0] != 0:
                region = []
                q = deque([(y0, x0)])
                visited[y0, x0] = True
                while q:
                    y, x = q.popleft()
                    region.append((y, x))
                    for dy, dx in [(-1,0),(1,0),(0,-1),(0,1)]:
                        ny, nx = y+dy, x+dx
                        if 0 <= ny < H and 0 <= nx < W:
                            if not visited[ny, nx] and arr[ny, nx] == arr[y, x]:
                                visited[ny, nx] = True
                                q.append((ny, nx))
                components.append(region)
    if not components:
        return np.zeros_like(arr)
    largest = max(components, key=len)
    out = np.zeros_like(arr)
    for (y, x) in largest:
        out[y, x] = arr[y, x]
    return out

def _conn_comps(arr):
    """Extract all connected components (4-neighbor) as (color, pts) tuples."""
    H, W = arr.shape
    seen = np.zeros((H, W), dtype=bool)
    comps = []
    for y in range(H):
        for x in range(W):
            if arr[y, x] != 0 and not seen[y, x]:
                color = arr[y, x]
                q = [(y, x)]
                seen[y, x] = True
                pts = []
                while q:
                    yy, xx = q.pop()
                    pts.append((yy, xx))
                    for dy, dx in [(-1,0),(1,0),(0,-1),(0,1)]:
                        ny, nx = yy+dy, xx+dx
                        if 0 <= ny < H and 0 <= nx < W and not seen[ny, nx] and arr[ny, nx] == color:
                            seen[ny, nx] = True
                            q.append((ny, nx))
                comps.append((color, pts))
    return comps


def _flood_holes_mask(mask: np.ndarray) -> np.ndarray:
    """Return a boolean mask marking interior background pixels (holes) of a binary mask."""
    H, W = mask.shape
    bg = mask == 0
    ext = np.zeros_like(bg, dtype=bool)
    q = deque()

    for x in range(W):
        if bg[0, x]:
            q.append((0, x))
        if bg[H - 1, x]:
            q.append((H - 1, x))
    for y in range(H):
        if bg[y, 0]:
            q.append((y, 0))
        if bg[y, W - 1]:
            q.append((y, W - 1))

    while q:
        y, x = q.popleft()
        if not (0 <= y < H and 0 <= x < W):
            continue
        if ext[y, x] or not bg[y, x]:
            continue
        ext[y, x] = True
        for dy, dx in [(-1, 0), (1, 0), (0, -1), (0, 1)]:
            q.append((y + dy, x + dx))

    return bg & ~ext


def _holes_count(grid: np.ndarray) -> int:
    """Count interior holes (background regions fully enclosed by non-zero cells)."""
    mask = (grid != 0).astype(np.uint8)
    return int(_flood_holes_mask(mask).sum())


def _component_count(grid: np.ndarray) -> int:
    """Return number of connected non-zero components (4-conn)."""
    labels, num = _cc_label(grid != 0)
    return int(num)


def _pixel_acc(pred_item, truth_np: np.ndarray) -> float:
    """Best-of-two if dict; otherwise single. Shapes must match."""
    if isinstance(pred_item, dict) and "attempt_1" in pred_item:
        p1 = np.asarray(pred_item["attempt_1"])
        p2 = np.asarray(pred_item["attempt_2"])

        def _acc(p):
            return float((p == truth_np).mean()) if p.shape == truth_np.shape else 0.0

        return max(_acc(p1), _acc(p2))
    p = np.asarray(pred_item)
    return float((p == truth_np).mean()) if p.shape == truth_np.shape else 0.0


def get_test_truths(solutions_obj: dict, tid: str):
    """
    Local notebook helper (submission-safe): robustly load test truths across common shapes.
    • {"tid": {"test":[...grids...]}}
    • {"tid": [...grids...]}
    • {"tid": {"outputs":[...grids...]}}  # rare legacy
    Each entry may be {"output":[...]} or the grid directly.
    """
    entry = solutions_obj.get(tid)
    if entry is None:
        return []
    if isinstance(entry, dict) and "test" in entry:
        outs = entry["test"]
    elif isinstance(entry, list):
        outs = entry
    elif isinstance(entry, dict) and "outputs" in entry:
        outs = entry["outputs"]
    else:
        outs = []
    resolved = []
    for out in outs:
        if isinstance(out, dict) and "output" in out:
            resolved.append(np.array(out["output"], dtype=np.int32))
        else:
            resolved.append(np.array(out, dtype=np.int32))
    return resolved


# ===== v2.9.0: Policy Prior =====


class PolicyPrior:
    """Return an additive logit (prior) for an operator under context."""

    def op_logit(self, op_name: str, ctx) -> float:
        return 0.0


class HeuristicPrior(PolicyPrior):
    """
    Heuristic prior using φ-family hints (scale, objectness, palette, alignment, geometry, pattern, topology, composition)
    and simple grid checks (divisibility for tiling, symmetry flags if available).
    """

    def op_logit(self, op_name: str, ctx) -> float:
        # ctx: dict with fields we pass from the beam: phi, grid, grid_shape, divs, sym, family
        divs = ctx.get("divs", ())
        family = ctx.get("family", None)

        score = 0.0

        if ("tile" in op_name or "phase_tile" in op_name) and divs:
            score += 0.6

        if family in ("alignment", "geometry") and (
            "mirror" in op_name or "rot" in op_name
        ):
            score += 0.3

        if family in ("objectness", "topology") and (
            "keep_n_largest" in op_name
            or "fill_holes" in op_name
            or "remove_isolated" in op_name
        ):
            score += 0.25

        if family == "palette" and any(word in op_name for word in ("palette_map", "recolor")):
            score += 0.2
        elif family != "palette" and "palette" in op_name:
            score -= 0.15

        # --- GOF-9000: object-centric soft preference ---
        phi = None
        depth = 0
        try:
            if isinstance(ctx, dict):
                phi = ctx.get("phi")
                depth = int(ctx.get("depth", 0) or 0)
        except Exception:
            phi = None

        obj_mag = 0.0
        if isinstance(phi, (list, tuple)) and len(phi) > 1:
            try:
                obj_mag = abs(float(phi[1]))
            except Exception:
                obj_mag = 0.0
        else:
            try:
                if isinstance(ctx, dict) and ctx.get("has_shape_match"):
                    obj_mag = max(obj_mag, 0.35)
            except Exception:
                pass

        if depth == 0 and obj_mag > 0.30:
            if op_name in ("keep_n_largest", "remove_isolated"):
                score += 1.10
            elif op_name in ("fill_holes", "largest", "center_largest"):
                score += 0.80
            elif op_name in ("keep_rings",):
                score += 0.20
        elif obj_mag > 0.30:
            if op_name in ("keep_n_largest", "remove_isolated"):
                score += 0.35
            elif op_name in ("fill_holes", "keep_rings", "largest", "center_largest"):
                score += 0.30
            elif op_name in ("mask",):
                score += 0.20
        # --- end object-centric soft preference ---

        return score


_DEFAULT_POLICY_PRIOR = HeuristicPrior()


def _op_fill_holes(grid: np.ndarray) -> np.ndarray:
    """Fill interior holes of each color component independently."""
    out = grid.copy()
    for c in np.unique(grid):
        if c == 0:
            continue
        mask = (grid == c).astype(np.uint8)
        if not mask.any():
            continue
        holes = _flood_holes_mask(mask)
        if holes.any():
            out[holes] = c
    return out


def _op_keep_rings(grid: np.ndarray) -> np.ndarray:
    """Keep only components (per color) that contain at least one hole; zero everything else."""
    out = np.zeros_like(grid)
    structure = np.ones((3, 3), dtype=int)
    for c in np.unique(grid):
        if c == 0:
            continue
        mask = (grid == c).astype(np.uint8)
        if not mask.any():
            continue
        lbl, n = _cc_label(mask, structure=structure)
        for k in range(1, n + 1):
            comp = (lbl == k).astype(np.uint8)
            holes = _flood_holes_mask(comp)
            if holes.any():
                out[comp == 1] = c
    return out


def _op_remove_isolated(grid: np.ndarray, min_size: int) -> np.ndarray:
    """Remove color components smaller than min_size (per color), keep the rest."""
    out = grid.copy()
    structure = np.ones((3, 3), dtype=int)
    for c in np.unique(grid):
        if c == 0:
            continue
        mask = (grid == c).astype(np.uint8)
        if not mask.any():
            continue
        lbl, n = _cc_label(mask, structure=structure)
        for k in range(1, n + 1):
            comp = lbl == k
            if np.count_nonzero(comp) < int(min_size):
                out[comp] = 0
    return out


def _op_keep_n_largest(grid, n):
    """keep_n_largest <n>
    Keep only the n largest connected components by size.
    """
    comps = _conn_comps(grid)
    comps.sort(key=lambda c: len(c[1]), reverse=True)
    keep = set()
    for color, pts in comps[:max(0, int(n))]:
        for y, x in pts:
            keep.add((y, x))
    out = np.zeros_like(grid)
    for (y, x) in keep:
        out[y, x] = grid[y, x]
    return out

def _op_keep_size_range(grid, amin, amax):
    """keep_size_range <amin> <amax>
    Keep only connected components with size in [amin, amax].
    """
    amin, amax = int(amin), int(amax)
    comps = _conn_comps(grid)
    out = np.zeros_like(grid)
    for color, pts in comps:
        if amin <= len(pts) <= amax:
            for y, x in pts:
                out[y, x] = color
    return out

def _op_fill(grid, color):
    """fill <color>"""
    out = np.full_like(grid, color)
    return out

def _op_mirror(grid, axis):
    """mirror <axis>
    axis=0->flipud, axis=1->fliplr
    """
    if axis == 0:
        return np.flipud(grid)
    return np.fliplr(grid)

def _op_rot90(grid, k):
    """rot90 <k>
    k=1 => 90deg, k=2 => 180deg, k=3 => 270deg
    """
    return np.rot90(grid, k=k)

def _op_transpose(grid):
    """transpose"""
    return grid.T

def _op_shrink(grid):
    """shrink (margin=1)"""
    return _crop_border(grid, margin=1)

def _op_grow(grid):
    """grow (pad=1)"""
    return _pad_expand(grid, amount=1, fillval=0)

def _op_add(grid1, grid2):
    """add <grid2>"""
    combined = np.where(grid1 != 0, grid1, grid2)
    return combined

def _op_mask(grid, color):
    """mask <color>"""
    out = np.where(grid == color, grid, 0)
    return out

def _op_invert(grid):
    """invert (mod 10 for ARC)"""
    out = (grid + 5) % 10
    return out

def _op_duplicate(grid):
    """duplicate (identity)"""
    return grid.copy()

def _op_scale(grid, factor):
    """scale <factor>
    Naive nearest-neighbor. factor must be an int>=1.
    """
    if factor <= 0:
        return grid
    H, W = grid.shape
    out = np.zeros((H * factor, W * factor), dtype=grid.dtype)
    for i in range(H):
        for j in range(W):
            val = grid[i, j]
            for di in range(factor):
                for dj in range(factor):
                    out[i*factor+di, j*factor+dj] = val
    return out

def _op_tile(grid, vert, horiz):
    """tile <vert> <horiz>"""
    return np.tile(grid, (vert, horiz))

def _op_tile_masked(grid, ky, kx, mode):
    """
    tile_masked <ky> <kx> <mode>

    Tiling with selective block placement:
      mode=0: cross    -> keep blocks in center row or center column
      mode=1: border   -> keep blocks on the perimeter only
      mode=2: main_diag (optional) -> keep blocks where by == bx (requires ky==kx)
      mode=3: anti_diag (optional) -> keep blocks where by + bx == ky - 1 (requires ky==kx)

    Unselected blocks are set to 0. This enables sparse lattice patterns (e.g., plus frames).
    """
    import numpy as np
    g = np.asarray(grid)
    H, W = g.shape
    out = np.zeros((H*ky, W*kx), dtype=g.dtype)

    same = (ky == kx)
    for by in range(ky):
        for bx in range(kx):
            keep = False
            if mode == 0:
                keep = (by == ky // 2) or (bx == kx // 2)
            elif mode == 1:
                keep = (by == 0) or (by == ky - 1) or (bx == 0) or (bx == kx - 1)
            elif mode == 2 and same:
                keep = (by == bx)
            elif mode == 3 and same:
                keep = ((by + bx) == (ky - 1))
            if keep:
                y0, x0 = by*H, bx*W
                out[y0:y0+H, x0:x0+W] = g
    return out

def _op_crop(grid, color):
    """crop <color>
    Crop to bounding box of `color`.
    """
    mask = (grid == color)
    if not mask.any():
        return grid
    rows = np.any(mask, axis=1)
    cols = np.any(mask, axis=0)
    r0, r1 = np.where(rows)[0][[0, -1]]
    c0, c1 = np.where(cols)[0][[0, -1]]
    return grid[r0:r1+1, c0:c1+1].copy()

def _op_shift(grid, dy, dx):
    """shift <dy> <dx>"""
    H, W = grid.shape
    out = np.zeros_like(grid)
    y_src_start = max(0, -dy)
    y_dst_start = max(0, dy)
    x_src_start = max(0, -dx)
    x_dst_start = max(0, dx)
    h = H - abs(dy)
    w = W - abs(dx)
    if h > 0 and w > 0:
        out[y_dst_start:y_dst_start+h, x_dst_start:x_dst_start+w] = \
            grid[y_src_start:y_src_start+h, x_src_start:x_src_start+w]
    return out

def _op_replacecolor(grid, old_color, new_color):
    """replacecolor <old> <new>"""
    out = grid.copy()
    out[out == old_color] = new_color
    return out

def _op_swapcolors(grid, c1, c2):
    """swapcolors <c1> <c2>"""
    out = grid.copy()
    mask1 = (out == c1)
    mask2 = (out == c2)
    out[mask1] = c2
    out[mask2] = c1
    return out

def _op_outline(grid, color):
    """outline <color>
    Set the perimeter of all non-zero cells to <color>.
    """
    if grid.size == 0:
        return grid.copy()
    mask = (grid != 0)
    edges = np.zeros_like(grid, dtype=bool)
    H, W = grid.shape
    for y in range(H):
        for x in range(W):
            if mask[y, x]:
                for dy, dx in [(-1,0),(1,0),(0,-1),(0,1)]:
                    ny, nx = y+dy, x+dx
                    if 0 <= ny < H and 0 <= nx < W:
                        if not mask[ny, nx]:
                            edges[y, x] = True
                            break
    out = grid.copy()
    out[edges] = color
    return out

def _op_majority(grid):
    """majority
    Fill the entire grid with the most frequent non-zero color.
    """
    vals = grid[grid != 0]
    if vals.size == 0:
        return grid.copy()
    unique, counts = np.unique(vals, return_counts=True)
    maj = unique[np.argmax(counts)]
    return np.full_like(grid, maj)

def _op_minority(grid):
    """minority
    Fill the entire grid with the *least* frequent non-zero color.
    """
    vals = grid[grid != 0]
    if vals.size == 0:
        return grid.copy()
    unique, counts = np.unique(vals, return_counts=True)
    min_color = unique[np.argmin(counts)]
    return np.full_like(grid, min_color)

def _op_threshold(grid, val):
    """threshold <val>
    If pixel >= val => pixel, else 0.
    """
    out = np.where(grid >= val, grid, 0)
    return out

def _op_largest(grid):
    """largest
    Isolate the largest connected component.
    """
    return _isolate_largest_region(grid)

def _op_resize(grid, h, w):
    """resize <h> <w>
    Simple nearest-neighbor resize. If target is smaller, we truncate. If larger, we replicate edge.
    """
    H, W = grid.shape
    if H == h and W == w:
        return grid.copy()
    out = np.zeros((h, w), dtype=grid.dtype)
    for i in range(h):
        for j in range(w):
            src_i = min(int(i * H / h), H-1)
            src_j = min(int(j * W / w), W-1)
            out[i, j] = grid[src_i, src_j]
    return out

def _op_stack(grid1, grid2, axis):
    """stack <grid2> <axis>
    axis=0 => vertical stack, axis=1 => horizontal stack
    """
    if axis == 0:
        return np.vstack([grid1, grid2])
    return np.hstack([grid1, grid2])

def _op_subtract(grid1, grid2):
    """subtract <grid2>
    Wherever grid2 != 0, set grid1 to 0.
    """
    out = grid1.copy()
    out[grid2 != 0] = 0
    return out


def _op_phase_tile(grid, ky, kx, mode=0):
    """
    phase_tile <ky> <kx> [mode]
    mode=0: alternate rot180 on odd (by+bx)
    mode=1: alternate flipud on odd
    mode=2: alternate fliplr on odd
    Creates a k-by-k quilt where blocks with parity (by+bx)%2==1 use a transformed base.
    """
    import numpy as np
    H, W = grid.shape
    out = np.zeros((H*ky, W*kx), dtype=grid.dtype)
    for by in range(ky):
        for bx in range(kx):
            block = grid
            odd = (by + bx) % 2
            if odd:
                if mode == 0:
                    block = np.rot90(block, 2)
                elif mode == 1:
                    block = np.flipud(block)
                elif mode == 2:
                    block = np.fliplr(block)
            y0, x0 = by*H, bx*W
            out[y0:y0+H, x0:x0+W] = block
    return out

def _op_phase_tile_row(grid, ky, kx, mode=2):
    """
    phase_tile_row <ky> <kx> <mode>
    Transform every block in odd block-rows (by % 2 == 1).
    mode=0: rot180, 1: flipud, 2: fliplr (default)
    """
    import numpy as np
    H, W = grid.shape
    out = np.zeros((H*ky, W*kx), dtype=grid.dtype)
    for by in range(ky):
        for bx in range(kx):
            block = grid
            if (by % 2) == 1:
                if mode == 0:
                    block = np.rot90(block, 2)
                elif mode == 1:
                    block = np.flipud(block)
                else:
                    block = np.fliplr(block)
            y0, x0 = by*H, bx*W
            out[y0:y0+H, x0:x0+W] = block
    return out


def _op_phase_tile_col(grid, ky, kx, mode=2):
    """
    phase_tile_col <ky> <kx> <mode>
    Transform every block in odd block-columns (bx % 2 == 1).
    mode=0: rot180, 1: flipud, 2: fliplr (default)
    """
    import numpy as np
    H, W = grid.shape
    out = np.zeros((H*ky, W*kx), dtype=grid.dtype)
    for by in range(ky):
        for bx in range(kx):
            block = grid
            if (bx % 2) == 1:
                if mode == 0:
                    block = np.rot90(block, 2)
                elif mode == 1:
                    block = np.flipud(block)
                else:
                    block = np.fliplr(block)
            y0, x0 = by*H, bx*W
            out[y0:y0+H, x0:x0+W] = block
    return out



OP_NAMES_BASIC = [
    # (name,arity, param_types)
    # Shape-critical operations first for beam efficiency
    ("resize", 3, ["grid", "int", "int"]),
    ("tile", 3, ["grid", "int", "int"]),
    ("tile_masked", 4, ["grid", "int", "int", "int"]),
    ("phase_tile_row", 4, ["grid", "int", "int", "int"]),
    ("phase_tile_col", 4, ["grid", "int", "int", "int"]),
    ("phase_tile", 4, ["grid", "int", "int", "int"]),
    # Geometric and color operations
    ("fill", 2, ["grid", "int"]),
    ("mirror", 2, ["grid", "int"]),
    ("rot90", 2, ["grid", "int"]),
    ("transpose", 1, ["grid"]),
    ("shrink", 1, ["grid"]),
    ("grow", 1, ["grid"]),
    ("add", 2, ["grid", "grid"]),
    ("mask", 2, ["grid", "int"]),
    ("invert", 1, ["grid"]),
    ("duplicate", 1, ["grid"]),
    ("scale", 2, ["grid", "int"]),
    ("crop", 2, ["grid", "int"]),
    ("shift", 3, ["grid", "int", "int"]),
    ("replacecolor", 3, ["grid", "int", "int"]),
    ("swapcolors", 3, ["grid", "int", "int"]),
    ("outline", 2, ["grid", "int"]),
    ("majority", 1, ["grid"]),
    ("minority", 1, ["grid"]),
    ("threshold", 2, ["grid", "int"]),
    ("largest", 1, ["grid"]),
    ("fill_holes", 1, ["grid"]),
    ("keep_rings", 1, ["grid"]),
    ("remove_isolated", 2, ["grid", "int"]),
    ("stack", 3, ["grid", "grid", "int"]),
    ("subtract", 2, ["grid", "grid"]),
]

_OP_ENTRY_BY_NAME = {name: (name, arity, param_types) for name, arity, param_types in OP_NAMES_BASIC}

OP_REGISTRY = {
    "fill": _op_fill,
    "mirror": _op_mirror,
    "rot90": _op_rot90,
    "transpose": _op_transpose,
    "shrink": _op_shrink,
    "grow": _op_grow,
    "add": _op_add,
    "mask": _op_mask,
    "invert": _op_invert,
    "duplicate": _op_duplicate,
    "scale": _op_scale,
    "tile": _op_tile,
    "tile_masked": _op_tile_masked,
    "crop": _op_crop,
    "shift": _op_shift,
    "replacecolor": _op_replacecolor,
    "swapcolors": _op_swapcolors,
    "outline": _op_outline,
    "majority": _op_majority,
    "minority": _op_minority,
    "threshold": _op_threshold,
    "largest": _op_largest,
    "fill_holes": _op_fill_holes,
    "keep_rings": _op_keep_rings,
    "remove_isolated": _op_remove_isolated,
    "keep_n_largest": _op_keep_n_largest,
    "keep_size_range": _op_keep_size_range,
    "resize": _op_resize,
    "stack": _op_stack,
    "subtract": _op_subtract,
    "phase_tile": _op_phase_tile,
    "phase_tile_row": _op_phase_tile_row,
    "phase_tile_col": _op_phase_tile_col,
}


# === GOF-9000: Early-step Directional Guards + Identity Policy ===============
# Purpose: stop wrong-way resize/scale in the first few steps, and block noop
# starts unless the task is truly identity. Works regardless of successor API.

# How many earliest steps to police (conservative)
K_DIR = 3
# New: forbid any size-changing op on the very first step (structure-first).
# This is a hard rule; we can make it configurable later if needed.
K_NO_SIZEOPS_AT_START = 1


def _gof_collect_ints(*values):
    """Collect ints from nested args, tolerating numpy scalars and digit strings."""
    ints: List[int] = []
    stack = list(values)
    np_integer = getattr(np, "integer", ()) if hasattr(np, "integer") else ()
    while stack:
        item = stack.pop()
        if isinstance(item, bool):  # avoid treating booleans as ints
            continue
        if isinstance(item, int):
            ints.append(int(item))
            continue
        if np_integer and isinstance(item, np_integer):  # type: ignore[arg-type]
            ints.append(int(item))
            continue
        if isinstance(item, (list, tuple)):
            stack.extend(item)
            continue
        if isinstance(item, str):
            filtered = "".join(ch for ch in item if ch.isdigit() or ch == "-")
            if filtered:
                try:
                    ints.append(int(filtered))
                except Exception:
                    pass
    ints.reverse()
    return ints


def _gof_prog_from_args(args):
    for item in args:
        if hasattr(item, "steps") or hasattr(item, "ctx") or hasattr(item, "input_grid"):
            return item
    return None


def _gof_depth_maybe(prog):
    if prog is None:
        return None
    try:
        return len(prog.steps)
    except Exception:
        fallback = getattr(prog, "_rails_steps", None)
        try:
            return None if fallback is None else int(fallback)
        except Exception:
            return None


def _gof_identity_from_args(args):
    for item in args:
        try:
            if isinstance(item, np.ndarray):
                return item.copy()
            if hasattr(item, "copy") and hasattr(item, "shape"):
                return item.copy()
        except Exception:
            pass
    for item in args:
        if isinstance(item, (list, tuple)) and item:
            try:
                return np.array(item).copy()
            except Exception:
                return item
    return args[0] if args else None


def _gof_sanitize_token(tok: str) -> str:
    """Normalize token strings so integer arguments appear without extra wrappers."""
    try:
        name, rest = tok.split("(", 1)
        rest = rest.rstrip(")")
        if not rest:
            return tok
        parts = [p.strip() for p in rest.split(",")]
        cleaned: List[str] = []
        for part in parts:
            digits = "".join(ch for ch in part if (ch.isdigit() or ch == "-"))
            cleaned.append(digits if digits else part)
        return f"{name}(" + ",".join(cleaned) + ")"
    except Exception:
        return tok


def _gof_shape_of(g):
    try:
        import numpy as _np

        if hasattr(g, "shape") and len(g.shape) >= 2:
            return (int(g.shape[-2]), int(g.shape[-1]))
    except Exception:
        pass
    if isinstance(g, list) and g and isinstance(g[0], list):
        return (len(g), len(g[0]))
    return None


def _gof_prog_depth(p):
    try:
        return len(p.steps)
    except Exception:
        return int(getattr(p, "_rails_steps", 0))


def _gof_infer_scale_need_from_prog(p):
    """Return s in {-1.0, 0.0, +1.0}: compress, neutral, expand."""
    # Try program.ctx first if present (search layer usually sets these)
    ctx = getattr(p, "ctx", None)
    inp = None
    tgt = None
    if isinstance(ctx, dict):
        inp = ctx.get("in_shape") or _gof_shape_of(ctx.get("input") or ctx.get("input_grid"))
        tgt = ctx.get("out_shape") or _gof_shape_of(ctx.get("target") or ctx.get("target_grid"))
    # Fall back to common attributes
    inp = inp or (_gof_shape_of(getattr(p, "input_grid", None))
                  or _gof_shape_of(getattr(p, "state0", None))
                  or _gof_shape_of(getattr(p, "state", None)))
    tgt = tgt or (_gof_shape_of(getattr(p, "target", None))
                  or _gof_shape_of(getattr(p, "goal", None)))
    if not (inp and tgt):
        return 0.0
    ia, ta = int(inp[0]) * int(inp[1]), int(tgt[0]) * int(tgt[1])
    if ta < 0.9 * ia:
        return -1.0  # need shrink
    if ta > 1.1 * ia:
        return +1.0  # need expand
    return 0.0


def _gof_is_identity(op_name, args, program):
    # tile(1,1), scale(1), resize to same size
    try:
        if op_name == "tile" and len(args) >= 2:
            return int(args[0]) == 1 and int(args[1]) == 1
        if op_name == "scale" and len(args) >= 1:
            return float(args[0]) == 1.0
        if op_name == "resize" and len(args) >= 2:
            h, w = int(args[0]), int(args[1])
            cur = (_gof_shape_of(getattr(program, "state", None))
                   or _gof_shape_of(getattr(program, "grid", None))
                   or _gof_shape_of(getattr(program, "input_grid", None)))
            return bool(cur) and (tuple(cur) == (h, w))
    except Exception:
        pass
    return False


def _gof_wrap_resize(op_obj):
    """Wrap resize to enforce early identity bans and directional checks."""

    def _wrap_apply(f):
        def _wrapped(*a, **k):
            prog = _gof_prog_from_args(a)
            depth = _gof_depth_maybe(prog)
            identity = _gof_identity_from_args(a)
            if depth == 0:
                return identity

            if depth is not None and prog is not None and depth < K_DIR:
                s = getattr(prog, "_rails_s", None)
                if s is None:
                    s = _gof_infer_scale_need_from_prog(prog)
                    try:
                        setattr(prog, "_rails_s", s)
                    except Exception:
                        pass
                try:
                    nums = _gof_collect_ints(*a)
                    if len(nums) >= 2:
                        h, w = nums[-2], nums[-1]
                        cur_shape = (
                            _gof_shape_of(getattr(prog, "state", None))
                            or _gof_shape_of(getattr(prog, "grid", None))
                            or _gof_shape_of(getattr(prog, "input_grid", None))
                        )
                        if cur_shape:
                            new_area = h * w
                            cur_area = int(cur_shape[0]) * int(cur_shape[1])
                            if s <= -1.0 and new_area >= cur_area:
                                return identity
                            if s >= +1.0 and new_area <= cur_area:
                                return identity
                except Exception:
                    pass
            return f(*a, **k)

        return _wrapped

    if hasattr(op_obj, "apply"):
        op_obj.apply = _wrap_apply(op_obj.apply)
        return op_obj
    if callable(op_obj):
        return _wrap_apply(op_obj)
    return op_obj


def _gof_wrap_scale(op_obj):
    """Wrap scale to respect depth-0 bans and directional hints."""

    def _wrap_apply(f):
        def _wrapped(*a, **k):
            prog = _gof_prog_from_args(a)
            depth = _gof_depth_maybe(prog)
            identity = _gof_identity_from_args(a)
            if depth == 0:
                return identity

            if depth is not None and prog is not None and depth < K_DIR:
                s = getattr(prog, "_rails_s", None)
                if s is None:
                    s = _gof_infer_scale_need_from_prog(prog)
                    try:
                        setattr(prog, "_rails_s", s)
                    except Exception:
                        pass
                try:
                    factor = None
                    for arg in reversed(a):
                        if isinstance(arg, bool):
                            continue
                        if isinstance(arg, (int, float)):
                            factor = float(arg)
                            break
                        if hasattr(np, "floating") and isinstance(arg, np.floating):  # type: ignore[arg-type]
                            factor = float(arg)
                            break
                        if isinstance(arg, str):
                            filtered = "".join(ch for ch in arg if ch.isdigit() or ch in {".", "-"})
                            if filtered:
                                factor = float(filtered)
                                break
                    if factor is not None:
                        if s <= -1.0 and factor >= 1.0:
                            return identity
                        if s >= +1.0 and factor <= 1.0:
                            return identity
                except Exception:
                    pass
            return f(*a, **k)

        return _wrapped

    if hasattr(op_obj, "apply"):
        op_obj.apply = _wrap_apply(op_obj.apply)
        return op_obj
    if callable(op_obj):
        return _wrap_apply(op_obj)
    return op_obj


def _gof_wrap_tile_identity(op_obj):
    def _wrap_apply(f):
        def _wrapped(*a, **k):
            prog = _gof_prog_from_args(a)
            depth = _gof_depth_maybe(prog)
            if depth == 0:
                nums = _gof_collect_ints(*a)
                if len(nums) >= 2 and nums[-2] == 1 and nums[-1] == 1:
                    return _gof_identity_from_args(a)
            return f(*a, **k)

        return _wrapped

    if hasattr(op_obj, "apply"):
        op_obj.apply = _wrap_apply(op_obj.apply)
        return op_obj
    if callable(op_obj):
        return _wrap_apply(op_obj)
    return op_obj


# New: also block masked-identity at depth 0: tile_masked(1,1,*)
def _gof_wrap_tile_masked_identity(op_obj):
    def _wrap_apply(f):
        def _wrapped(*a, **k):
            prog = _gof_prog_from_args(a)
            depth = _gof_depth_maybe(prog)
            if depth == 0:
                nums = _gof_collect_ints(*a)
                if len(nums) >= 3 and nums[-3] == 1 and nums[-2] == 1:
                    return _gof_identity_from_args(a)
            return f(*a, **k)

        return _wrapped

    if hasattr(op_obj, "apply"):
        op_obj.apply = _wrap_apply(op_obj.apply)
        return op_obj
    if callable(op_obj):
        return _wrap_apply(op_obj)
    return op_obj


# Utility: coerce a “single int” argument in many forms into a plain int
def _gof_coerce_single_int_arg(x):
    try:
        if isinstance(x, bool):
            return x
        if isinstance(x, int):
            return int(x)
        np_integer = getattr(np, "integer", ()) if hasattr(np, "integer") else ()
        if np_integer and isinstance(x, np_integer):  # type: ignore[arg-type]
            return int(x)
        if isinstance(x, (tuple, list)) and len(x) == 1:
            return _gof_coerce_single_int_arg(x[0])
        if isinstance(x, str):
            filtered = "".join(ch for ch in x if ch.isdigit() or ch == "-")
            if filtered and filtered not in {"-", "--"}:
                return int(filtered)
    except Exception:
        pass
    return x


# Generic single-int sanitizer for ops like remove_isolated, keep_n_largest, etc.
def _gof_wrap_single_int_arg(op_obj):
    if hasattr(op_obj, "apply"):
        original_apply = op_obj.apply

        def _apply(program, *args, **kwargs):
            if len(args) == 1:
                coerced = _gof_coerce_single_int_arg(args[0])
                args = (coerced,)
            return original_apply(program, *args, **kwargs)

        op_obj.apply = _apply
        return op_obj

    if callable(op_obj):
        original_call = op_obj

        def _call(*args, **kwargs):
            if len(args) == 1:
                coerced = _gof_coerce_single_int_arg(args[0])
                args = (coerced,)
            return original_call(*args, **kwargs)

        return _call

    return op_obj


# Depth-0 identity ban for phase-tile family: phase_tile, phase_tile_row, phase_tile_col
def _gof_wrap_phase_tile_identity(op_obj):
    def _wrap_apply(f):
        def _wrapped(*a, **k):
            prog = _gof_prog_from_args(a)
            depth = _gof_depth_maybe(prog)
            if depth == 0:
                ints = _gof_collect_ints(*a)
                if len(ints) >= 2:
                    if len(ints) >= 3:
                        ky, kx = ints[-3], ints[-2]
                    else:
                        ky, kx = ints[-2], ints[-1]
                    try:
                        if int(ky) == 1 and int(kx) == 1:
                            return _gof_identity_from_args(a)
                    except Exception:
                        pass
            return f(*a, **k)

        return _wrapped

    if hasattr(op_obj, "apply"):
        op_obj.apply = _wrap_apply(op_obj.apply)
        return op_obj
    if callable(op_obj):
        return _wrap_apply(op_obj)
    return op_obj


# === GOF-9000: Final early shift de-dupe (apply + args_enum) ================
# Refuse a second `shift(...)` in the first two steps (depth < 2) and treat
# `shift(0,0)` as an identity opener. Works for method-style/callable ops and
# removes bad candidates from args_enum.

# === Name/args helpers for successor gating ==================================
def _step_name_and_args(program):
    """Return (name_lower, args_list) for the program's last step, best-effort."""
    try:
        step = program.steps[-1]
        name = str(getattr(step, "op", step)).lower()
        args = list(getattr(step, "args", []))
        return name, args
    except Exception:
        return str(getattr(program, "_gof_last_op", "")).lower(), []


def _is_shift_name(name: str) -> bool:
    try:
        return "shift" in str(name).lower()
    except Exception:
        return False


def _last_step_name(program) -> str:
    try:
        step = program.steps[-1]
        return str(getattr(step, "op", step)).lower()
    except Exception:
        return str(getattr(program, "_gof_last_op", "")).lower()


def _append_step_clone(program, StepCls, ProgramCls, op_name, args):
    """Clone program and append Step(op_name, args); return None on failure."""
    try:
        new_steps = list(getattr(program, "steps", [])) + [StepCls(op=op_name, args=tuple(args or ()))]
        return ProgramCls(steps=new_steps)
    except Exception:
        return None


def _prog_signature(program) -> str:
    """Compact signature of a Program's steps, to drop duplicates early."""
    try:
        items = []
        for s in getattr(program, "steps", []):
            name = str(getattr(s, "op", s))
            args = tuple(getattr(s, "args", ()))
            items.append((name, args))
        raw = repr(items).encode("utf-8")
        return hashlib.sha1(raw).hexdigest()[:16]
    except Exception:
        return ""


def _op_class(name: str):
    """Rough class bucketing using known registries."""
    n = (name or "").lower()
    try:
        if n in (PALETTE_OPS if 'PALETTE_OPS' in globals() else set()):
            return "palette"
        if n in (TOPOLOGY_OPS if 'TOPOLOGY_OPS' in globals() else set()):
            return "topology"
        if n in (ALIGNMENT_OPS if 'ALIGNMENT_OPS' in globals() else set()):
            return "align"
        if n in (EXPANSION_OPS if 'EXPANSION_OPS' in globals() else set()):
            return "expand"
        if "color" in n or "swap" in n or "palette" in n:
            return "palette"
        if "tile" in n or "resize" in n or "scale" in n:
            return "expand"
        if "shift" in n or "center" in n or "align" in n:
            return "align"
        return "topology"
    except Exception:
        return "other"


class SelfMonitor:
    """Watches operator pressure, novelty, duplicates, and HFP health."""

    def __init__(self, settings):
        self.on = bool(getattr(settings, "monitor_enable", True))
        self.pressure_thresh = float(getattr(settings, "monitor_pressure_thresh", 0.70))
        self.inject_underused = bool(getattr(settings, "monitor_inject_underused", True))
        self.drop_dupes = bool(getattr(settings, "monitor_drop_dupe_sigs", True))
        self.visited_cap = int(getattr(settings, "monitor_visited_cap", 50000))
        self.sig_set = set()
        self.sig_q = deque()
        self.op_counts = {"palette": 0, "topology": 0, "align": 0, "expand": 0, "other": 0}
        self.face = "Explorer"  # Explorer / Observer / Navigator
        self.face_log: List[Tuple[int, str]] = []
        self.events: List[Any] = []
        self.tension_hist: List[float] = []
        self.plateau_N = int(getattr(settings, "monitor_plateau_N", 3))
        self.conflict_spike = float(getattr(settings, "monitor_conflict_spike", 0.03))
        self.novelty_bonus = float(getattr(settings, "monitor_novelty_bonus", 0.1))
        self.face_enabled = bool(getattr(settings, "monitor_face_log", True))
        try:
            dwell_default = int(os.getenv("ARC_FACE_DWELL_STEPS", "6"))
        except Exception:
            dwell_default = 6
        self._face_dwell = 0
        self._face_dwell_max = max(0, int(getattr(settings, "face_dwell_steps", dwell_default)))

        # HFP integration
        self.hfp_on = bool(getattr(settings, "hfp_enable", True))
        self.hfp_alpha = float(getattr(settings, "hfp_alpha", 0.5))
        self.hfp_rho_hi = float(getattr(settings, "hfp_rho_hi", 0.8))
        self.hfp_rho_lo = float(getattr(settings, "hfp_rho_lo", 0.55))
        self.hfp_break_R_min = float(getattr(settings, "hfp_break_R_min", 0.010))
        self.hfp_gate = bool(getattr(settings, "hfp_gate_by_health", True))
        self.hfp_gate_keep = bool(getattr(settings, "hfp_gate_keep", True))
        self.hfp_dbg = bool(getattr(settings, "hfp_debug_telemetry", True))
        self.rho_sm = 1.0
        self.c_hist: List[float] = []
        self.rho_hist: List[float] = []
        self.c_gof_hist: List[Tuple[float, float]] = []
        self.R_last: Optional[float] = None
        self.health = "guarded"

    def record_accept(self, program):
        if not self.on or program is None:
            return
        name = _last_step_name(program)
        cls = _op_class(name)
        self.op_counts[cls] = self.op_counts.get(cls, 0) + 1
        self.events.append(("accept", name, cls))

    def _pressure(self):
        total = sum(self.op_counts.values()) or 1
        top = max(self.op_counts.values()) if total > 0 else 0
        return top / total

    def prune_dupes(self, successors):
        if not (self.on and self.drop_dupes):
            return successors
        uniq = []
        for sp in successors:
            sig = _prog_signature(sp)
            if sig and sig in self.sig_set:
                continue
            uniq.append(sp)
            if sig:
                self.sig_set.add(sig)
                self.sig_q.append(sig)
                if len(self.sig_q) > self.visited_cap:
                    old = self.sig_q.popleft()
                    if old in self.sig_set:
                        self.sig_set.remove(old)
        return uniq

    def inject_underused_dimension(self, current, successors):
        if not (self.on and self.inject_underused):
            return successors
        if self._pressure() < self.pressure_thresh:
            return successors
        want = min(self.op_counts, key=lambda k: self.op_counts[k])
        reps_map = {
            "topology": [("keep_n_largest", (1,)), ("remove_isolated", (1,)), ("fill_holes", ())],
            "align": [("center_largest", ()), ("shift", (1, 0))],
            "palette": [("fill", (1,)), ("swapcolors", (1, 2))],
            "expand": [("resize", (3, 3)), ("tile", (2, 2))],
        }
        reps = [c for c in reps_map.get(want, []) if c[0] in OP_REGISTRY]
        if not reps:
            return successors
        try:
            StepCls = Step
            ProgramCls = Program
        except Exception:
            return successors
        have = {_last_step_name(sp) for sp in successors}
        for oname, oargs in reps:
            if oname in have:
                continue
            sp = _append_step_clone(current.program, StepCls, ProgramCls, oname, oargs)
            if sp is not None:
                successors.append(sp)
                self.events.append(("inject", want, oname))
                break
        return successors

    def update_hfp_from_tension(
        self,
        tension=None,
        eps: float = 1e-9,
        rho_override: float = None,
    ):
        if not self.hfp_on:
            return

        if rho_override is None:
            C_now = 1.0 / (eps + max(0.0, float(tension if tension is not None else 0.0)))
            self.c_hist.append(C_now)
            if len(self.c_hist) >= 2:
                rho = C_now / max(eps, self.c_hist[-2])
            else:
                rho = 1.0
        else:
            rho = float(rho_override)
            self.c_hist.append((self.c_hist[-1] * rho) if self.c_hist else 1.0)

        a = self.hfp_alpha
        self.rho_sm = a * rho + (1.0 - a) * self.rho_sm
        self.rho_hist.append(rho)

        if self.rho_sm >= self.hfp_rho_hi:
            self.health = "healthy"
        elif self.rho_sm < self.hfp_rho_lo:
            self.health = "degrading"
        else:
            self.health = "guarded"

        self.R_last = rvd_R(self.rho_sm)

    def gof_consciousness(self, program_depth: int = 0) -> Tuple[float, float]:
        try:
            kinds = sum(1 for v in self.op_counts.values() if v > 0)
            div = min(1.0, kinds / 6.0)
            dep = min(1.0, program_depth / 12.0)
            C_base = 100.0 * (0.5 * div + 0.5 * dep)
        except Exception:
            C_base = 50.0
        try:
            if not self.hfp_on:
                C_mod = C_base
            else:
                if self.rho_sm >= self.hfp_rho_hi:
                    C_mod = 1.5 * C_base
                elif self.rho_sm < self.hfp_rho_lo:
                    C_mod = 0.5 * C_base
                else:
                    C_mod = C_base
        except Exception:
            C_mod = C_base
        self.c_gof_hist.append((C_base, C_mod))
        return C_base, C_mod

    def should_health_break(self) -> bool:
        return bool(
            self.hfp_on
            and (self.R_last is not None)
            and (self.R_last < self.hfp_break_R_min)
        )

    def determine_face(self, C_mod: float) -> str:
        try:
            lo = self.hfp_rho_lo
            candidate = self.face

            if self.rho_sm < lo:
                candidate = "Explorer" if C_mod >= 50.0 else "Navigator"
            else:
                sustained = False
                if hasattr(self, "rho_hist") and len(self.rho_hist) >= 3:
                    sustained = self.rho_hist[-1] > 1.0 and self.rho_hist[-2] > 1.0
                if sustained:
                    candidate = "Navigator" if C_mod >= 50.0 else "Observer"
                else:
                    if C_mod < 50.0:
                        candidate = "Observer"
                    elif C_mod < 150.0:
                        candidate = "Navigator"
                    else:
                        candidate = "Explorer"

            if candidate != self.face and self._face_dwell > 0:
                self._face_dwell -= 1
                return self.face

            if candidate != self.face:
                self.face = candidate
                self._face_dwell = self._face_dwell_max

            return self.face
        except Exception:
            return self.face

    def gate_by_health(self, successors):
        if not (self.hfp_on and self.hfp_gate):
            return successors
        try:
            names_all = set(OP_REGISTRY.keys())
        except Exception:
            return successors
        try:
            topo = set(TOPOLOGY_OPS) if 'TOPOLOGY_OPS' in globals() else set()
            align = set(ALIGNMENT_OPS) if 'ALIGNMENT_OPS' in globals() else set()
            expa = set(EXPANSION_OPS) if 'EXPANSION_OPS' in globals() else set()
        except Exception:
            topo = align = expa = set()
        safe_low = (topo | align) - {"keep_rings"}
        medium = names_all - {n for n in names_all if n.startswith("phase_tile")}
        if self.rho_sm < self.hfp_rho_lo:
            allowed = set(safe_low)
        elif self.rho_sm >= self.hfp_rho_hi:
            allowed = set(names_all)
        else:
            allowed = set(medium)
        if self.hfp_gate_keep:
            keepers = {"keep_n_largest", "remove_isolated", "fill_holes", "largest", "center_largest", "shift", "rot90"}
            allowed |= {k for k in keepers if k in names_all}
        out = []
        for sp in successors:
            if _last_step_name(sp) in allowed:
                out.append(sp)
        return out

    def hfp_gof_dict(self):
        try:
            base, mod = self.c_gof_hist[-1] if self.c_gof_hist else (0.0, 0.0)
        except Exception:
            base, mod = 0.0, 0.0
        return {
            "rho": self.rho_hist[-1] if self.rho_hist else 1.0,
            "rho_sm": self.rho_sm,
            "R": self.R_last,
            "health": self.health,
            "C_gof_base": base,
            "C_gof": mod,
            "face": self.face,
        }

    def as_dict(self):
        return {
            "op_counts": dict(self.op_counts),
            "pressure": self._pressure(),
            "face": self.face,
            "events": self.events[:16],
            "visited": len(self.sig_set),
        }

def _gof_prev_op_lower(prog):
    """Best-effort previous op name in lowercase."""
    try:
        if hasattr(prog, "steps") and prog.steps:
            op = getattr(prog.steps[-1], "op", None)
            if isinstance(op, str):
                return op.lower()
    except Exception:
        pass
    try:
        return str(getattr(prog, "_gof_last_op", "")).lower()
    except Exception:
        return ""


def _gof_depth_of(prog):
    try:
        return len(prog.steps)
    except Exception:
        return int(getattr(prog, "_rails_steps", 0))


def _gof_wrap_shift_apply(op_obj):
    # Works for method-style and callable ops
    def _wrap(f):
        def _call(*a, **k):
            prog = None
            for x in a:
                if hasattr(x, "steps") or hasattr(x, "ctx") or hasattr(x, "input_grid"):
                    prog = x
                    break
            if prog is not None:
                depth = _gof_depth_of(prog)
                if depth < 2 and _gof_prev_op_lower(prog) == "shift":
                    return prog
                if depth == 0:
                    ints = _gof_collect_ints(*a)
                    if len(ints) >= 2:
                        try:
                            dy, dx = int(ints[-2]), int(ints[-1])
                            if dy == 0 and dx == 0:
                                return prog
                        except Exception:
                            pass
            return f(*a, **k)

        return _call

    if hasattr(op_obj, "apply"):
        op_obj.apply = _wrap(op_obj.apply)
        return op_obj
    if callable(op_obj):
        return _wrap(op_obj)
    return op_obj


def _gof_wrap_shift_args_enum(op_obj):
    # Remove shift candidates entirely if previous op was shift and depth < 2
    if not hasattr(op_obj, "args_enum"):
        return op_obj

    original_enum = op_obj.args_enum

    def _enum(*aa, **kk):
        prog = None
        for item in aa:
            if hasattr(item, "steps") or hasattr(item, "ctx") or hasattr(item, "input_grid"):
                prog = item
                break
        if prog is not None and _gof_depth_of(prog) < 2 and _gof_prev_op_lower(prog) == "shift":
            return iter(())
        return original_enum(*aa, **kk)

    op_obj.args_enum = _enum
    return op_obj
# === end shift de-dupe =======================================================


def _gof_wrap_mark_last(op_name, op_obj):
    def _wrap_apply(f):
        def _wrapped(*a, **k):
            result = f(*a, **k)
            if result is None:
                return result
            prog = None
            for x in a:
                if hasattr(x, "steps") or hasattr(x, "ctx") or hasattr(x, "input_grid"):
                    prog = x
                    break
            if prog is not None:
                try:
                    setattr(prog, "_gof_last_op", str(op_name).lower())
                except Exception:
                    pass
                try:
                    op_lower = str(op_name).lower()
                    if op_lower == "shift":
                        current = getattr(prog, "_gof_shift_count", 0)
                        if isinstance(current, bool):
                            current = 0
                        try:
                            current_int = int(current)
                        except Exception:
                            current_int = 0
                        setattr(prog, "_gof_shift_count", current_int + 1)
                    else:
                        current = getattr(prog, "_gof_shift_count", 0)
                        if isinstance(current, bool):
                            current = 0
                        try:
                            current_int = int(current)
                        except Exception:
                            current_int = 0
                        setattr(prog, "_gof_shift_count", current_int)
                except Exception:
                    pass
            return result

        return _wrapped

    if hasattr(op_obj, "apply"):
        op_obj.apply = _wrap_apply(op_obj.apply)
        return op_obj
    if callable(op_obj):
        wrapped = _wrap_apply(op_obj)
        try:
            setattr(wrapped, "__name__", getattr(op_obj, "__name__", wrapped.__name__))
        except Exception:
            pass
        if hasattr(op_obj, "name"):
            try:
                setattr(wrapped, "name", getattr(op_obj, "name"))
            except Exception:
                pass
        return wrapped
    return op_obj


# Optional: args_enum shields so bad candidates are never proposed
def _gof_shield_resize_args_enum(op_obj):
    if not hasattr(op_obj, "args_enum"):
        return op_obj
    old = op_obj.args_enum

    def new_enum(*aa, **kk):
        # Try to pull ctx/program if provided by the framework
        ctx = None
        prog = None
        for x in aa:
            if isinstance(x, dict):
                ctx = x
            else:
                prog = x if getattr(x, "__class__", None) else prog
        depth = 0
        if prog is not None:
            depth = _gof_prog_depth(prog)
        s = 0.0
        if prog is not None:
            s = getattr(prog, "_rails_s", None)
            if s is None:
                s = _gof_infer_scale_need_from_prog(prog)
                try:
                    setattr(prog, "_rails_s", s)
                except Exception:
                    pass
        elif isinstance(ctx, dict) and ("in_shape" in ctx and "out_shape" in ctx):
            ia = int(ctx["in_shape"][0]) * int(ctx["in_shape"][1])
            oa = int(ctx["out_shape"][0]) * int(ctx["out_shape"][1])
            s = -1.0 if oa < 0.9 * ia else (+1.0 if oa > 1.1 * ia else 0.0)
        hard = 1.0
        env_val = os.getenv("ARC_SCALE_HARD_THRESH")
        if env_val is not None:
            try:
                hard = float(env_val)
            except Exception:
                pass
        if depth < K_NO_SIZEOPS_AT_START and abs(s) < hard:
            return iter(())
        for args in old(*aa, **kk):
            try:
                h, w = int(args[0]), int(args[1])
            except Exception:
                continue
            # No direction known: let non-zero-depth cases through; step-0 will be blocked by apply()
            if s == 0.0:
                yield args
                continue
            # Direction known: keep only area-consistent candidates
            # (Shrinking: new area < current; Growing: new area > current.
            # We can't see current here reliably, so approximate by comparing to out_shape if present.)
            if isinstance(ctx, dict) and "out_shape" in ctx:
                oa = int(ctx["out_shape"][0]) * int(ctx["out_shape"][1])
                na = h * w
                if s <= -1.0 and na >= oa:  # shrinking needed → prefer <= target area
                    continue
                if s >= +1.0 and na <= oa:  # growing needed → prefer >= target area
                    continue
            yield args

    op_obj.args_enum = new_enum
    return op_obj


# Patch the current OP_REGISTRY once
try:
    _REG = OP_REGISTRY
    for _k in list(getattr(_REG, "keys", lambda: [])()):
        _op = _REG[_k]
        _nm = (getattr(_op, "name", _k)).lower()
        if _nm == "resize":
            _op = _gof_wrap_resize(_op)
            _op = _gof_shield_resize_args_enum(_op)
            _REG[_k] = _gof_wrap_mark_last("resize", _op)
        elif _nm == "scale":
            _REG[_k] = _gof_wrap_mark_last("scale", _gof_wrap_scale(_op))
        elif _nm == "tile":
            _REG[_k] = _gof_wrap_mark_last("tile", _gof_wrap_tile_identity(_op))
        elif _nm == "tile_masked":
            _REG[_k] = _gof_wrap_mark_last("tile_masked", _gof_wrap_tile_masked_identity(_op))
        elif _nm in ("phase_tile", "phase_tile_row", "phase_tile_col"):
            _REG[_k] = _gof_wrap_mark_last(_nm, _gof_wrap_phase_tile_identity(_op))
        elif _nm in ("remove_isolated", "keep_n_largest", "largest"):
            _REG[_k] = _gof_wrap_mark_last(_nm, _gof_wrap_single_int_arg(_op))
        elif "shift" in _nm:
            _op = _gof_wrap_shift_args_enum(_op)
            _op = _gof_wrap_shift_apply(_op)
            _REG[_k] = _gof_wrap_mark_last("shift", _op)
        else:
            _REG[_k] = _gof_wrap_mark_last(_nm, _op)
except Exception:
    pass

try:
    _telemetry_note(globals(), early_dir_guard_steps=K_DIR, no_sizeops_start=K_NO_SIZEOPS_AT_START)
except Exception:
    pass
# === end GOF-9000 directional/identity guards ===============================


# ============================================================================
# ============ Operation Order and Families (O2) ============
# ============================================================================

# --- Operation order and families (O2) ---
OP_ORDER_O2 = [
    # Tile family first (agent's best ordering)
    "tile", "tile_masked", "phase_tile_row", "phase_tile_col", "phase_tile", "resize",
    # Core geometry next
    "rot90", "mirror", "transpose", "shift", "crop", "grow", "shrink",
    # Composition / structural
    "stack", "add", "subtract", "largest", "keep_n_largest", "keep_size_range", "remove_isolated", "fill_holes", "keep_rings", "scale", "duplicate",
    # Color / palette last
    "mask", "replacecolor", "swapcolors", "invert", "fill", "outline", "threshold", "majority", "minority",
]

# Build a map for fast ordering
_OP_RANK = {name: i for i, name in enumerate(OP_ORDER_O2)}

# Whitelists for shape-then-color gating
SHAPE_OPS = {
    "tile", "tile_masked", "phase_tile_row", "phase_tile_col", "phase_tile", "resize",
    "rot90", "mirror", "transpose", "shift", "crop", "grow", "shrink",
    "stack", "add", "subtract", "largest", "keep_n_largest", "keep_size_range", "remove_isolated", "fill_holes", "keep_rings", "scale", "duplicate",
}
COLOR_OPS = {
    "mask", "replacecolor", "swapcolors", "invert", "fill", "outline", "threshold", "majority", "minority",
    # permit small finishing shifts too if desired:
    "shift",
}

PALETTE_OPS = {
    "mask",
    "replacecolor",
    "swapcolors",
    "invert",
    "fill",
    "outline",
    "threshold",
    "majority",
    "minority",
}
# --- Guard: define underscore op-family sets if not yet defined (import-order safe) ---
try:
    _TOPOLOGY_OPS
except NameError:
    _TOPOLOGY_OPS = {
        "largest", "keep_n_largest", "keep_size_range",
        "fill_holes", "keep_rings", "remove_isolated", "mask",
        "center_largest", "fill"
    }
try:
    _ALIGNMENT_OPS
except NameError:
    _ALIGNMENT_OPS = {
        "shift", "flipud", "fliplr",
        "rot90", "rot180", "rot270", "transpose", "align"
    }
try:
    _TILING_OPS
except NameError:
    _TILING_OPS = {"tile", "tile_masked", "phase_tile", "phase_tile_row", "phase_tile_col"}
try:
    _RESIZE_OPS
except NameError:
    _RESIZE_OPS = {"resize", "scale"}
try:
    _PALETTE_OPS
except NameError:
    _PALETTE_OPS = {"replacecolor", "swapcolors", "invert", "recolor", "palette_map"}
# --- end guard ---

TOPOLOGY_OPS = set(_TOPOLOGY_OPS)
ALIGNMENT_OPS = set(_ALIGNMENT_OPS)
EXPANSION_OPS = set(_TILING_OPS) | set(_RESIZE_OPS)


# ============================================================================
# ============ dsl/program.py ============
# ============================================================================

@dataclass
class Step:
    """A single step in a program: (op_name, args)."""
    op: str
    args: Tuple[Any, ...]

    def __repr__(self):
        return f"Step({self.op}, {self.args})"


@dataclass
class Program:
    """A sequence of Steps."""
    steps: List[Step] = field(default_factory=list)

    def __repr__(self):
        return f"Program({len(self.steps)} steps)"

    def __len__(self):
        return len(self.steps)

    def copy(self):
        return Program([Step(s.op, s.args) for s in self.steps])

    def to_tuple(self):
        """Hashable representation."""
        return tuple((s.op, s.args) for s in self.steps)


def program_to_str(prog: Program) -> str:
    """Human-readable format."""
    lines = []
    for i, step in enumerate(prog.steps):
        lines.append(f"{i+1}. {step.op}{step.args}")
    return "\n".join(lines)


def _apply_op_priors_soft(priors: Dict[str, float], alpha: float = 1.0) -> None:
    """Softly bias op ordering for successor generation."""
    try:
        def score(name: str) -> float:
            p = float(priors.get(name, 0.0))
            return math.log1p(max(0.0, alpha) * max(0.0, p))

        names = list(OP_REGISTRY.keys())
        names.sort(key=lambda n: score(n), reverse=True)
        globals()["_OP_NAMES_BIASED"] = names
    except Exception:
        pass


_TOKEN_PARSE_RE = re.compile(r"^\s*([a-zA-Z_]\w*)\s*\((.*)\)\s*$")


def _program_from_ops_tokens(tokens: List[str]) -> Program:
    """Parse a list of op tokens into a Program. Safe fallback returns empty program."""
    if MEM is not None and hasattr(MEM, "parse_op_tokens"):
        try:
            steps = MEM.parse_op_tokens(tokens)
            return Program(list(steps))
        except Exception:
            pass

    steps: List[Step] = []
    for tok in tokens or []:
        tok = str(tok).strip()
        match = _TOKEN_PARSE_RE.match(tok)
        if not match:
            if "(" not in tok:
                steps.append(Step(tok, ()))
            continue
        name, arg_str = match.group(1), match.group(2)
        parts = [p.strip() for p in arg_str.split(",") if p.strip()]
        args: List[Any] = []
        for part in parts:
            try:
                args.append(int(part))
                continue
            except Exception:
                pass
            if part.lower() in ("true", "false"):
                args.append(part.lower() == "true")
            else:
                args.append(part)
        steps.append(Step(name, tuple(args)))
    return Program(steps)


def _format_step_token(step: Step) -> str:
    op = step.op
    if not step.args:
        return op

    def _fmt(arg):
        if isinstance(arg, (list, tuple)):
            return "(" + ",".join(_fmt(a) for a in arg) + ")"
        if isinstance(arg, np.ndarray):
            return f"array{tuple(arg.shape)}"
        if hasattr(arg, "tolist") and not isinstance(arg, (str, bytes)):
            try:
                arr = np.asarray(arg)
                return f"array{tuple(arr.shape)}"
            except Exception:
                pass
        if isinstance(arg, (np.integer, int)):
            return str(int(arg))
        if isinstance(arg, (np.floating, float)):
            return f"{float(arg):.3g}"
        return str(arg)

    args_str = ",".join(_fmt(a) for a in step.args)
    return f"{op}({args_str})"


_TILING_OPS = {"tile", "tile_masked", "phase_tile", "phase_tile_row", "phase_tile_col"}
_ALIGNMENT_OPS = {"rot90", "mirror", "transpose"}
_GEOMETRY_OPS = {"shift", "crop", "grow", "shrink", "duplicate", "stack"}
_TOPOLOGY_OPS = {"largest", "keep_n_largest", "keep_size_range", "fill_holes", "keep_rings", "remove_isolated"}
_PALETTE_KEYWORDS = ("palette", "recolor", "swap", "replacecolor", "invert", "majority", "minority")
_COMPOSITION_OPS = {"stack", "add", "subtract"}
_RESIZE_OPS = {"resize", "scale"}
_MASK_KEYWORDS = ("mask", "outline", "threshold")


# --- Identity + rail helpers ------------------------------------------------
# Identity policy: only allow a no-op when shapes already match AND we're not at depth 0.
IDENTITY_POLICY = "allow_equal_shape_only"


def _last_step_op(program):
    try:
        return program.steps[-1].op
    except Exception:
        return None


def _last_step_args(program):
    try:
        return tuple(program.steps[-1].args)
    except Exception:
        return ()


def _is_identity_op(op_name, args, in_shape):
    try:
        if op_name == "tile" and len(args) >= 2:
            return int(args[0]) == 1 and int(args[1]) == 1
        if op_name == "scale" and len(args) >= 1:
            return float(args[0]) == 1.0
        if op_name == "resize" and len(args) == 2 and in_shape:
            h, w = int(args[0]), int(args[1])
            try:
                in_h, in_w = int(in_shape[0]), int(in_shape[1])
            except Exception:
                return False
            return (in_h * in_w) == (h * w)
    except Exception:
        pass
    return False


def _should_block_identity(op_name, args, in_shape, out_shape, depth):
    if not _is_identity_op(op_name, args, in_shape):
        return False

    if IDENTITY_POLICY == "allow_equal_shape_only":
        if in_shape and out_shape:
            try:
                in_tuple = (int(in_shape[0]), int(in_shape[1]))
                out_tuple = (int(out_shape[0]), int(out_shape[1]))
            except Exception:
                in_tuple = tuple(in_shape) if in_shape else None
                out_tuple = tuple(out_shape) if out_shape else None
            if in_tuple and out_tuple and in_tuple != out_tuple:
                return True
        return depth == 0

    return True


def _gate_allowed_ops(
    settings: _InternalSearchSettings,
    depth: int,
    phi: Optional[np.ndarray],
    allowed_names: Iterable[str],
) -> Set[str]:
    names = set(allowed_names) if allowed_names is not None else set(OP_REGISTRY.keys())

    try:
        k_palette = int(getattr(settings, "early_palette_block_steps", 3))
    except Exception:
        k_palette = 3
    if depth < max(0, k_palette):
        names -= (PALETTE_OPS & names)

    if depth < K_NO_SIZEOPS_AT_START:
        try:
            s = float(phi[0]) if (phi is not None and len(phi) > 0) else 0.0
        except Exception:
            s = 0.0
        try:
            hard = float(getattr(settings, "scale_hard_thresh", 0.60))
        except Exception:
            hard = 0.60
        if abs(s) < hard:
            names = {n for n in names if n not in _RESIZE_OPS and n not in _TILING_OPS}

    if getattr(settings, "rails_scale_hard", True) and phi is not None and len(phi) > 0:
        try:
            s = float(phi[0])
            thr = float(getattr(settings, "scale_hard_thresh", 0.60))
            k_rail = int(getattr(settings, "scale_hard_steps", 3))
        except Exception:
            s, thr, k_rail = 0.0, 1.0, 3

        if depth < max(0, k_rail):
            if s <= -thr:
                names = names & (TOPOLOGY_OPS | ALIGNMENT_OPS)
            elif s >= thr:
                names = names & (EXPANSION_OPS | ALIGNMENT_OPS)

    # --- GOF-9000: keep object tools available when object signal is strong ---
    try:
        obj_mag = abs(float(phi[1])) if (phi is not None and len(phi) > 1) else 0.0
    except Exception:
        obj_mag = 0.0

    object_soft_pref = obj_mag > 0.30
    if object_soft_pref:
        STRONG_TOPO = {"keep_n_largest", "remove_isolated", "fill_holes", "largest", "center_largest"}
        try:
            names = (set(names) - {"keep_rings"}) | (STRONG_TOPO & set(OP_REGISTRY.keys()))
        except Exception:
            names = (set(names) - {"keep_rings"}) | STRONG_TOPO
        try:
            setattr(settings, "_object_soft_pref", True)
        except Exception:
            pass
    # --- end widen ---

    return names


def _op_family_name(op_name: str) -> str:
    name = op_name or ""
    lower = name.lower()
    if lower in _TILING_OPS or "tile" in lower:
        return "tiling"
    if lower in _ALIGNMENT_OPS:
        return "alignment"
    if lower in _RESIZE_OPS:
        return "resize"
    if lower in _COMPOSITION_OPS:
        return "composition"
    if lower in _TOPOLOGY_OPS:
        return "topology"
    if any(key in lower for key in _PALETTE_KEYWORDS):
        return "palette"
    if lower in _GEOMETRY_OPS:
        return "geometry"
    if any(key in lower for key in _MASK_KEYWORDS):
        return "mask"
    return "other"


def _op_family_tag(step: Step) -> str:
    try:
        return _op_family_name(step.op)
    except Exception:
        return "other"


# ============================================================================
# ============ dsl/interpreter.py ============
# ============================================================================

def interpret_program(prog: Program, input_grid: np.ndarray) -> np.ndarray:
    """Execute a program on an input grid, returning the final grid."""
    state = input_grid.copy()
    states = [state.copy()]  # Keep history for register references
    
    for step in prog.steps:
        state = apply_step(state, step, states)
        states.append(state.copy())
    
    return state


def apply_step(grid: np.ndarray, step: Step, states=None) -> np.ndarray:
    """Apply a single step to a grid, with support for grid register references."""
    if states is None:
        states = [grid]
    
    op_name = step.op
    args = step.args

    if op_name not in OP_REGISTRY:
        raise ValueError(f"Unknown op: {op_name}")

    op_fn = OP_REGISTRY[op_name]

    # Handle multi-grid ops with register resolution
    try:
        if step.op in ("add", "subtract"):
            g2 = resolve_grid_arg(args[0], states)
            if g2 is None:
                return grid.copy()
            return op_fn(grid, g2)
        
        if step.op == "stack":
            g2 = resolve_grid_arg(args[0], states)
            if g2 is None:
                return grid.copy()
            axis = int(args[1])
            return op_fn(grid, g2, axis)
        
        # Regular ops
        result = op_fn(grid, *args)
    except Exception as e:
        # If error, return unchanged grid
        result = grid.copy()

    if result is None:
        try:
            return grid.copy()
        except Exception:
            return grid

    return result


# ============================================================================
# ============ perception/features.py ============
# ============================================================================

def task_features(train_pairs: List[Tuple[np.ndarray, np.ndarray]]) -> Dict[str, Any]:
    """Extract features from train pairs for OCO."""
    if not train_pairs:
        return {
            "n_examples": 0,
            "avg_in_size": (0, 0),
            "avg_out_size": (0, 0),
            "size_stable": False,
            "shape_stable": False,
            "palette_size_in": 0,
            "palette_size_out": 0,
            "complexity": 0.0,
            "aspect_ratio_in": 1.0,
            "aspect_ratio_out": 1.0,
        }

    n = len(train_pairs)
    sizes_in = []
    sizes_out = []
    palettes_in = []
    palettes_out = []

    for (x, y) in train_pairs:
        sizes_in.append(x.shape)
        sizes_out.append(y.shape)
        palettes_in.append(len(np.unique(x)))
        palettes_out.append(len(np.unique(y)))

    avg_in_size = (
        int(np.mean([s[0] for s in sizes_in])),
        int(np.mean([s[1] for s in sizes_in]))
    )
    avg_out_size = (
        int(np.mean([s[0] for s in sizes_out])),
        int(np.mean([s[1] for s in sizes_out]))
    )

    size_stable = all(s == sizes_in[0] for s in sizes_in)
    shape_stable = all(s == sizes_out[0] for s in sizes_out)

    palette_in = int(np.mean(palettes_in))
    palette_out = int(np.mean(palettes_out))

    complexity = (palette_in + palette_out) / 2.0

    aspect_in = avg_in_size[1] / max(avg_in_size[0], 1)
    aspect_out = avg_out_size[1] / max(avg_out_size[0], 1)

    return {
        "n_examples": n,
        "avg_in_size": avg_in_size,
        "avg_out_size": avg_out_size,
        "size_stable": size_stable,
        "shape_stable": shape_stable,
        "palette_size_in": palette_in,
        "palette_size_out": palette_out,
        "complexity": complexity,
        "aspect_ratio_in": aspect_in,
        "aspect_ratio_out": aspect_out,
    }


# ============================================================================
# ============ oco/octonion.py ============
# ============================================================================

def compute_phi(features: Dict[str, Any]) -> np.ndarray:
    """
    Compute 8D feature embedding φ from task features.

    φ = [scale, objectness, palette, geometry, alignment, topology, pattern, composition]
    (We no longer lean on octonion algebra; this is a compact feature vector.)
    """
    n = features["n_examples"]
    size_stable = features["size_stable"]
    shape_stable = features["shape_stable"]
    palette_in = features["palette_size_in"]
    palette_out = features["palette_size_out"]
    complexity = features["complexity"]
    aspect_in = features["aspect_ratio_in"]
    aspect_out = features["aspect_ratio_out"]

    # Scale component
    in_h, in_w = features["avg_in_size"]
    out_h, out_w = features["avg_out_size"]
    scale = math.log(max(out_h * out_w, 1)) - math.log(max(in_h * in_w, 1))

    # Objectness (palette difference)
    objectness = palette_out - palette_in

    # Palette (color complexity)
    palette = complexity / 10.0

    # Geometry (aspect ratio change)
    geometry = abs(aspect_out - aspect_in)

    # Alignment (size stability)
    alignment = 1.0 if size_stable else 0.0

    # Topology (shape stability)
    topology = 1.0 if shape_stable else 0.0

    # Pattern (number of examples)
    pattern = n / 10.0

    # Composition (interaction term)
    composition = scale * objectness * 0.1

    phi = np.array([
        scale,
        objectness,
        palette,
        geometry,
        alignment,
        topology,
        pattern,
        composition
    ], dtype=np.float32)

    return phi


def phi_to_family(phi: np.ndarray) -> str:
    """Classify task family from φ."""
    if phi is None or len(phi) < 8:
        return "unknown"

    core = np.array(phi[:8], dtype=np.float32)
    abs_phi = np.abs(core)
    idx = np.argmax(abs_phi)

    families = [
        "scale",
        "objectness",
        "palette",
        "geometry",
        "alignment",
        "topology",
        "pattern",
        "composition"
    ]

    return families[idx]


# ============================================================================
# ============ oco/associator.py ============
# ============================================================================

def compute_program_tension(prog: Program, phi: np.ndarray) -> float:
    """
    Compute tension T_prog(φ) between program structure and task embedding.
    
    T_prog = Σ_i |op_i ⊗ φ|
    
    This measures how well the program's operations align with the task's
    octonion structure.
    """
    if phi is None or len(phi) != 8:
        return 0.0

    tension = 0.0
    for step in prog.steps:
        op_vec = op_to_vector(step.op)
        # Simple dot product as pseudo-octonion multiplication
        tension += abs(np.dot(op_vec, phi))

    return tension


def compute_slice_tension(state: np.ndarray, phi: np.ndarray) -> float:
    """
    Compute tension T_slice(φ) between current state and task embedding.
    
    T_slice = |state_features ⊗ φ|
    
    This measures how well the current state aligns with the task's
    expected structure.
    """
    if phi is None or len(phi) != 8:
        return 0.0

    state_vec = state_to_vector(state)
    tension = abs(np.dot(state_vec, phi))

    return tension


def op_to_vector(op_name: str) -> np.ndarray:
    """Map operation to 8D vector for tension computation."""
    # Phase-tiling family gets scale + pattern axes to avoid zero-tension free ride
    PHASE_TILE_VEC = np.array([1, 0, 0, 0, 0, 0, 1, 0], dtype=np.float32)
    
    op_map = {
        "fill": [0, 0, 1, 0, 0, 0, 0, 0],
        "mirror": [0, 0, 0, 1, 1, 0, 0, 0],
        "rot90": [0, 0, 0, 1, 0, 0, 0, 0],
        "transpose": [0, 0, 0, 1, 0, 0, 0, 0],
        "shrink": [1, 0, 0, 0, 0, 0, 0, 0],
        "grow": [1, 0, 0, 0, 0, 0, 0, 0],
        "add": [0, 1, 0, 0, 0, 0, 0, 1],
        "mask": [0, 0, 1, 0, 0, 0, 0, 0],
        "invert": [0, 0, 1, 0, 0, 0, 0, 0],
        "duplicate": [0, 0, 0, 0, 0, 0, 0, 0],
        "scale": [1, 0, 0, 0, 0, 0, 0, 0],
        "tile": PHASE_TILE_VEC.tolist(),
        "tile_masked": PHASE_TILE_VEC.tolist(),
        "phase_tile": PHASE_TILE_VEC.tolist(),
        "phase_tile_row": PHASE_TILE_VEC.tolist(),
        "phase_tile_col": PHASE_TILE_VEC.tolist(),
        "crop": [1, 0, 0, 0, 0, 0, 0, 0],
        "shift": [0, 0, 0, 0, 1, 0, 0, 0],
        "replacecolor": [0, 0, 1, 0, 0, 0, 0, 0],
        "swapcolors": [0, 0, 1, 0, 0, 0, 0, 0],
        "outline": [0, 1, 0, 0, 0, 1, 0, 0],
        "majority": [0, 0, 1, 0, 0, 0, 0, 0],
        "minority": [0, 0, 1, 0, 0, 0, 0, 0],
        "threshold": [0, 0, 1, 0, 0, 0, 0, 0],
        "largest": [0, 1, 0, 0, 0, 1, 0, 0],
        "fill_holes": [0, 0, 0, 0, 0, 1, 0, 0],
        "keep_rings": [0, 1, 0, 0, 0, 1, 0, 0],
        "keep_n_largest": [0, 1, 0, 0, 0, 1, 0, 0],
        "keep_size_range": [0, 1, 0, 0, 0, 1, 0, 0],
        "resize": [1, 0, 0, 0, 0, 0, 0, 0],
        "stack": [0, 0, 0, 0, 0, 0, 0, 1],
        "subtract": [0, 1, 0, 0, 0, 0, 0, 0],
    }

    vec = op_map.get(op_name, [0, 0, 0, 0, 0, 0, 0, 0])
    return np.array(vec, dtype=np.float32)


def state_to_vector(state: np.ndarray) -> np.ndarray:
    """Map grid state to 8D vector for tension computation."""
    h, w = state.shape
    size = math.log(max(h * w, 1))
    n_colors = len(np.unique(state))
    aspect = w / max(h, 1)
    density = np.count_nonzero(state) / max(state.size, 1)

    vec = np.array([
        size,
        n_colors,
        density,
        aspect,
        0.0,  # alignment (computed elsewhere)
        0.0,  # topology (computed elsewhere)
        0.0,  # pattern (computed elsewhere)
        0.0,  # composition (computed elsewhere)
    ], dtype=np.float32)

    return vec


# ============================================================================
# ============ oco/cost.py ============
# ============================================================================

@dataclass
class _InternalSearchSettings:
    """Configuration for beam search with OCO."""
    beam_width: int = 128
    max_depth: int = 10
    max_seconds: float = 3.0
    lambda_len: float = 0.20
    lambda1: float = 0.30  # program tension weight
    lambda2: float = 0.20  # slice tension weight
    slice_guard_thresh: float = 0.80
    allow_offslice_early: bool = False
    public_mode: bool = False
    log_every: int = 200
    seed: int = 1337
    _disable_rotation: bool = False
    max_train_pairs_for_beam: int = 2
    use_meta_controller: bool = False
    always_two_attempts: bool = False  # wrapper now manages alternates; keep default conservative
    test_palette_policy: str = "second_only_guarded"
    allow_finishers_on_masked: bool = False
    no_polish: bool = False
    stop_if_diversity: float = 0.20
    div_lambda: float = 0.70
    iou_cap: float = 0.92
    max_bounces: int = 8
    _hfp_prevC: Optional[float] = None
    _hfp_rho: Optional[float] = None
    _hfp_rho_smoothed: Optional[float] = None
    _hfp_ready: bool = False
    _rho_samples: List[float] = field(default_factory=list)
    _hfp_sustained: bool = False
    _hfp_prev_increase: bool = False
    # Octonion (palette8) difficulty prior — default ON (disable via CLI flag)
    use_octo_prior: bool = True
    octo_alpha: float = 0.25
    octo_clip: float = 2.0
    _octo_stats: Optional[_RunningStats] = None
    bounce_if_lowdiv: bool = True
    lowdiv_thr: float = 0.05
    octo_z_min_for_bounce: float = 1.20
    bounce_max: int = 8
    # GOF-9000 constraint pack
    block_identity: bool = True
    rails_scale_hard: bool = True
    scale_hard_thresh: float = 0.60
    scale_hard_steps: int = 3
    early_palette_block_steps: int = 3
    # GOF-9000 SelfMonitor defaults
    monitor_enable: bool = True
    monitor_pressure_thresh: float = 0.70
    monitor_plateau_N: int = 3
    monitor_visited_cap: int = 50000
    monitor_inject_underused: bool = True
    monitor_drop_dupe_sigs: bool = True
    # === Memory v1.0 ===
    use_memory: bool = True                 # master switch
    memory_use_motifs: bool = True          # seed short programs
    memory_use_priors: bool = True          # bias op policy
    memory_dir: Optional[str] = None        # default from env ARC_MEMORY_DIR or ./memory_bank
    priors_alpha: float = 1.0               # strength for priors nudges
    motif_topk: int = 1                     # max motifs to seed
    diversity_guard: bool = True            # A/B diversity guard
    diversity_b_force_first: bool = False   # force different first op for B when align-first
    attemptB_beam_scale: float = 1.0        # heuristic scaling for diversity alt
    attemptB_time_scale: float = 1.0        # heuristic scaling for debate/time
    attemptB_auto_boost: bool = True        # auto-boost B when IoU cap keeps triggering
    attemptB_boost_beam: float = 1.5
    attemptB_boost_time: float = 1.3
    shaped_cost: bool = True
    explore_fraction: float = 0.35
    allow_uphill: bool = True
    uphill_keep_fraction: float = 0.15
    ephemeral_macros: bool = False
    macro_max_depth_start: int = 2
    macro_candidates_limit: int = 6
    hfp_enable: bool = True
    hfp_alpha: float = 0.50
    hfp_rho_hi: float = 0.80
    hfp_rho_lo: float = 0.55
    hfp_break_R_min: float = 0.010
    hfp_gate_by_health: bool = True
    hfp_gate_keep: bool = True
    hfp_debug_telemetry: bool = True

    def __post_init__(self):
        env_val = os.getenv("ARC_SCALE_HARD_THRESH")
        if env_val is not None:
            try:
                self.scale_hard_thresh = float(env_val)
            except Exception:
                pass
        self.shaped_cost = _envbool("ARC_SHAPED_COST", self.shaped_cost)
        self.explore_fraction = _envfloat("ARC_EXPLORE_FRAC", self.explore_fraction)
        self.allow_uphill = _envbool("ARC_ALLOW_UPHILL", self.allow_uphill)
        self.uphill_keep_fraction = _envfloat("ARC_UPHILL_KEEP", self.uphill_keep_fraction)
        self.attemptB_auto_boost = _envbool("ARC_ATTEMPTB_AUTO_BOOST", self.attemptB_auto_boost)
        self.attemptB_boost_beam = _envfloat("ARC_ATTEMPTB_BOOST_BEAM", self.attemptB_boost_beam)
        self.attemptB_boost_time = _envfloat("ARC_ATTEMPTB_BOOST_TIME", self.attemptB_boost_time)
        self.ephemeral_macros = _envbool("ARC_EPHEMERAL_MACROS", self.ephemeral_macros)
        self.macro_max_depth_start = _envint("ARC_MACRO_MAX_DEPTH_START", self.macro_max_depth_start)
        self.macro_candidates_limit = _envint("ARC_MACRO_LIMIT", self.macro_candidates_limit)


def _infer_bg_color(arr: np.ndarray) -> int:
    hist = np.bincount(arr.ravel().astype(int), minlength=10)
    return int(hist.argmax())


def _binary_mask(arr: np.ndarray, bg: Optional[int] = None) -> np.ndarray:
    if bg is None:
        bg = _infer_bg_color(arr)
    return (arr != bg).astype(np.uint8)


def _pool2x(mask: np.ndarray, iters: int = 1) -> np.ndarray:
    m = mask
    for _ in range(max(0, iters)):
        h, w = m.shape
        if h < 2 or w < 2:
            break
        hh, ww = (h // 2) * 2, (w // 2) * 2
        if hh == 0 or ww == 0:
            break
        m = m[:hh, :ww].reshape(hh // 2, 2, ww // 2, 2).max(axis=(1, 3))
    return m


def _pad_to_shape(arr: np.ndarray, shape: Tuple[int, int], fill: int) -> np.ndarray:
    h, w = arr.shape
    H, W = shape
    out = np.full((H, W), int(fill), dtype=int)
    out[:h, :w] = arr[:H, :W]
    return out


def _soft_iou(a: np.ndarray, b: np.ndarray) -> float:
    if a.ndim != 2 or b.ndim != 2:
        return 0.0
    H = max(a.shape[0], b.shape[0])
    W = max(a.shape[1], b.shape[1])
    if H == 0 or W == 0:
        return 1.0
    bg_a = _infer_bg_color(a)
    bg_b = _infer_bg_color(b)
    a_pad = _pad_to_shape(a, (H, W), bg_a)
    b_pad = _pad_to_shape(b, (H, W), bg_b)
    ma = _pool2x(_binary_mask(a_pad, bg_a))
    mb = _pool2x(_binary_mask(b_pad, bg_b))
    inter = np.logical_and(ma, mb).sum(dtype=float)
    union = np.logical_or(ma, mb).sum(dtype=float)
    return float(inter / union) if union > 0 else 1.0


def _hist_l1(a: np.ndarray, b: np.ndarray) -> float:
    ha = np.bincount(a.ravel().astype(int), minlength=10).astype(float)
    hb = np.bincount(b.ravel().astype(int), minlength=10).astype(float)
    ha /= max(1.0, ha.sum())
    hb /= max(1.0, hb.sum())
    return float(np.abs(ha - hb).sum()) * 0.5


def _center_error(a: np.ndarray, b: np.ndarray) -> float:
    ma = _binary_mask(a)
    mb = _binary_mask(b)
    if ma.sum() == 0 or mb.sum() == 0:
        return 1.0
    ya, xa = np.nonzero(ma)
    yb, xb = np.nonzero(mb)
    cy_a, cx_a = ya.mean(), xa.mean()
    cy_b, cx_b = yb.mean(), xb.mean()
    H = max(a.shape[0], b.shape[0])
    W = max(a.shape[1], b.shape[1])
    if H <= 1 and W <= 1:
        return 0.0
    return (abs(cy_a - cy_b) / max(1.0, H - 1) + abs(cx_a - cx_b) / max(1.0, W - 1)) * 0.5


def _compute_shaped_cost(curr_grid: np.ndarray, target_grid: np.ndarray, phi: np.ndarray) -> Dict[str, float]:
    try:
        curr = np.asarray(curr_grid)
        tgt = np.asarray(target_grid)
    except Exception:
        return {"shaped": 1.0, "soft_iou": 0.0, "com_err": 1.0, "hist_l1": 1.0}
    if curr.ndim != 2 or tgt.ndim != 2:
        return {"shaped": 1.0, "soft_iou": 0.0, "com_err": 1.0, "hist_l1": 1.0}
    s_iou = _soft_iou(curr, tgt)
    com_err = _center_error(curr, tgt)
    hist = _hist_l1(curr, tgt)
    scale_sig = float(abs(phi[0])) if phi is not None and len(phi) > 0 else 0.0
    palette_sig = float(abs(phi[2])) if phi is not None and len(phi) > 2 else 0.0
    w_shape = 0.6 + 0.3 * min(1.0, scale_sig)
    w_com = 0.25
    w_hist = 0.15 + 0.2 * min(1.0, palette_sig)
    total = w_shape + w_com + w_hist
    w_shape /= total
    w_com /= total
    w_hist /= total
    shaped = w_shape * (1.0 - s_iou) + w_com * com_err + w_hist * hist
    return {
        "shaped": float(shaped),
        "soft_iou": float(s_iou),
        "com_err": float(com_err),
        "hist_l1": float(hist),
    }


def compute_cost(
    prog: Program,
    input_grid: np.ndarray,
    target_grid: np.ndarray,
    phi: np.ndarray,
    settings: _InternalSearchSettings
) -> float:
    """
    OCO-augmented cost function:
    
    C = C_match + λ_len*L + λ1*T_prog + λ2*T_slice
    
    Where:
    - C_match: Pixel mismatch cost
    - L: Program length
    - T_prog: Program tension
    - T_slice: Slice tension
    
    Shape-stage bias: When shape matches, downweight both λ1 and λ2, add accuracy bonus.
    """
    # Execute program
    try:
        pred = interpret_program(prog, input_grid)
    except Exception:
        setattr(
            prog,
            "_cost_stats",
            {
                "error": True,
                "binary_cost": 1.0,
                "shaped_cost": 1.0,
                "soft_iou": 0.0,
                "com_err": 1.0,
                "hist_l1": 1.0,
            },
        )
        return 1e9

    pred_arr = np.asarray(pred)
    tgt_arr = np.asarray(target_grid)
    shape_same = pred_arr.shape == tgt_arr.shape
    if shape_same:
        diff = np.sum(pred_arr != tgt_arr)
        c_match = diff / max(tgt_arr.size, 1)
    else:
        c_match = 1.0

    acc = (1.0 - c_match) if shape_same else 0.0

    # Keep original lambda1, lambda2 first
    lam1 = settings.lambda1
    lam2 = settings.lambda2

    # AFTER shape-match: reduce both tensions + small pixel-accuracy bonus
    if shape_same:
        lam1 *= 0.50   # downweight program tension when shape matches
        lam2 *= 0.30   # downweight slice tension when shape matches
        beta = 0.05    # pixel-accuracy bonus
    else:
        beta = 0.0

    # Length cost
    c_len = len(prog) * settings.lambda_len

    # OCO costs
    t_prog = compute_program_tension(prog, phi)
    t_slice = compute_slice_tension(pred, phi)

    c_oco = lam1 * t_prog + lam2 * t_slice

    shaped_stats = _compute_shaped_cost(pred_arr, tgt_arr, phi)
    px_term = c_match
    if getattr(settings, "shaped_cost", False):
        px_term = float(shaped_stats.get("shaped", c_match))

    total = px_term + c_len + c_oco - beta * acc

    # shape-stage: tiny prior for topology selection ops (mirrors tile_masked prior)
    if shape_same and len(prog) > 0:
        last = prog.steps[-1].op
        if last in ("keep_rings", "fill_holes"):
            total -= 0.01

    prog_stats = {
        "binary_cost": float(c_match),
        "shaped_cost": float(px_term),
        "soft_iou": float(shaped_stats.get("soft_iou", 0.0)),
        "com_err": float(shaped_stats.get("com_err", 1.0)),
        "hist_l1": float(shaped_stats.get("hist_l1", 1.0)),
        "program_len": int(len(prog)),
        "cost": float(total),
    }
    setattr(prog, "_cost_stats", prog_stats)

    return float(total)


# ============================================================================
# ============ search/beam.py ============
# ============================================================================

@dataclass
class Candidate:
    """A candidate program with its cost."""
    program: Program
    cost: float
    depth: int = 0
    telemetry: Dict[str, Any] = field(default_factory=dict)
    cost_delta: float = 0.0

    def __lt__(self, other):
        return self.cost < other.cost


def _two_lane_prune(beam: List[Candidate], beam_width: int, best_cost: float, settings) -> List[Candidate]:
    if not beam:
        return []
    K = max(1, int(beam_width))
    explore_frac = float(getattr(settings, "explore_fraction", 0.0))
    explore_frac = max(0.0, min(1.0, explore_frac))
    k_explore = int(round(explore_frac * K))
    k_explore = min(max(0, k_explore), K)
    k_exploit = max(0, K - k_explore)

    sorted_by_cost = sorted(beam, key=lambda c: c.cost)
    exploit_lane = sorted_by_cost[:k_exploit] if k_exploit > 0 else []

    allow_uphill = bool(getattr(settings, "allow_uphill", True))
    uphill_keep = float(getattr(settings, "uphill_keep_fraction", 0.15))
    uphill_cap = int(round(max(0.0, min(1.0, uphill_keep)) * K))

    def _progress_key(c: Candidate):
        telem = getattr(c, "telemetry", {}) or {}
        soft_iou = float(telem.get("soft_iou", 0.0))
        com_err = float(telem.get("com_err", 1.0))
        delta = float(getattr(c, "cost_delta", 0.0))
        prog_len = len(getattr(c.program, "steps", [])) if getattr(c, "program", None) else 0
        return (-soft_iou, com_err, delta, c.cost, prog_len)

    explore_pool: List[Candidate] = []
    exploit_set = set(id(c) for c in exploit_lane)
    for cand in beam:
        if id(cand) in exploit_set:
            continue
        if not allow_uphill and cand.cost > best_cost + 1e-9:
            continue
        explore_pool.append(cand)

    explore_pool.sort(key=_progress_key)
    explore_lane = explore_pool[:k_explore] if k_explore > 0 else []

    if allow_uphill and explore_lane:
        uphill = [c for c in explore_lane if c.cost > best_cost + 1e-9]
        if uphill_cap >= 0 and len(uphill) > uphill_cap:
            uphill = uphill[:uphill_cap]
            explore_lane = [c for c in explore_lane if c.cost <= best_cost + 1e-9] + uphill

    def _sig(c: Candidate):
        prog = getattr(c, "program", None)
        if prog is not None:
            try:
                return prog.to_tuple()
            except Exception:
                return id(c)
        return id(c)

    merged: List[Candidate] = []
    seen: Set[Any] = set()
    for cand in exploit_lane + explore_lane:
        key = _sig(cand)
        if key in seen:
            continue
        seen.add(key)
        merged.append(cand)
        if len(merged) >= K:
            break

    if len(merged) < K:
        for cand in sorted_by_cost:
            key = _sig(cand)
            if key in seen:
                continue
            seen.add(key)
            merged.append(cand)
            if len(merged) >= K:
                break

    return merged


def _centroid_nonzero(a):
    import numpy as np
    ys, xs = np.where(a != 0)
    if ys.size == 0: return None
    return int(np.round(ys.mean())), int(np.round(xs.mean()))


def _quick_shape_candidates(input_grid, target_grid):
    """
    Try a small set of single-step shape transforms; return up to top-3 seeds
    (Program, acc) ranked by pixel accuracy (shape must match).
    Targets sparse tilings like border/cross and basic phase tilings.
    """
    H, W = input_grid.shape
    Ho, Wo = target_grid.shape
    cand = [Program([Step("resize", (Ho, Wo))])]

    if H and W and (Ho % H == 0) and (Wo % W == 0):
        ky, kx = Ho // H, Wo // W
        cand += [
            Program([Step("tile", (ky, kx))]),
            Program([Step("tile_masked", (ky, kx, 0))]),  # cross
            Program([Step("tile_masked", (ky, kx, 1))]),  # border
            Program([Step("phase_tile_row", (ky, kx, 0))]),
            Program([Step("phase_tile_row", (ky, kx, 1))]),
            Program([Step("phase_tile_row", (ky, kx, 2))]),
            Program([Step("phase_tile_col", (ky, kx, 0))]),
            Program([Step("phase_tile_col", (ky, kx, 1))]),
            Program([Step("phase_tile_col", (ky, kx, 2))]),
            Program([Step("phase_tile",     (ky, kx, 0))]),
            Program([Step("phase_tile",     (ky, kx, 1))]),
            Program([Step("phase_tile",     (ky, kx, 2))]),
        ]

    scored, seen = [], set()
    for prog in cand:
        try:
            pred = interpret_program(prog, input_grid)
        except Exception:
            continue
        if pred.shape != target_grid.shape:
            continue
        acc = (pred == target_grid).mean()
        sig = prog.to_tuple()
        if sig not in seen:
            scored.append((float(acc), prog)); seen.add(sig)
    scored.sort(key=lambda t: (-t[0], len(t[1])))
    return scored[:3]


def _consensus_one_step_candidate(train_pairs):
    """
    Quick shape-consensus sweep across train pairs.
    Returns (best_prog_or_None, best_mean_acc) for a small set of one-step programs
    evaluated on ALL train pairs. Only shape-matched predictions are scored.
    """
    try:
        x0, y0 = train_pairs[0]
        H, W = x0.shape
        Ho, Wo = y0.shape
    except Exception:
        return None, 0.0

    cands = [Program([Step("resize", (Ho, Wo))])]

    if H and W and (Ho % H == 0) and (Wo % W == 0):
        ky, kx = Ho // H, Wo // W
        cands += [
            Program([Step("tile",        (ky, kx))]),
            Program([Step("tile_masked", (ky, kx, 0))]),  # cross
            Program([Step("tile_masked", (ky, kx, 1))]),  # border
            Program([Step("phase_tile_row", (ky, kx, 0))]),
            Program([Step("phase_tile_row", (ky, kx, 1))]),
            Program([Step("phase_tile_row", (ky, kx, 2))]),
            Program([Step("phase_tile_col", (ky, kx, 0))]),
            Program([Step("phase_tile_col", (ky, kx, 1))]),
            Program([Step("phase_tile_col", (ky, kx, 2))]),
            Program([Step("phase_tile",     (ky, kx, 0))]),
            Program([Step("phase_tile",     (ky, kx, 1))]),
            Program([Step("phase_tile",     (ky, kx, 2))]),
        ]

    # Dedup by signature
    uniq, seen = [], set()
    for p in cands:
        sig = p.to_tuple()
        if sig not in seen:
            uniq.append(p); seen.add(sig)

    # Score each candidate on all train pairs (pixel acc if shapes match, else 0)
    best_prog, best_mean = None, 0.0
    for prog in uniq:
        accs = []
        for (xi, yi) in train_pairs:
            try:
                pred = interpret_program(prog, xi)
            except Exception:
                accs.append(0.0); continue
            if pred.shape != yi.shape:
                accs.append(0.0)
            else:
                accs.append(float((pred == yi).mean()))
        if accs:
            mean_acc = float(sum(accs) / len(accs))
            if mean_acc > best_mean:
                best_mean, best_prog = mean_acc, prog

    return best_prog, best_mean


def build_synth_context(input_grid, target_grid):
    import numpy as np
    Hin, Win = input_grid.shape
    Hout, Wout = target_grid.shape
    pal_in = sorted(np.unique(input_grid).tolist())
    pal_out = sorted(np.unique(target_grid).tolist())
    return {
        "in_shape": (Hin, Win),
        "out_shape": (Hout, Wout),
        "palette_in": pal_in,
        "palette_out": pal_out,
        "centroid_in": _centroid_nonzero(input_grid),
        "centroid_out": _centroid_nonzero(target_grid),
    }


def _int_space_for_base(op_name, idx, ctx):
    # idx: index among non-grid integer params
    if op_name == "rot90":
        return [1, 2, 3]
    if op_name == "mirror":
        return [0, 1]  # axis: 0=flipud, 1=fliplr
    if op_name in ("fill", "mask", "outline", "threshold"):
        vals = (ctx.get("palette_out") or []) + (ctx.get("palette_in") or [])
        vals = [v for v in dict.fromkeys(vals) if 0 <= v <= 9]
        return vals or list(range(10))
    if op_name == "resize":
        Hout, Wout = ctx["out_shape"]
        return [(Hout, Wout)]
    if op_name == "tile":
        Hin, Win = ctx["in_shape"]; Hout, Wout = ctx["out_shape"]
        pairs = []
        if Hin and Win and Hout % Hin == 0 and Wout % Win == 0:
            ky, kx = Hout // Hin, Wout // Win
            # ✅ Hard-ban identity tiling
            if ky > 1 or kx > 1:
                pairs.append((ky, kx))
        # Keep a single useful fallback (not identity)
        if not pairs:
            pairs = [(2, 2), (3, 3)]  # choose one or both; neither is (1,1)
        return pairs
    if op_name == "tile_masked":
        Hin, Win = ctx["in_shape"]; Hout, Wout = ctx["out_shape"]
        triples = []
        if Hin and Win and (Hout % Hin == 0) and (Wout % Win == 0):
            ky, kx = Hout // Hin, Wout // Win
            # never enumerate identity
            if not (ky == 1 and kx == 1):
                # v1: only cross(0) and border(1) to keep search tight
                for mode in (0, 1):
                    triples.append((ky, kx, mode))
                # leave diag modes out of enumeration for now
        return triples or []
    if op_name in ("phase_tile", "phase_tile_row", "phase_tile_col"):
        # Return (ky, kx, mode) triples
        Hin, Win = ctx["in_shape"]; Hout, Wout = ctx["out_shape"]
        triples = []
        if Hin and Win and Hout % Hin == 0 and Wout % Win == 0:
            ky, kx = Hout // Hin, Wout // Win
            if not (ky == 1 and kx == 1):
                # Try all three modes
                triples.extend([(ky, kx, 0), (ky, kx, 1), (ky, kx, 2)])
        if not triples:
            # Fallback
            triples = [(2, 2, 0), (3, 3, 0)]
        return triples
    if op_name == "shift":
        base = list(range(-3, 4))
        dy = dx = None
        ci, co = ctx.get("centroid_in"), ctx.get("centroid_out")
        if ci is not None and co is not None:
            dy = int(co[0] - ci[0]); dx = int(co[1] - ci[1])
        if idx == 0:
            return ([dy] + [v for v in base if v != dy]) if dy is not None else base
        if idx == 1:
            return ([dx] + [v for v in base if v != dx]) if dx is not None else base
        return base
    if op_name in ("replacecolor", "swapcolors"):
        pals = (ctx.get("palette_in") or []) + (ctx.get("palette_out") or [])
        pals = [c for c in dict.fromkeys(pals) if 0 <= c <= 9][:6]
        pairs = []
        for i, a in enumerate(pals):
            for b in pals[i+1:]:
                pairs.append((a, b))
        return pairs or [(1, 2), (2, 3), (3, 4)]
    if op_name in ("fill_holes", "keep_rings"):
        return [()]
    if op_name == "remove_isolated":
        return [(1,), (2,), (3,)]
    if op_name == "keep_n_largest":
        return [(1,), (2,), (3,)]
    if op_name == "keep_size_range":
        Hout, Wout = ctx["out_shape"]
        A = Hout * Wout if Hout and Wout else 0
        small  = max(1, A // 100)   # ~1%
        medium = max(2, A // 40)    # ~2.5%
        big    = max(3, A // 20)    # ~5%
        return [(small, medium), (medium, big)]
    return list(range(10))


def _ctx_sig(ctx):
    depth = getattr(ctx, "depth", None)
    block_identity = getattr(ctx, "block_identity", True)
    rails_scale_hard = getattr(ctx, "rails_scale_hard", True)
    scale_hard_steps = getattr(ctx, "scale_hard_steps", 1)
    in_shape = ()
    out_shape = ()
    palette_in = ()
    palette_out = ()
    centroid_in = ()
    centroid_out = ()
    if isinstance(ctx, dict):
        depth = ctx.get("depth", depth)
        block_identity = ctx.get("block_identity", block_identity)
        rails_scale_hard = ctx.get("rails_scale_hard", rails_scale_hard)
        scale_hard_steps = ctx.get("scale_hard_steps", scale_hard_steps)
        in_shape = tuple(ctx.get("in_shape") or ())
        out_shape = tuple(ctx.get("out_shape") or ())
        palette_in = tuple(ctx.get("palette_in") or ())
        palette_out = tuple(ctx.get("palette_out") or ())
        centroid_in = tuple(ctx.get("centroid_in") or ())
        centroid_out = tuple(ctx.get("centroid_out") or ())
    else:
        in_shape = tuple(getattr(ctx, "in_shape", ()) or ())
        out_shape = tuple(getattr(ctx, "out_shape", ()) or ())
        palette_in = tuple(getattr(ctx, "palette_in", ()) or ())
        palette_out = tuple(getattr(ctx, "palette_out", ()) or ())
        centroid_in = tuple(getattr(ctx, "centroid_in", ()) or ())
        centroid_out = tuple(getattr(ctx, "centroid_out", ()) or ())
    depth_val = None if depth is None else int(depth)
    try:
        scale_steps = int(scale_hard_steps)
    except Exception:
        scale_steps = 1
    return (
        depth_val,
        bool(block_identity),
        bool(rails_scale_hard),
        scale_steps,
        in_shape,
        out_shape,
        palette_in,
        palette_out,
        centroid_in,
        centroid_out,
    )


def _ctx_from_sig(sig):
    (
        _depth,
        block_identity,
        rails_scale_hard,
        scale_hard_steps,
        in_shape,
        out_shape,
        palette_in,
        palette_out,
        centroid_in,
        centroid_out,
    ) = sig
    ctx = {
        "in_shape": tuple(in_shape) if in_shape else (),
        "out_shape": tuple(out_shape) if out_shape else (),
        "palette_in": list(palette_in) if palette_in else [],
        "palette_out": list(palette_out) if palette_out else [],
        "centroid_in": tuple(centroid_in) if centroid_in else None,
        "centroid_out": tuple(centroid_out) if centroid_out else None,
    }
    # propagate policy flags when available
    ctx["block_identity"] = bool(block_identity)
    ctx["rails_scale_hard"] = bool(rails_scale_hard)
    ctx["scale_hard_steps"] = scale_hard_steps
    return ctx


@lru_cache(maxsize=None)
def _int_space_for_cached(op_name, idx, ctx_sig):
    if op_name == "stack":
        return (0, 1)
    if op_name == "scale":
        return (2, 3)
    ctx = _ctx_from_sig(ctx_sig)
    return tuple(_int_space_for_base(op_name, idx, ctx))


def _int_space_for(op_name, idx, ctx):
    ctx_sig = _ctx_sig(ctx)
    result = _int_space_for_cached(op_name, idx, ctx_sig)
    return list(result)


def beam_search_one_pair(
    input_grid: np.ndarray,
    target_grid: np.ndarray,
    phi: np.ndarray,
    settings: _InternalSearchSettings,
    logger: Optional[Any] = None,
    deadline: Optional[_Deadline] = None
) -> Optional[Program]:
    """
    OCO-guided beam search for a single train pair.
    
    Returns the best program found, or None if time/depth exceeded.
    """
    if deadline is None:
        max_secs = getattr(settings, "max_seconds", 0.0)
        if max_secs is None:
            max_secs = 0.0
        deadline = _Deadline(max_secs)
    start_time = _now()
    policy_prior = getattr(settings, "_policy_prior", _DEFAULT_POLICY_PRIOR) or _DEFAULT_POLICY_PRIOR
    trace_buffer = getattr(settings, "_trace_buffer", None) if getattr(settings, "_trace_ops", False) else None
    family_hint = phi_to_family(phi)
    divs_hint = _divisible_shape(input_grid.shape, target_grid.shape)
    monitor = getattr(settings, "_monitor", None)

    # Early-abort tracking (no-shape)
    no_shape_seen = True           # flip to False once any successor matches target shape
    base_seconds = settings.max_seconds

    # Early-abort tracking (no-improvement)
    last_improve_t = _now()
    best_cost_seen = 1e9

    # Build context for op-specific argument generation
    ctx = build_synth_context(input_grid, target_grid)
    if monitor is not None:
        try:
            ctx["monitor"] = monitor
        except Exception:
            pass

    # === Meta-seed discovery ===
    meta = _quick_shape_candidates(input_grid, target_grid)
    seeds = [prog for (acc, prog) in meta]
    memory_seed_tokens = list(getattr(settings, "_memory_seed_tokens", []) or [])
    memory_seeds = list(getattr(settings, "_memory_seeded_programs", []) or [])
    if memory_seeds:
        seeds.extend(memory_seeds)

    # --- φ-aware topo gating for seeds / refiners ---
    dholes_g = dcomps_g = dholes_pc = dcomps_pc = 0.0
    # v2.8.7: read topology from side-channel hint (works for 8-D φ)
    topo_hint = getattr(settings, "_topo_hint", None)
    if topo_hint and len(topo_hint) == 4:
        dholes_g, dcomps_g, dholes_pc, dcomps_pc = map(float, topo_hint)

    try:
        if dholes_g > 0.0 or dholes_pc > 0.0:
            seeds.insert(0, Program([Step("keep_rings", ())]))
        if dholes_g < 0.0 or dholes_pc < 0.0:
            seeds.append(Program([Step("fill_holes", ())]))
    except Exception:
        pass

    # if shape is identical sizes and accuracy is high-ish, try denoising early
    try:
        if target_grid.size == input_grid.size:
            seeds.append(Program([Step("remove_isolated", (1,))]))
            seeds.append(Program([Step("remove_isolated", (2,))]))
    except Exception:
        pass

    # Early-latch: if any seed already strong, prefer it
    best_so_far, best_cost = None, 1e9
    for acc, prog in meta:
        if acc >= 0.80:  # strong shape match
            # try palette immediately; if exact, return
            mapping = _palette_map_from_train_pairs([(input_grid, target_grid)])
            if mapping:
                pred = interpret_program(prog, input_grid)
                p2 = np.array(_apply_palette_map_ll(pred.tolist(), mapping), dtype=np.int32)
                if p2.shape == target_grid.shape and np.array_equal(p2, target_grid):
                    return prog
            # not exact: make the beam expand this branch first
            best_so_far = prog
            best_cost   = 1.0 - acc
            break  # one is enough

    # Initialize beam with scored seeds (plus an empty program fallback)
    beam: List[Any] = []
    visited: Set[Tuple] = set()

    seed_count = _enqueue_seed_programs(
        frontier=beam,
        input_grid=input_grid,
        target_grid=target_grid,
        seed_tokens_list=memory_seed_tokens,
        interpret_program_fn=interpret_program,
        compute_cost_fn=lambda prog, ig, tg: compute_cost(prog, ig, tg, phi, settings),
        step_cls=Step,
        prog_cls=Program,
        telem_carrier=settings,
    )

    if seed_count:
        try:
            print(f"[memory] seeded {seed_count} motif program(s) into frontier")
        except Exception:
            pass

    sig_seen: Set[Tuple] = set()
    seeded_entries: List[Any] = beam[-seed_count:] if seed_count else []
    for entry in seeded_entries:
        prog_obj = None
        if hasattr(entry, "program"):
            prog_obj = getattr(entry, "program", None)
        elif isinstance(entry, dict):
            prog_obj = entry.get("program")
        if prog_obj is not None:
            try:
                sig = prog_obj.to_tuple()
                sig_seen.add(sig)
                visited.add(sig)
            except Exception:
                pass

    def _seed_cost(p):
        try:
            return compute_cost(p, input_grid, target_grid, phi, settings)
        except Exception:
            return 1e9

    # add deduped seeds
    for prog in seeds:
        sig = prog.to_tuple()
        if sig in sig_seen:
            continue
        sig_seen.add(sig)
        cost_val = _seed_cost(prog)
        stats = getattr(prog, "_cost_stats", {}) or {}
        beam.append(
            Candidate(
                prog,
                cost_val,
                depth=len(prog),
                telemetry=dict(stats),
                cost_delta=0.0,
            )
        )
        visited.add(sig)

    # always include empty program fallback
    empty_sig = Program([]).to_tuple()
    if empty_sig not in sig_seen:
        beam.append(Candidate(Program([]), cost=1e9, depth=0, telemetry={}, cost_delta=0.0))
        visited.add(empty_sig)

    # if early-latch found a strong seed, bias its cost so it's explored first
    if best_so_far is not None:
        sig = best_so_far.to_tuple()
        if sig not in sig_seen:
            stats = getattr(best_so_far, "_cost_stats", {}) or {}
            beam.append(
                Candidate(
                    best_so_far,
                    best_cost,
                    depth=len(best_so_far),
                    telemetry=dict(stats),
                    cost_delta=0.0,
                )
            )
            visited.add(sig)

    # prune to beam width right away
    beam.sort()
    beam = beam[:settings.beam_width]

    # Track best solution found
    if best_so_far is None:
        best_so_far = None
        best_cost = 1e9

    # State cache for in-beam gating
    state_cache: Dict[Tuple, np.ndarray] = {}
    def _run(prog: Program) -> np.ndarray:
        sig = prog.to_tuple()
        if sig in state_cache:
            return state_cache[sig]
        out = interpret_program(prog, input_grid)
        state_cache[sig] = out
        return out

    iteration = 0

    while beam:
        iteration += 1

        if iteration >= 2 and not getattr(settings, "_hfp_ready", False):
            settings._hfp_ready = True

        stall_window, abort_after = _adjust_abort_windows(settings, base_seconds, not no_shape_seen)
        if deadline is not None:
            time_left = max(0.0, deadline.time_left())
            stall_window = min(stall_window, time_left)
            abort_after = min(abort_after, time_left)
        rho_raw = getattr(settings, "_hfp_rho", None)
        rho_s = getattr(settings, "_hfp_rho_smoothed", None)
        if rho_raw is not None:
            rho_s = 0.5 * (rho_s if rho_s is not None else rho_raw) + 0.5 * rho_raw
            setattr(settings, "_hfp_rho_smoothed", rho_s)
        ready = bool(getattr(settings, "_hfp_ready", False))
        sustained = bool(getattr(settings, "_hfp_sustained", False))
        max_secs = getattr(settings, "max_seconds", base_seconds)
        if sustained and max_secs:
            stall_window = max(stall_window, max_secs * 0.80)
            abort_after = max(abort_after, max_secs * 0.80)
        elif ready and rho_s is not None and rho_s < 0.70 and max_secs and (_now() - start_time) > max_secs / 3.0:
            stall_window = min(stall_window, max_secs * 0.20)
            abort_after = min(abort_after, max_secs * 0.20)

        # Check timeout
        if deadline is not None and deadline.expired():
            setattr(settings, "_deadline_hit", True)
            if logger:
                logger.log_iteration(iteration, len(beam), best_cost)
            if best_so_far is not None:
                return best_so_far
            return Program([Step("resize", (target_grid.shape[0], target_grid.shape[1]))])

        # No-improvement early-abort check
        if _now() - last_improve_t > stall_window:
            if logger:
                print(f"[early-abort] No cost improvement for {stall_window:.1f}s, returning best")
            # Return best-so-far or resize fallback
            if best_so_far is not None:
                return best_so_far
            # Safe fallback: resize to target shape
            return Program([Step("resize", (target_grid.shape[0], target_grid.shape[1]))])

        # No-shape early-abort check: if ⅓ budget spent with no shape progress
        if _now() - start_time > abort_after and no_shape_seen:
            if logger:
                print(f"[early-abort] No shape match after {abort_after:.1f}s, returning fallback")
            # Return best-so-far or resize fallback
            if best_so_far is not None:
                return best_so_far
            # Safe fallback: resize to target shape
            return Program([Step("resize", (target_grid.shape[0], target_grid.shape[1]))])

        # Get candidate with lowest cost
        current = beam.pop(0)
        if monitor is not None:
            try:
                monitor.record_accept(current.program)
            except Exception:
                pass

        # Track improvement for no-improvement abort
        if current.cost < best_cost_seen - 1e-6:
            best_cost_seen = current.cost
            last_improve_t = _now()

        # Check if solved
        if current.cost < 0.01:
            best_so_far = current.program
            best_cost = current.cost
            if logger:
                logger.log_iteration(iteration, len(beam), best_cost)
            break

        # Track best
        if current.cost < best_cost:
            best_cost = current.cost
            best_so_far = current.program

        # Check depth limit
        if current.depth >= settings.max_depth:
            continue

        # Compute current state for gating
        try:
            cur_state = _run(current.program)
            # Check if we've seen shape match at this level
            if cur_state.shape == target_grid.shape:
                no_shape_seen = False
        except Exception:
            cur_state = input_grid

        # --- v2.8.3 Topology-aware refiner: keep_rings post-alignment ---
        dholes_g = dcomps_g = dholes_pc = dcomps_pc = 0.0
        # v2.8.7: read topology from side-channel hint (works for 8-D φ)
        topo_hint = getattr(settings, "_topo_hint", None)
        if topo_hint and len(topo_hint) == 4:
            dholes_g, dcomps_g, dholes_pc, dcomps_pc = map(float, topo_hint)

        acc_proxy = 0.0
        try:
            if cur_state.shape == target_grid.shape:
                acc_proxy = float((cur_state == target_grid).mean())
        except Exception:
            acc_proxy = 0.0

        if (
            cur_state.shape == target_grid.shape
            and (dholes_g > 0.0 or dholes_pc > 0.0)
            and acc_proxy >= 0.55
            and current.depth < 3
        ):
            try:
                ring_prog = current.program.copy()
                ring_prog.steps.append(Step("keep_rings", ()))
                sig = ring_prog.to_tuple()
                if sig not in visited:
                    cost_ring = compute_cost(ring_prog, input_grid, target_grid, phi, settings)
                    stats_ring = getattr(ring_prog, "_cost_stats", {}) or {}
                    beam.append(
                        Candidate(
                            ring_prog,
                            cost_ring,
                            current.depth + 1,
                            telemetry=dict(stats_ring),
                            cost_delta=cost_ring - current.cost,
                        )
                    )
                    visited.add(sig)
            except Exception:
                pass
        # --- end v2.8.3 insert ---

        # --- v2.8.4 Topology-aware refiner: fill_holes post-alignment ---
        if (
            cur_state.shape == target_grid.shape
            and (dholes_g < 0.0 or dholes_pc < 0.0)
            and acc_proxy >= 0.55
            and current.depth < 3
        ):
            try:
                fill_prog = current.program.copy()
                fill_prog.steps.append(Step("fill_holes", ()))
                sig = fill_prog.to_tuple()
                if sig not in visited:
                    cost_fill = compute_cost(fill_prog, input_grid, target_grid, phi, settings)
                    stats_fill = getattr(fill_prog, "_cost_stats", {}) or {}
                    beam.append(
                        Candidate(
                            fill_prog,
                            cost_fill,
                            current.depth + 1,
                            telemetry=dict(stats_fill),
                            cost_delta=cost_fill - current.cost,
                        )
                    )
                    visited.add(sig)
            except Exception:
                pass
        # --- end v2.8.4 insert ---

        # --- Conditional Smart Refiner (one extra post-shape step) ---
        try:
            if cur_state.shape == target_grid.shape and current.depth < 2:
                acc, _, _ = _current_acc_state(cur_state, target_grid)
                if acc >= 0.60:
                    # 1) alignment-guided shift (best (dy,dx) from existing helper)
                    try:
                        best = _propose_alignment_deltas(cur_state, target_grid, window=3)[:1]
                    except Exception:
                        best = []
                    for (dy, dx) in best:
                        if dy or dx:
                            prog_shift = current.program.copy()
                            prog_shift.steps.append(Step("shift", (int(dy), int(dx))))
                            sig = prog_shift.to_tuple()
                            if sig not in visited:
                                cost = compute_cost(prog_shift, input_grid, target_grid, phi, settings)
                                stats_shift = getattr(prog_shift, "_cost_stats", {}) or {}
                                beam.append(
                                    Candidate(
                                        prog_shift,
                                        cost,
                                        current.depth + 1,
                                        telemetry=dict(stats_shift),
                                        cost_delta=cost - current.cost,
                                    )
                                )
                                visited.add(sig)

                    # 2) alternate tile_masked mode (swap cross/border once)
                    if current.program.steps and current.program.steps[-1].op == "tile_masked":
                        ky, kx, m = map(int, current.program.steps[-1].args)
                        if m in (0, 1):
                            alt = 1 - m
                            prog_alt = current.program.copy()
                            prog_alt.steps.append(Step("tile_masked", (ky, kx, alt)))
                            sig = prog_alt.to_tuple()
                            if sig not in visited:
                                cost = compute_cost(prog_alt, input_grid, target_grid, phi, settings)
                                stats_alt = getattr(prog_alt, "_cost_stats", {}) or {}
                                beam.append(
                                    Candidate(
                                        prog_alt,
                                        cost,
                                        current.depth + 1,
                                        telemetry=dict(stats_alt),
                                        cost_delta=cost - current.cost,
                                    )
                                )
                                visited.add(sig)

                    # 3) optional mirror when fairly close
                    if acc >= 0.70 and current.program.steps:
                        axis = 1  # horizontal mirror default
                        prog_m = current.program.copy()
                        prog_m.steps.append(Step("mirror", (axis,)))
                        sig = prog_m.to_tuple()
                        if sig not in visited:
                            cost = compute_cost(prog_m, input_grid, target_grid, phi, settings)
                            stats_m = getattr(prog_m, "_cost_stats", {}) or {}
                            beam.append(
                                Candidate(
                                    prog_m,
                                    cost,
                                    current.depth + 1,
                                    telemetry=dict(stats_m),
                                    cost_delta=cost - current.cost,
                                )
                            )
                            visited.add(sig)

                    # NOTE: we rely on beam pruning to cap to beam_width; refiners add ≤3 branches
        except Exception:
            pass

        # Choose allowed ops based on shape match
        if cur_state.shape == target_grid.shape:
            allowed = COLOR_OPS
        else:
            allowed = SHAPE_OPS

        ctx_iter = dict(ctx)
        try:
            ctx_iter["phi"] = phi
            ctx_iter["in_shape"] = input_grid.shape
            ctx_iter["out_shape"] = target_grid.shape
            ctx_iter["has_shape_match"] = bool(cur_state.shape == target_grid.shape)
            ctx_iter["depth"] = len(current.program.steps)
            ctx_iter["early_palette_block_steps"] = getattr(settings, "early_palette_block_steps", 3)
            ctx_iter["monitor"] = monitor
        except Exception:
            pass

        # Generate successors with gating + O2 ordering
        successors = generate_successors(current.program, ctx_iter, allowed=allowed)
        if monitor is not None and monitor.on:
            successors = monitor.prune_dupes(successors)
            successors = monitor.inject_underused_dimension(current, successors)
            successors = monitor.gate_by_health(successors)
        # --- GOF-9000: early opener hygiene -----------------------------------------
        try:
            depth_here = len(current.program.steps)
        except Exception:
            depth_here = 0

        if depth_here == 0:
            # --- GOF-9000: ban weak opener keep_rings at depth-0 ---------------------
            filtered_successors = []
            for sp in successors:
                nm, args = _step_name_and_args(sp)
                if nm == "keep_rings":
                    continue  # too weak as an opener
                if _is_shift_name(nm) and len(args) >= 2:
                    try:
                        dy = int(args[0])
                        dx = int(args[1])
                    except Exception:
                        dy = dx = None
                    if dy == 0 and dx == 0:
                        continue  # drop shift(0,0) openers
                filtered_successors.append(sp)
            successors = filtered_successors

            # Inject robust object-centric openers at depth-0 if missing
            try:
                StepCls = Step
                ProgramCls = Program
            except Exception:
                StepCls = ProgramCls = None
            if StepCls and ProgramCls:
                have = {_last_step_name(sp) for sp in successors}
                candidates = [
                    ("keep_n_largest", (1,)),
                    ("remove_isolated", (1,)),
                    ("fill_holes", ()),
                    ("largest", ()),
                ]
                for oname, oargs in candidates:
                    if oname in have or oname not in OP_REGISTRY:
                        continue
                    sp = _append_step_clone(current.program, StepCls, ProgramCls, oname, oargs)
                    if sp is not None:
                        successors.append(sp)

        if depth_here < 2 and _is_shift_name(_last_step_name(current.program)):
            filtered_successors = []
            for sp in successors:
                nm, _ = _step_name_and_args(sp)
                if not _is_shift_name(nm):
                    filtered_successors.append(sp)
            successors = filtered_successors
        # --- end early opener hygiene -----------------------------------------------

        # === GOF-9000: rails + identity policy ===
        try:
            depth = len(current.program.steps)
        except Exception:
            depth = 0

        gated_names = set(allowed) if allowed else set(OP_REGISTRY.keys())
        try:
            k_palette = int(getattr(settings, "early_palette_block_steps", 3))
            k_scale = int(getattr(settings, "scale_hard_steps", 3))
        except Exception:
            k_palette, k_scale = 3, 3

        if depth < k_palette:
            gated_names -= (PALETTE_OPS & gated_names)

        phi_ctx = ctx_iter.get("phi") if isinstance(ctx_iter, dict) else None
        in_shape = ctx_iter.get("in_shape") if isinstance(ctx_iter, dict) else None
        out_shape = ctx_iter.get("out_shape") if isinstance(ctx_iter, dict) else None

        scale_sign = 0.0
        try:
            if isinstance(phi_ctx, (list, tuple)) and len(phi_ctx) > 0:
                scale_sign = float(phi_ctx[0])
            elif in_shape and out_shape:
                ia = int(in_shape[0]) * int(in_shape[1])
                oa = int(out_shape[0]) * int(out_shape[1])
                if oa < 0.9 * ia:
                    scale_sign = -1.0
                elif oa > 1.1 * ia:
                    scale_sign = +1.0
        except Exception:
            scale_sign = 0.0

        def _resize_is_shrink(args):
            try:
                if not in_shape or len(args) < 2:
                    return False
                h, w = int(args[0]), int(args[1])
                return (h * w) < (int(in_shape[0]) * int(in_shape[1]))
            except Exception:
                return False

        def _scale_is_shrink(args):
            try:
                return float(args[0]) < 1.0
            except Exception:
                return False

        if depth < k_scale:
            next_names = set()
            for nm in gated_names:
                if scale_sign <= -1.0:
                    if nm in TOPOLOGY_OPS or nm in ALIGNMENT_OPS or nm in {"resize", "scale"}:
                        next_names.add(nm)
                elif scale_sign >= 1.0:
                    if nm in (EXPANSION_OPS | ALIGNMENT_OPS) or nm in {"resize", "scale"}:
                        next_names.add(nm)
                else:
                    next_names.add(nm)
            if next_names:
                gated_names = next_names

        filtered = []
        block_identity = bool(getattr(settings, "block_identity", True)) if settings else True

        for sp in list(successors):
            lop = _last_step_op(sp)
            largs = _last_step_args(sp)
            if gated_names and lop not in gated_names:
                continue
            if depth < k_scale and scale_sign <= -1.0:
                if lop == "resize" and not _resize_is_shrink(largs):
                    continue
                if lop == "scale" and not _scale_is_shrink(largs):
                    continue
            if depth < k_scale and scale_sign >= 1.0:
                if lop == "resize" and _resize_is_shrink(largs):
                    continue
                if lop == "scale" and _scale_is_shrink(largs):
                    continue
            if block_identity and _should_block_identity(lop, largs, in_shape, out_shape, depth):
                continue
            filtered.append(sp)

        successors = filtered
        try:
            _telemetry_note(
                current,
                rails_scale_sign=float(scale_sign),
                rail_depth=int(depth),
                rail_allowed=list(sorted(gated_names))[:8],
                identity_policy=IDENTITY_POLICY,
            )
        except Exception:
            pass
        # === end rails + identity policy ===

        # Update GOF/HFP face and consciousness
        try:
            depth_for_face = len(current.program.steps)
        except Exception:
            depth_for_face = 0
        if monitor is not None:
            try:
                C_base, C_mod = monitor.gof_consciousness(program_depth=depth_for_face)
                new_face = monitor.determine_face(C_mod)
                if monitor.face_enabled:
                    if new_face != monitor.face:
                        monitor.face = new_face
                    monitor.face_log.append((len(monitor.face_log), new_face))
                    if len(monitor.face_log) > 64:
                        monitor.face_log = monitor.face_log[-64:]
                if 'controller' in locals() and hasattr(controller, "mode") and new_face != controller.mode:
                    controller.mode = new_face
                    if hasattr(controller, "rotation_count"):
                        controller.rotation_count = 0
            except Exception:
                pass

        # Score each successor
        for succ_prog in successors:
            if getattr(settings, "block_identity", True):
                tok = None
                try:
                    tok = succ_prog.steps[-1]
                except Exception:
                    tok = None
                name = getattr(tok, "op", None) or getattr(tok, "name", None)
                args = getattr(tok, "args", ()) if tok is not None else ()
                if isinstance(args, np.ndarray):
                    args = tuple(args.tolist())
                if name in ("tile", "phase_tile"):
                    args_seq = tuple(args[:2]) if isinstance(args, (list, tuple)) else ()
                    if args_seq == (1, 1):
                        continue
                if name == "shift":
                    if tuple(args) == (0, 0):
                        continue
                if name == "scale":
                    args_tuple = tuple(args) if isinstance(args, (list, tuple)) else (args,)
                    if len(args_tuple) == 1 and args_tuple[0] in (0, 1):
                        continue
                    if args_tuple in ((0, 0), (1, 1)):
                        continue
            # Skip if visited
            prog_sig = succ_prog.to_tuple()
            if prog_sig in visited:
                continue
            visited.add(prog_sig)

            op_name = succ_prog.steps[-1].op if succ_prog.steps else ""

            # Compute cost
            cost = compute_cost(succ_prog, input_grid, target_grid, phi, settings)
            if monitor is not None:
                try:
                    t_prog_val = float(compute_program_tension(succ_prog, phi))
                    monitor.tension_hist.append(t_prog_val)
                    monitor.update_hfp_from_tension(t_prog_val)
                except Exception:
                    pass

            # Check successor output for tie-break, early exit, and shape tracking
            pred = None
            try:
                pred = interpret_program(succ_prog, input_grid)

                # Track shape match
                if pred.shape == target_grid.shape:
                    no_shape_seen = False

                # Tiny tie-break for tile_masked when shape matches
                    if pred.shape == target_grid.shape:
                        last = succ_prog.steps[-1].op if succ_prog.steps else None
                        if last == "tile_masked":
                            cost -= SHAPE_MATCH_BONUS

                    # Early exit if exact match
                    if np.array_equal(pred, target_grid):
                        # Robust exact-win logging that can never block the early return
                        try:
                            # Resolve a best-effort task_id without raising
                            tid_val = (locals().get("tid")
                                       or locals().get("task_id")
                                       or getattr(settings, "task_id", None)
                                       or getattr(settings, "_current_task_id", None))
                            last_op = None
                            try:
                                if getattr(succ_prog, "steps", None):
                                    last_op = succ_prog.steps[-1].op
                            except Exception:
                                pass
                            pre_err = None
                            try:
                                parent_prog = current.program if hasattr(current, "program") else None
                                if parent_prog is not None:
                                    parent_pred = interpret_program(parent_prog, input_grid)
                                    if parent_pred.shape == target_grid.shape:
                                        pre_err = float((parent_pred != target_grid).mean())
                            except Exception:
                                pass
                            if "_log_exact_win" in globals():
                                _log_exact_win({
                                    "task_id": tid_val,
                                    "win_attributed_to": last_op,
                                    "pre_win_err": pre_err,
                                    "depth_at_win": (current.depth + 1) if hasattr(current, "depth") else None,
                                    "rho_at_win": getattr(settings, "_hfp_rho_smoothed", None),
                                    "attempt_tag": "A",  # adjust if you tag A/B separately in this function
                                })
                        except Exception:
                            # Never let logging failures block the solve
                            pass
                        # Always exit immediately on exact
                        return succ_prog
            except Exception:
                pass

            # Apply policy prior as an additive bonus in score space (subtract from cost)
            try:
                grid_shape = pred.shape if isinstance(pred, np.ndarray) else (None, None)
                ctx_prior = {
                    "phi": phi,
                    "grid": pred,
                    "grid_shape": grid_shape,
                    "divs": divs_hint if divs_hint is not None else (),
                    "family": family_hint,
                }
                prior_logit = policy_prior.op_logit(op_name, ctx_prior)
                cost -= 0.05 * prior_logit
            except Exception:
                pass

            if trace_buffer is not None and op_name:
                try:
                    delta_cost = float(cost - current.cost)
                except Exception:
                    delta_cost = float(cost)
                trace_buffer.append((op_name, delta_cost))

            # Add to beam
            stats_succ = getattr(succ_prog, "_cost_stats", {}) or {}
            new_cand = Candidate(
                succ_prog,
                cost,
                current.depth + 1,
                telemetry=dict(stats_succ),
                cost_delta=cost - current.cost,
            )
            beam.append(new_cand)

        parent_best_cost = min((cand.cost for cand in beam), default=current.cost)
        ref_cost = min(best_cost, parent_best_cost)
        beam = _two_lane_prune(beam, settings.beam_width, ref_cost, settings)
        beam.sort(key=lambda c: c.cost)

        if monitor is not None:
            try:
                if monitor.should_health_break():
                    monitor.events.append(("hfp_break", round(monitor.R_last, 3)))
                    break
            except Exception:
                pass

        # Periodic logging
        if logger and iteration % settings.log_every == 0:
            logger.log_iteration(iteration, len(beam), best_cost)

    if logger:
        logger.log_iteration(iteration, len(beam), best_cost)

    return best_so_far


def generate_successors(prog: Program, ctx, allowed: Optional[Set[str]] = None) -> List[Program]:
    """Generate all valid single-step extensions of a program, filtered & ordered."""
    # Order operations by OP_ORDER_O2, allowing memory bias overrides
    biased_names = globals().get("_OP_NAMES_BIASED")
    if biased_names:
        seen = set()
        ordered = []
        for name in biased_names:
            entry = _OP_ENTRY_BY_NAME.get(name)
            if entry and name not in seen:
                ordered.append(entry)
                seen.add(name)
        for entry in OP_NAMES_BASIC:
            if entry[0] not in seen:
                ordered.append(entry)
    else:
        ordered = sorted(OP_NAMES_BASIC, key=lambda t: _OP_RANK.get(t[0], 10_000))
    successors = []
    for op_name, arity, param_types in ordered:
        if allowed is not None and op_name not in allowed:
            continue
        arg_combos = enumerate_args(op_name, param_types, ctx)
        for args in arg_combos:
            new_prog = prog.copy()
            new_prog.steps.append(Step(op_name, args))
            successors.append(new_prog)
    return successors


def enumerate_args(op_name: str, param_types: List[str], ctx) -> List[Tuple[Any, ...]]:
    """Enumerate concrete arguments for an operation with context awareness."""
    if not param_types:
        return [()]

    # Convert param_types: first 'grid' is implicit (current state), 
    # subsequent 'grid' become 'grid_ref' for register references
    filtered = []
    seen_grid = False
    for t in param_types:
        if t == "grid":
            if not seen_grid:
                seen_grid = True
                continue  # Skip first grid (current state)
            filtered.append("grid_ref")  # Subsequent grids are references
        else:
            filtered.append(t)
    
    if not filtered:
        return [()]

    # Bundled int-pair/triple ops
    if op_name == "resize":
        return [(h, w) for (h, w) in _int_space_for("resize", 0, ctx)]
    if op_name == "tile":
        return [(v, h) for (v, h) in _int_space_for("tile", 0, ctx)]
    if op_name == "tile_masked":
        return [(ky, kx, mode) for (ky, kx, mode) in _int_space_for("tile_masked", 0, ctx)]
    if op_name == "phase_tile":
        return [(ky, kx, mode) for (ky, kx, mode) in _int_space_for("phase_tile", 0, ctx)]
    if op_name == "phase_tile_row":
        return [(ky, kx, mode) for (ky, kx, mode) in _int_space_for("phase_tile_row", 0, ctx)]
    if op_name == "phase_tile_col":
        return [(ky, kx, mode) for (ky, kx, mode) in _int_space_for("phase_tile_col", 0, ctx)]
    if op_name in ("replacecolor", "swapcolors"):
        return [(a, b) for (a, b) in _int_space_for(op_name, 0, ctx)]
    if op_name == "keep_n_largest":
        return [(n,) for (n,) in _int_space_for("keep_n_largest", 0, ctx)]
    if op_name == "keep_size_range":
        return [(amin, amax) for (amin, amax) in _int_space_for("keep_size_range", 0, ctx)]

    # Generic cartesian product across per-position spaces
    spaces = []
    for i, t in enumerate(filtered):
        if t == "grid_ref":
            spaces.append([REG_PREV, REG_PREV2])
        elif t == "int":
            spaces.append(_int_space_for(op_name, i, ctx))
        else:
            spaces.append([0])

    combos = [()]
    for space in spaces:
        combos = [c + (v,) for c in combos for v in space]

    depth_val = None
    if isinstance(ctx, dict):
        depth_val = ctx.get("depth", depth_val)
    else:
        depth_val = getattr(ctx, "depth", depth_val)

    if op_name == "scale":
        if depth_val == 0:
            return []
        filtered = []
        for args in combos:
            if not args:
                continue
            try:
                val = int(args[0])
            except Exception:
                continue
            if val in (2, 3):
                filtered.append(args)
        combos = filtered

    if op_name in {"tile", "phase_tile", "phase_tile_row", "phase_tile_col"}:
        combos = [
            args
            for args in combos
            if not (len(args) >= 2 and tuple(args[:2]) == (1, 1))
        ]

    if op_name == "tile_masked" and depth_val == 0:
        filtered = []
        for args in combos:
            if len(args) >= 2:
                try:
                    if int(args[0]) == 1 and int(args[1]) == 1:
                        continue
                except Exception:
                    pass
            filtered.append(args)
        combos = filtered

    if op_name == "shift":
        filtered = []
        for args in combos:
            if len(args) >= 2:
                try:
                    dy, dx = int(args[0]), int(args[1])
                except Exception:
                    filtered.append(args)
                    continue
                if dy == 0 and dx == 0:
                    continue
            filtered.append(args)
        combos = filtered

    if op_name == "stack":
        # --- stack enumeration fix & optional debug ---
        combos = [args for args in combos if len(args) >= 2 and int(args[1]) in (0, 1)]

        try:
            _dbg = os.getenv("ARC_DEBUG_STACK", "0") == "1"
        except Exception:
            _dbg = False

        if _dbg and not globals().get("_DEBUG_STACK_ONCE_FLAG__", False):
            globals()["_DEBUG_STACK_ONCE_FLAG__"] = True
            try:
                print("[debug] enumerate_args('stack') combos (sample):", combos[:8])
            except Exception:
                pass

    return combos


# ============================================================================
# ============ controller/modes.py ============
# ============================================================================

@dataclass
class ControllerState:
    """State of the OCO controller."""
    mode: str = "observer"  # observer, navigator, explorer
    rotation_count: int = 0
    last_rotation_cost: float = 1e9


# ============================================================================
# ============ controller/meta.py (opt-in meta-controller) ============
# ============================================================================


class _UCB:
    def __init__(self, n_arms: int):
        self.n = [0] * n_arms
        self.r = [0.0] * n_arms
        self.t = 0

    def select(self) -> int:
        self.t += 1
        for i, c in enumerate(self.n):
            if c == 0:
                return i
        import math

        avg = [self.r[i] / self.n[i] for i in range(len(self.n))]
        bonus = [math.sqrt(2 * math.log(self.t) / self.n[i]) for i in range(len(self.n))]
        ucb = [avg[i] + bonus[i] for i in range(len(self.n))]
        return int(np.argmax(ucb))

    def update(self, arm: int, reward: float):
        self.n[arm] += 1
        self.r[arm] += reward


class MetaControllerUCB:
    """
    Per-φ-family UCB over a few fixed bundles (beam, λ1, λ2, K).
    Reward is provided by caller (e.g., acc/sec).
    """

    def __init__(self):
        # bundles are minimal & stable; adjust offline if needed
        self.bundle_map = {
            "scale": [
                {"beam_width": 48, "lambda1": 0.25, "lambda2": 0.10, "max_train_pairs_for_beam": 2},
                {"beam_width": 96, "lambda1": 0.30, "lambda2": 0.20, "max_train_pairs_for_beam": 2},
                {"beam_width": 128, "lambda1": 0.30, "lambda2": 0.25, "max_train_pairs_for_beam": 2},
            ],
            "objectness": [
                {"beam_width": 48, "lambda1": 0.35, "lambda2": 0.15, "max_train_pairs_for_beam": 2},
                {"beam_width": 128, "lambda1": 0.20, "lambda2": 0.10, "max_train_pairs_for_beam": 2},
            ],
            "palette": [
                {"beam_width": 64, "lambda1": 0.20, "lambda2": 0.15, "max_train_pairs_for_beam": 2},
                {"beam_width": 96, "lambda1": 0.10, "lambda2": 0.25, "max_train_pairs_for_beam": 2},
            ],
            "alignment": [
                {"beam_width": 64, "lambda1": 0.25, "lambda2": 0.30, "max_train_pairs_for_beam": 2},
                {"beam_width": 64, "lambda1": 0.30, "lambda2": 0.25, "max_train_pairs_for_beam": 2},
            ],
            "geometry": [
                {"beam_width": 48, "lambda1": 0.25, "lambda2": 0.15, "max_train_pairs_for_beam": 2},
                {"beam_width": 128, "lambda1": 0.20, "lambda2": 0.10, "max_train_pairs_for_beam": 2},
            ],
            "pattern": [
                {"beam_width": 128, "lambda1": 0.25, "lambda2": 0.25, "max_train_pairs_for_beam": 2},
                {"beam_width": 128, "lambda1": 0.20, "lambda2": 0.10, "max_train_pairs_for_beam": 2},
            ],
            "default": [
                {"beam_width": 64, "lambda1": 0.30, "lambda2": 0.20, "max_train_pairs_for_beam": 2},
            ],
        }
        self.bandits = {}

    def _bundles_for(self, fam: str):
        return self.bundle_map.get(fam, self.bundle_map["default"])

    def select(self, fam: str):
        bundles = self._bundles_for(fam)
        if fam not in self.bandits:
            self.bandits[fam] = _UCB(len(bundles))
        arm = self.bandits[fam].select()
        return arm, bundles[arm]

    def update(self, fam: str, arm: int, reward: float):
        self.bandits[fam].update(arm, reward)


def should_rotate(state: ControllerState, current_cost: float, settings: _InternalSearchSettings) -> bool:
    """Decide if controller should rotate to next mode."""
    if settings._disable_rotation:
        return False

    # Rotate if cost isn't improving
    if current_cost >= state.last_rotation_cost * 0.98:
        return True

    # Rotate after 3 attempts in same mode
    if state.rotation_count >= 3:
        return True

    return False


def rotate_mode(state: ControllerState) -> str:
    """Rotate to next controller mode."""
    modes = ["observer", "navigator", "explorer"]
    idx = modes.index(state.mode)
    next_idx = (idx + 1) % len(modes)
    return modes[next_idx]


def apply_mode_bias(settings: _InternalSearchSettings, mode: str) -> _InternalSearchSettings:
    """Adjust search settings based on controller mode."""
    from dataclasses import replace

    if mode == "observer":
        # Conservative: low beam, high OCO
        return replace(settings, beam_width=64, lambda1=0.40, lambda2=0.30)
    elif mode == "navigator":
        # Balanced: default settings
        return settings
    elif mode == "explorer":
        # Aggressive: high beam, low OCO
        return replace(settings, beam_width=384, lambda1=0.05, lambda2=0.05)
    else:
        return settings


# ============================================================================
# ============ io/tasks.py ============
# ============================================================================

def load_tasks_from_dir(tasks_dir: str) -> List[Tuple[str, dict]]:
    """
    Load ARC tasks from a directory.

    Supports:
      (1) dict-of-tasks: {"id": {"train":[...], "test":[...]}, ...}
      (2) single dict:   {"train":[...], "test":[...]}
      (3) list of dicts: [{"train":...,"test":...}, ...]
      (4) optional wrappers: {"challenges":[...]}, {"training":[...]}, {"evaluation":[...]}, {"test":[...]}
    """
    import glob, json, os
    tasks: List[Tuple[str, dict]] = []

    def normalize(obj, base_id):
        out = []
        # (1) dict-of-tasks
        if isinstance(obj, dict) and not ("train" in obj and "test" in obj):
            ok = False
            for k, v in obj.items():
                if isinstance(v, dict) and "train" in v and "test" in v:
                    out.append((k, {"train": v["train"], "test": v["test"]}))
                    ok = True
            if ok:
                return out
            # wrapped lists
            for bucket in ("challenges", "training", "evaluation", "test"):
                if bucket in obj and isinstance(obj[bucket], list):
                    for i, e in enumerate(obj[bucket]):
                        if isinstance(e, dict) and "train" in e and "test" in e:
                            tid = e.get("task_id") or e.get("id") or f"{base_id}_{bucket}_{i:05d}"
                            out.append((tid, {"train": e["train"], "test": e["test"]}))
                    return out

        # (2) single dict
        if isinstance(obj, dict) and "train" in obj and "test" in obj:
            out.append((base_id, {"train": obj["train"], "test": obj["test"]}))
            return out

        # (3) list of dicts
        if isinstance(obj, list) and obj and isinstance(obj[0], dict):
            for i, e in enumerate(obj):
                if "train" in e and "test" in e:
                    tid = e.get("task_id") or e.get("id") or f"{base_id}_{i:05d}"
                    out.append((tid, {"train": e["train"], "test": e["test"]}))
            return out

        return out

    def load_json(path):
        with open(path, "r") as f:
            return json.load(f)

    # Prefer ARC Prize consolidated files if present
    got_any = False
    for name in ["arc-agi_training_challenges.json",
                 "arc-agi_evaluation_challenges.json",
                 "arc-agi_test_challenges.json"]:
        fp = os.path.join(tasks_dir, name)
        if os.path.exists(fp):
            got_any = True
            obj = load_json(fp)
            tasks.extend(normalize(obj, os.path.splitext(name)[0]))

    # Fallback: any *.json
    if not tasks:
        for fp in sorted(glob.glob(os.path.join(tasks_dir, "*.json"))):
            got_any = True
            try:
                tasks.extend(normalize(load_json(fp), os.path.splitext(os.path.basename(fp))[0]))
            except Exception:
                continue

    if not got_any:
        raise RuntimeError(f"No ARC tasks found under {tasks_dir}.")
    if not tasks:
        raise RuntimeError(f"Found JSON under {tasks_dir} but no (train/test) tasks parsed. Schema mismatch.")
    return tasks


def trains_from_task(task_json: Dict) -> List[Tuple[np.ndarray, np.ndarray]]:
    """Extract training pairs from a task JSON."""
    pairs = []
    for ex in task_json.get("train", []):
        x = np.array(ex["input"], dtype=np.int32)
        y = np.array(ex["output"], dtype=np.int32)
        pairs.append((x, y))
    return pairs


def tests_from_task(task_json: Dict) -> List[np.ndarray]:
    """Extract test inputs from a task JSON."""
    tests = []
    for ex in task_json.get("test", []):
        x = np.array(ex["input"], dtype=np.int32)
        tests.append(x)
    return tests


def write_submission_json(output_path: str, predictions: Dict[str, Any]):
    """Write predictions to submission JSON."""
    with open(output_path, 'w') as f:
        json.dump(predictions, f, indent=2)


# ============================================================================
# ============ io/telemetry.py ============
# ============================================================================

class StepLogger:
    """Logger for telemetry during search."""

    def __init__(self, public_mode: bool = False, jsonl_path: Optional[str] = None, log_every: int = 200):
        self.public_mode = public_mode
        self.jsonl_path = jsonl_path
        self.log_every = log_every
        self.jsonl_file = None

    def __enter__(self):
        if self.jsonl_path:
            self.jsonl_file = open(self.jsonl_path, 'w')
        return self

    def __exit__(self, *args):
        if self.jsonl_file:
            self.jsonl_file.close()

    def log_iteration(self, iteration: int, beam_size: int, best_cost: float):
        """Log iteration progress."""
        if iteration % self.log_every == 0:
            msg = f"Iter {iteration}: beam={beam_size}, cost={best_cost:.4f}"
            print(msg)

        if self.jsonl_file:
            record = {
                "iteration": iteration,
                "beam_size": beam_size,
                "best_cost": best_cost,
            }
            self.jsonl_file.write(json.dumps(record) + "\n")

    def log_task_start(self, task_id: str):
        """Log task start."""
        print(f"\n{'='*60}")
        print(f"Task: {task_id}")
        print(f"{'='*60}")

    def log_task_end(self, task_id: str, success: bool, elapsed: float):
        """Log task end."""
        status = "✓ SOLVED" if success else "✗ UNSOLVED"
        print(f"{status} ({elapsed:.1f}s)")

    def log(self, task_id: str, stage: str, payload: Dict[str, Any]):
        """Structured per-stage telemetry logging."""
        record = {"task_id": task_id, "stage": stage}
        data = payload or {}
        try:
            record.update(data)
        except Exception:
            record["payload"] = data
        if self.jsonl_file:
            self.jsonl_file.write(json.dumps(record) + "\n")
            self.jsonl_file.flush()


# ============================================================================
# ============ solver/task_solver.py ============
# ============================================================================


def _compute_train_px_err(best_program, train_pairs, phi, settings):
    if best_program is None or not train_pairs:
        return 1.0, 1.0, 0.0
    errs = []
    shape_hits = 0
    for (xi, yi) in train_pairs:
        try:
            pred = interpret_program(best_program, xi)
            if pred.shape == yi.shape:
                shape_hits += 1
                errs.append(float((pred != yi).mean()))
            else:
                errs.append(1.0)
        except Exception:
            errs.append(1.0)
    mean_err = float(sum(errs) / len(errs)) if errs else 1.0
    var_err = float(np.var(errs)) if errs else 1.0
    shape_rate = float(shape_hits / max(1, len(train_pairs)))
    return mean_err, var_err, shape_rate


def _update_rho(settings, best_program, train_pairs, phi, palette_is_safe):
    eps = 1e-3
    mean_err, var_err, shape_rate = _compute_train_px_err(best_program, train_pairs, phi, settings)
    t = max(eps, 1.0 - mean_err)
    r = 1.0 / (1.0 + max(0.0, min(1.0, var_err)))
    u_len = 1.0 / (1.0 + (len(best_program.steps) / 10.0)) if best_program else 0.5
    try:
        tens = compute_program_tension(best_program, phi) if best_program else 0.0
        u_ten = 1.0 / (1.0 + tens / 10.0)
    except Exception:
        u_ten = 0.7
    u = max(eps, 0.5 * u_len + 0.5 * u_ten)
    p = max(eps, shape_rate)
    a = 1.0 if palette_is_safe else 0.9
    C = max(eps, p * t * r * u * a)
    prevC = getattr(settings, "_hfp_prevC", None)
    prev_inc = bool(getattr(settings, "_hfp_prev_increase", False))
    increase = prevC is not None and C > float(prevC) + 1e-6
    rho = (C / prevC) if (prevC is not None and prevC > 0) else 1.0
    if increase and prev_inc:
        setattr(settings, "_hfp_sustained", True)
    else:
        setattr(settings, "_hfp_sustained", False)
    setattr(settings, "_hfp_prev_increase", bool(increase))
    setattr(settings, "_hfp_prevC", float(C))
    setattr(settings, "_hfp_rho", float(rho))
    hist = list(getattr(settings, "_rho_samples", []))
    hist.append(float(rho))
    setattr(settings, "_rho_samples", hist)
    if len(hist) >= 2:
        setattr(settings, "_hfp_ready", True)
    return rho

def _solve_task_internal(
    task_id: str,
    task_json: Dict,
    settings: _InternalSearchSettings,
    logger: Optional[StepLogger] = None
) -> List[Any]:
    """
    Solve a single ARC task.
    
    Returns:
        List of predicted outputs for test inputs.
        - If best_program ends with tile_masked(ky,kx,m) where m in {0,1}:
          Returns list of dicts: [{"attempt_1": grid, "attempt_2": grid_alt}, ...]
          where attempt_2 uses the alternate mode (0↔1 swap)
        - Otherwise: Returns list of grids (np.ndarray format)
    """
    if logger:
        logger.log_task_start(task_id)

    max_secs = getattr(settings, "max_seconds", 3.0)
    if max_secs is None:
        max_secs = 0.0
    deadline = _Deadline(max_secs)
    t0 = _now()
    hit_deadline = False
    setattr(settings, "_deadline_hit", False)
    setattr(settings, "_object_soft_pref", False)

    # Extract training pairs and test inputs
    train_pairs = trains_from_task(task_json)
    test_inputs = tests_from_task(task_json)

    if not train_pairs or not test_inputs:
        elapsed = _now() - t0
        if logger:
            logger.log_task_end(task_id, False, elapsed)
        return [np.array([[0]]) for _ in test_inputs]

    # Compute task features and φ
    features = task_features(train_pairs)
    phi = compute_phi(features)
    rho_samples = list(getattr(settings, "_rho_samples", []))

    # === Memory v1.0 hooks ==================================================
    memo: Dict[str, Any] = {}
    layout_fp: Dict[str, Any] = {}
    seeded_programs: List[Program] = []
    seed_token_lists: List[List[str]] = []
    memory_family = None
    priors_applied = False
    phi_family = None
    try:
        phi_family = phi_to_family(phi)
    except Exception:
        phi_family = None
    if getattr(settings, "use_memory", True) and MEM is not None:
        try:
            memory_dir = getattr(settings, "memory_dir", None) or os.environ.get("ARC_MEMORY_DIR", "./memory_bank")
            memo = MEM.load_all(memory_dir)
            layout_fp = MEM.fingerprint_layout(task_json)
            memory_family = MEM.detect_family(layout_fp)
            if memory_family is None:
                memory_family = phi_family or layout_fp.get("family_hint")
            if getattr(settings, "memory_use_priors", True):
                priors_map = memo.get("priors", {})
                priors = MEM.op_prior_for_task(
                    memory_family or (phi_family or "default"),
                    priors_map,
                    alpha=float(getattr(settings, "priors_alpha", 1.0)),
                )
                if priors:
                    _apply_op_priors_soft(priors, alpha=float(getattr(settings, "priors_alpha", 1.0)))
                    priors_applied = True
            if getattr(settings, "memory_use_motifs", True):
                motif_topk = int(max(0, getattr(settings, "motif_topk", 0) or 0))
                candidates = _collect_memory_motif_candidates(task_json, memo, motif_topk)
                picked: List[Tuple[List[str], str, float, Program]] = []
                if candidates:
                    limit = min(motif_topk, 3) if motif_topk else 0
                    best_per_family: Dict[str, Tuple[List[str], str, float, Program]] = {}
                    for cand in candidates:
                        tokens, fam, score, prog = cand
                        fam_key = fam or "unknown"
                        existing = best_per_family.get(fam_key)
                        if existing is None or score > existing[2]:
                            best_per_family[fam_key] = cand
                    if limit:
                        for cand in sorted(best_per_family.values(), key=lambda t: -t[2]):
                            if len(picked) >= limit:
                                break
                            picked.append(cand)
                        if len(picked) < limit:
                            for cand in sorted(candidates, key=lambda t: -t[2]):
                                if cand in picked:
                                    continue
                                picked.append(cand)
                                if len(picked) >= limit:
                                    break
                    effect_sigs: Set[Any] = set()
                    for tokens, fam, score, prog in picked:
                        if prog is None or not getattr(prog, "steps", None):
                            continue
                        sig = None
                        try:
                            previews = []
                            for xi, _yi in train_pairs[:2]:
                                pred = interpret_program(prog, xi)
                                arr = np.asarray(pred)
                                previews.append(tuple(tuple(int(v) for v in row) for row in arr.tolist()))
                            sig = tuple(previews)
                        except Exception:
                            sig = None
                        if sig is not None:
                            if sig in effect_sigs:
                                continue
                            effect_sigs.add(sig)
                        seed_token_lists.append(list(tokens))
                        seeded_programs.append(prog)
                else:
                    motif_tokens = MEM.choose_motifs(layout_fp, memo.get("motifs", {}), topk=motif_topk)
                    for toks in motif_tokens:
                        toks_list = [str(t) for t in toks]
                        seed_token_lists.append(toks_list)
                        prog = _program_from_ops_tokens(toks_list)
                        if prog is not None and prog.steps:
                            seeded_programs.append(prog)
        except Exception as mem_exc:
            print(f"[MEMORY] disabled ({mem_exc})")
            memo = {}
            seeded_programs = []
            memory_family = None
    else:
        memo = {}
        seeded_programs = []
        seed_token_lists = []
        memory_family = phi_family

    if not priors_applied:
        globals().pop("_OP_NAMES_BIASED", None)

    setattr(settings, "_memory_seeded_programs", seeded_programs)
    setattr(settings, "_memory_seed_tokens", seed_token_lists)
    setattr(settings, "_memory_layout_fp", layout_fp)
    setattr(settings, "_memory_family", memory_family)
    setattr(settings, "_phi_family", memory_family or phi_family)
    setattr(settings, "_memory_priors_applied", priors_applied)

    if getattr(settings, "_policy_prior", None) is None:
        settings._policy_prior = _DEFAULT_POLICY_PRIOR

    trace = getattr(settings, "_trace_ops", False)
    settings._trace_buffer = []
    setattr(settings, "_attemptB_forced_alternate", False)
    setattr(settings, "_palette_unanimous", False)
    setattr(settings, "_palette_mapping", None)

    monitor = SelfMonitor(settings)
    setattr(settings, "_monitor", monitor)

    # --- Topology hint (side-channel): averages over train pairs ---
    dholes_g = dcomps_g = dholes_pc = dcomps_pc = 0.0
    for xi, yi in train_pairs:
        try:
            xg = np.asarray(xi)
            yg = np.asarray(yi)
            dholes_g += _holes_count(yg) - _holes_count(xg)
            dcomps_g += _component_count(yg) - _component_count(xg)
            dholes_pc += _holes_count(yg)
            dcomps_pc += _component_count(yg)
        except Exception:
            pass
    n = float(max(1, len(train_pairs)))
    settings._topo_hint = (dholes_g / n, dcomps_g / n, dholes_pc / n, dcomps_pc / n)

    # Meta-controller (opt-in)
    meta = None
    if getattr(settings, "use_meta_controller", False):
        meta = getattr(_solve_task_internal, "_META", None)
        if meta is None:
            meta = MetaControllerUCB()
            _solve_task_internal._META = meta

    # --- Consensus one-step sweep (fast path) ---
    cons_prog, cons_mean = _consensus_one_step_candidate(train_pairs)

    # Learn program from training pairs
    best_program = None
    best_avg_cost = 1e9

    controller = ControllerState(mode="observer")
    selected_arm = None
    selected_bundle = None
    selected_fam = None

    # If consensus is strong, accept it and let finishers polish
    if cons_prog is not None and cons_mean >= 0.80:
        best_program = cons_prog
        best_avg_cost = 1.0 - cons_mean
    else:
        # Standard beam search on a subset of training pairs when consensus is weak
        K_eff = _maybe_escalate_K(cons_mean, rho_samples, settings, len(train_pairs))
        pairs_for_beam = train_pairs[:max(1, K_eff)]
        if logger:
            print(
                f"[beam] sampling {len(pairs_for_beam)}/{len(train_pairs)} pairs (consensus={cons_mean:.3f})"
            )
        selected_arm = None
        selected_bundle = None
        selected_fam = None
        if meta is not None:
            selected_fam = phi_to_family(phi)
            selected_arm, selected_bundle = meta.select(selected_fam)
            if logger:
                print(f"[meta] family={selected_fam} arm={selected_arm} bundle={selected_bundle}")
        for pair_idx, (x, y) in enumerate(pairs_for_beam):
            if deadline.expired():
                hit_deadline = True
                setattr(settings, "_deadline_hit", True)
                break
            # Apply controller mode bias
            if meta is not None and selected_bundle is not None:
                mode_settings = replace(
                    settings,
                    beam_width=selected_bundle["beam_width"],
                    lambda1=selected_bundle["lambda1"],
                    lambda2=selected_bundle["lambda2"],
                    max_train_pairs_for_beam=selected_bundle["max_train_pairs_for_beam"],
                )
            else:
                mode_settings = apply_mode_bias(settings, controller.mode)

            topo_hint = getattr(settings, "_topo_hint", None)
            if topo_hint is not None:
                setattr(mode_settings, "_topo_hint", topo_hint)
            setattr(mode_settings, "_policy_prior", getattr(settings, "_policy_prior", _DEFAULT_POLICY_PRIOR))
            setattr(mode_settings, "_trace_ops", getattr(settings, "_trace_ops", False))
            if hasattr(settings, "_trace_buffer"):
                setattr(mode_settings, "_trace_buffer", getattr(settings, "_trace_buffer"))
            if hasattr(settings, "_memory_seed_tokens"):
                setattr(mode_settings, "_memory_seed_tokens", getattr(settings, "_memory_seed_tokens", []))
            if hasattr(settings, "_memory_seeded_programs"):
                setattr(mode_settings, "_memory_seeded_programs", getattr(settings, "_memory_seeded_programs", []))

            # Search for program
            prog = beam_search_one_pair(x, y, phi, mode_settings, logger, deadline=deadline)

            if deadline.expired():
                hit_deadline = True
                setattr(settings, "_deadline_hit", True)
                if prog is None:
                    break
            if prog is None:
                continue

            # Evaluate on all training pairs
            costs = []
            for (x_eval, y_eval) in train_pairs:
                cost = compute_cost(prog, x_eval, y_eval, phi, settings)
                costs.append(cost)

            avg_cost = np.mean(costs)

            if avg_cost < best_avg_cost:
                best_avg_cost = avg_cost
                best_program = prog

            # Check if should rotate controller
            if should_rotate(controller, avg_cost, settings):
                controller.mode = rotate_mode(controller)
                controller.rotation_count = 0
            else:
                controller.rotation_count += 1

            controller.last_rotation_cost = avg_cost

            if hit_deadline:
                break

            # Early stop if perfect
            if best_avg_cost < 0.01:
                break

    # --- Micro-refinement for tile_masked consensus (optional shift nudge) ---
    if best_program is not None and len(best_program.steps) == 1:
        step = best_program.steps[0]
        if step.op == "tile_masked" and len(step.args) >= 3:
            # Try a single 'shift' around small deltas to catch off-by-one placement
            candidates = []
            for dy in (-1, 0, 1):
                for dx in (-1, 0, 1):
                    if dy == 0 and dx == 0: 
                        continue
                    prog_shift = Program(best_program.steps + [Step("shift", (dy, dx))])
                    candidates.append(prog_shift)

            # Evaluate train-average cost and keep if better
            def _avg_cost(p):
                cs = []
                for (xi, yi) in train_pairs:
                    try:
                        cs.append(compute_cost(p, xi, yi, phi, settings))
                    except Exception:
                        cs.append(1e9)
                return float(sum(cs) / len(cs)) if cs else 1e9

            if candidates:
                base_avg = _avg_cost(best_program)
                best_cand = min(candidates, key=_avg_cost, default=None)
                if best_cand is not None:
                    cand_avg = _avg_cost(best_cand)
                    if cand_avg + 1e-9 < base_avg:
                        best_program = best_cand
                        best_avg_cost = cand_avg

    # --- Alignment finisher (learn a single task-level shift) ---
    align = _learn_task_alignment(best_program, train_pairs, phi, settings) if best_program is not None else None
    if align is not None:
        dy, dx = align
        prog_shift = Program([Step(s.op, s.args) for s in best_program.steps] + [Step("shift", (dy, dx))])
        # accept shift if it improves average train cost
        avg_best  = float(np.mean([compute_cost(best_program, xi, yi, phi, settings) for (xi, yi) in train_pairs]))
        avg_shift = float(np.mean([compute_cost(prog_shift,   xi, yi, phi, settings) for (xi, yi) in train_pairs]))
        if avg_shift + 1e-9 < avg_best:
            best_program = prog_shift

    # --- Blockwise dominant-color finisher (sparse tiling) ---
    # Engage only when output is an integer multiple of input with small k
    learned_blockwise_proj = None
    try:
        x0, y0 = train_pairs[0]
        div = _divisible_shape(x0.shape, y0.shape) if (x0 is not None and y0 is not None) else None
    except Exception:
        div = None

    if div is not None and best_program is not None:
        ky, kx = div
        if ky in (3, 5) and kx in (3, 5):
            D = _learn_blockwise_projection(train_pairs, ky, kx)
            if D is not None:
                # Accept the projection only if it lowers or matches avg training cost when applied to predictions
                # (Compute average cost over train pairs with projection applied)
                def _avg_cost_with_projection(prog):
                    cs = []
                    for (xi, yi) in train_pairs:
                        try:
                            pi = interpret_program(prog, xi)
                            if pi.shape == yi.shape:
                                pi = _apply_blockwise_projection(pi, ky, kx, D)
                            # compute cost of fixed prediction vs target
                            cs.append(compute_cost(Program([]), pi, yi, phi, settings))
                        except Exception:
                            cs.append(1e9)
                    return float(np.mean(cs)) if cs else 1e9

                avg_cur = float(np.mean([compute_cost(best_program, xi, yi, phi, settings) for (xi, yi) in train_pairs]))
                avg_proj = _avg_cost_with_projection(best_program)
                if avg_proj + 1e-9 <= avg_cur:
                    learned_blockwise_proj = (ky, kx, D)

    # --- Block-mask finisher (sparse tiling) before palette ---
    # Learn a kxk mask only if all train shapes are consistent multiples of inputs
    try:
        # use the first train pair to infer divisibility
        x0, y0 = train_pairs[0]
        div = _divisible_shape(x0.shape, y0.shape) if (x0 is not None and y0 is not None) else None
    except Exception:
        div = None

    if div is not None and best_program is not None:
        ky, kx = div
        # small k only (keep it safe): 3 or 5
        if ky in (3,5) and kx in (3,5):
            M = _learn_block_mask(train_pairs, ky, kx)
            if M is not None:
                # Synthesize a program variant that applies the learned mask at prediction time
                # We do this as a *post-step* transform in training evaluation space:
                def _masked_eval_cost(prog):
                    # avg cost over train pairs with masking applied
                    cs = []
                    for (xi, yi) in train_pairs:
                        try:
                            pi = interpret_program(prog, xi)
                            # only mask if shapes are compatible
                            if pi.shape == yi.shape and _divisible_shape(xi.shape, yi.shape) == (ky, kx):
                                pi = _apply_block_mask(pi, ky, kx, M)
                            cs.append(compute_cost(Program([]), pi, yi, phi, settings))  # cost of fixed prediction vs target
                        except Exception:
                            cs.append(1e9)
                    return float(np.mean(cs)) if cs else 1e9

                avg_cur = _masked_eval_cost(best_program)  # masking applied to current program's outputs
                # Try appending an explicit mask step at inference time by wrapping predictions later;
                # since we don't have a DSL op for masking, we accept as a finisher iff it lowers cost.
                if avg_cur + 1e-9 < float(np.mean([compute_cost(best_program, xi, yi, phi, settings) for (xi, yi) in train_pairs])):
                    # Store learned mask for use on test predictions below
                    learned_block_mask = (ky, kx, M)
                else:
                    learned_block_mask = None
            else:
                learned_block_mask = None
        else:
            learned_block_mask = None
    else:
        learned_block_mask = None

    # Attach palette finisher - try appending palette remapping to improve accuracy
    if deadline.expired():
        hit_deadline = True
    if best_program is not None and not deadline.expired():
        mapping = _palette_map_from_train_pairs(train_pairs)
        if mapping:
            prog2 = Program([Step(s.op, s.args) for s in best_program.steps])
            for src, dst in mapping.items():
                prog2.steps.append(Step("replacecolor", (int(src), int(dst))))
            avg_best = np.mean([compute_cost(best_program, xi, yi, phi, settings) for (xi, yi) in train_pairs])
            avg_p2 = np.mean([compute_cost(prog2, xi, yi, phi, settings) for (xi, yi) in train_pairs])
            if avg_p2 < avg_best:
                best_program = prog2

    # ===== v2.9.1: Late auto-refiners (looser gate, extra candidate) =====
    if deadline.expired():
        hit_deadline = True
    if best_program is not None and not deadline.expired():
        def _avg_train_cost_pixels(prog: Program) -> float:
            errs = []
            for xi, yi in train_pairs:
                try:
                    pred = interpret_program(prog, xi)
                    tgt = np.asarray(yi)
                    if pred.shape != tgt.shape:
                        errs.append(1.0)
                    else:
                        errs.append(float((pred != tgt).mean()))
                except Exception:
                    errs.append(1.0)
            return float(np.mean(errs)) if errs else 1.0

        try:
            base_cost_px = _avg_train_cost_pixels(best_program)
            if base_cost_px <= 0.45:
                candidates = []
                base_steps = [Step(s.op, s.args) for s in best_program.steps]
                prog_keep1 = Program(base_steps + [Step("keep_n_largest", (1,))])
                prog_keep2 = Program(base_steps + [Step("keep_n_largest", (2,))])
                prog_fill = Program(base_steps + [Step("fill_holes", tuple())])
                candidates.append(("keep_n_largest(1)", prog_keep1, _avg_train_cost_pixels(prog_keep1)))
                candidates.append(("keep_n_largest(2)", prog_keep2, _avg_train_cost_pixels(prog_keep2)))
                candidates.append(("fill_holes", prog_fill, _avg_train_cost_pixels(prog_fill)))
                _, best_prog2, best_cost_px = min(candidates, key=lambda t: t[2])
                if best_cost_px + 1e-9 < base_cost_px:
                    best_program = best_prog2
                    best_avg_cost = np.mean(
                        [compute_cost(best_program, xi, yi, phi, settings) for (xi, yi) in train_pairs]
                    )
        except Exception:
            pass

    # Apply best program to test inputs
    # Learn palette map once from training pairs (used for TRAIN evaluation only)
    palette_map = _palette_map_from_train_pairs(train_pairs)
    
    # Validate palette safety on training pairs (for TRAIN-FIT logic)
    palette_is_safe = False
    if palette_map and best_program is not None:
        safe_count = 0
        for (xi, yi) in train_pairs:
            try:
                pred_train = interpret_program(best_program, xi)
                if pred_train.shape == yi.shape:
                    before = float((pred_train == yi).mean())
                    mapped = np.array(_apply_palette_map_ll(pred_train.tolist(), palette_map), dtype=np.int32)
                    after = float((mapped == yi).mean())
                    if after >= before:
                        safe_count += 1
            except Exception:
                continue
        # Palette is safe if it helps or maintains accuracy on all training pairs
        palette_is_safe = (safe_count == len(train_pairs))
    
    rho_composite = _update_rho(settings, best_program, train_pairs, phi, palette_is_safe)
    _smooth_rho(settings, getattr(settings, "_hfp_rho", None))
    prog_tension = float(compute_program_tension(best_program, phi)) if best_program is not None else 0.0
    if monitor is not None:
        try:
            monitor.update_hfp_from_tension(rho_override=rho_composite)
        except Exception:
            pass

    R_break = float(getattr(settings, "hfp_break_R_min", 0.010)) if settings else 0.010
    R_good = float(getattr(settings, "_hfp_R_good", 0.080)) if settings else 0.080
    R_val = None
    if monitor is not None:
        R_val = getattr(monitor, "R_last", None)
    if R_val is None and rho_composite is not None:
        try:
            R_val = rvd_R(rho_composite)
        except Exception:
            R_val = None
    if R_val is not None:
        if settings is not None:
            setattr(settings, "_hfp_R_last", float(R_val))
        if monitor is not None:
            monitor.R_last = float(R_val)
            if R_val < R_break:
                monitor.health = "low_R"
        if R_val < R_break:
            if settings is not None:
                setattr(settings, "_hfp_should_break", True)
                setattr(settings, "_hfp_break_reason", "low_R")
        elif R_val >= R_good:
            stretched = False
            if settings is not None:
                stretched = bool(getattr(settings, "_hfp_stretched", False))
            if not stretched:
                try:
                    current_budget = float(getattr(settings, "max_seconds", 0.0)) if settings else 0.0
                except Exception:
                    current_budget = 0.0
                new_budget = float(int(round(current_budget * 1.25))) if current_budget > 0 else current_budget
                if settings is not None and new_budget > current_budget:
                    settings.max_seconds = new_budget
                    setattr(settings, "_hfp_stretched", True)
                if settings is not None and new_budget > current_budget:
                    try:
                        deadline.t_end = t0 + new_budget
                    except Exception:
                        pass
                if monitor is not None:
                    setattr(monitor, "_stretched", True)

    # Check if we should generate two attempts using truncated shape-base
    # Truncate program at last tile_masked to get pure shape transformation
    base_steps, ky, kx, m = _truncate_at_last_tile_masked(best_program)
    emit_two_attempts = (base_steps is not None)

    seconds_adjusted = False
    
    # Debug telemetry
    if logger and emit_two_attempts:
        print(f"[two-attempts] tile_masked(ky={ky}, kx={kx}) found, using truncated shape-base (no finishers)")
    
    predictions = []
    
    # TWO-ATTEMPTS MODE: Use truncated shape-base programs, no finishers on TEST
    if emit_two_attempts:
        # Build truncated programs: attempt_1 = original mode, attempt_2 = swapped mode
        prog1 = Program(list(base_steps))  # keep original mode m

        # Build alternate with swapped mode
        alt_mode = 1 - m
        alt_steps = list(base_steps)
        alt_steps[-1] = Step("tile_masked", (ky, kx, alt_mode))
        prog2 = Program(alt_steps)

        for test_idx, test_x in enumerate(test_inputs):
            if deadline.expired():
                hit_deadline = True
                setattr(settings, "_deadline_hit", True)
                fallback = test_x.copy()
                fallback_list = fallback.tolist()
                entry = {
                    "attempt_1": fallback_list,
                    "attempt_2": fallback_list,
                    "grid": fallback_list,
                    "_telemetry": {},
                }
                seconds_val = float(getattr(settings, "max_seconds", 0.0) or 0.0)
                _telemetry_note(entry, seconds_after_schedule=seconds_val)
                predictions.append(entry)
                continue

            try:
                pred1 = interpret_program(prog1, test_x)
            except Exception:
                pred1 = test_x.copy()

            try:
                pred2 = interpret_program(prog2, test_x)
            except Exception:
                pred2 = pred1  # Fallback to attempt_1

            att1_grid = pred1.tolist()
            att2_grid = pred2.tolist()
            telemetry = {}
            _cap_telemetry(telemetry)
            att1_entry = {"grid": att1_grid, "_telemetry": telemetry, "meta": {"tension": prog_tension}}
            att2_entry = {"grid": att2_grid, "_telemetry": telemetry, "meta": {"tension": prog_tension}}

            truth_probe = None
            try:
                tests_full = task_json.get("test", [])
                if isinstance(tests_full, list) and test_idx < len(tests_full):
                    truth_probe = tests_full[test_idx].get("output")
            except Exception:
                truth_probe = None

            if not seconds_adjusted:
                base_secs = float(getattr(settings, "max_seconds", 30.0) or 0.0)
                rho_probe = _rho_from(att1_entry["grid"], truth_probe, (att1_entry.get("meta") or {}).get("tension"))
                new_secs = float(_seconds_schedule(base_secs, rho_probe))
                try:
                    settings.max_seconds = new_secs
                except Exception:
                    pass
                seconds_val = new_secs
                seconds_adjusted = True
            else:
                seconds_val = float(getattr(settings, "max_seconds", 0.0) or 0.0)

            att2_entry = _enforce_diversity(att1_entry, att2_entry)
            winner = _debate_reconcile(task_json, att1_entry, att2_entry)

            _cap_telemetry(telemetry)
            entry = {
                "attempt_1": att1_entry["grid"],
                "attempt_2": att2_entry["grid"],
                "grid": winner.get("grid", att1_entry["grid"]),
                "_telemetry": telemetry,
            }
            _telemetry_note(entry, seconds_after_schedule=seconds_val)
            predictions.append(entry)
    
    # SINGLE-ATTEMPT MODE: Apply finishers on TEST
    else:
        for test_idx, test_x in enumerate(test_inputs):
            expired_now = deadline.expired()
            if expired_now:
                hit_deadline = True
                setattr(settings, "_deadline_hit", True)

            if best_program is None or expired_now:
                pred1 = test_x.copy()
            else:
                try:
                    pred1 = interpret_program(best_program, test_x)
                except Exception:
                    pred1 = test_x.copy()

            if not expired_now:
                if 'learned_block_mask' in locals() and learned_block_mask is not None:
                    ky, kx, M = learned_block_mask
                    div = _divisible_shape(test_x.shape, pred1.shape)
                    if div == (ky, kx):
                        pred1 = _apply_block_mask(pred1, ky, kx, M)

                if 'learned_blockwise_proj' in locals() and learned_blockwise_proj is not None:
                    ky, kx, D = learned_blockwise_proj
                    div = _divisible_shape(test_x.shape, pred1.shape)
                    if div == (ky, kx):
                        pred1 = _apply_blockwise_projection(pred1, ky, kx, D)

            pred_np = np.asarray(pred1)
            ky_kx_meta = None
            if "learned_block_mask" in locals() and learned_block_mask is not None:
                ky_kx_meta = (learned_block_mask[0], learned_block_mask[1])
            elif "learned_blockwise_proj" in locals() and learned_blockwise_proj is not None:
                ky_kx_meta = (learned_blockwise_proj[0], learned_blockwise_proj[1])
            meta = {
                "bg": 0,
                "prog_len": (len(best_program.steps) if best_program else 0),
                "tension": prog_tension,
                "align_dy_dx": align,
                "block_mask": (learned_block_mask[2] if ("learned_block_mask" in locals() and learned_block_mask is not None) else None),
                "block_proj": (lambda g, ky=learned_blockwise_proj[0], kx=learned_blockwise_proj[1], D=learned_blockwise_proj[2]: _apply_blockwise_projection(g, ky, kx, D)) if ("learned_blockwise_proj" in locals() and learned_blockwise_proj is not None and not expired_now) else None,
                "palette_safe": bool(palette_is_safe),
                "ky_kx": ky_kx_meta,
            }
            entry = {"grid": pred_np.tolist(), "meta": meta, "_telemetry": {}}

            truth_probe = None
            try:
                tests_full = task_json.get("test", [])
                if isinstance(tests_full, list) and test_idx < len(tests_full):
                    truth_probe = tests_full[test_idx].get("output")
            except Exception:
                truth_probe = None

            if not seconds_adjusted:
                base_secs = float(getattr(settings, "max_seconds", 30.0) or 0.0)
                rho_probe = _rho_from(entry["grid"], truth_probe, meta.get("tension"))
                new_secs = float(_seconds_schedule(base_secs, rho_probe))
                try:
                    settings.max_seconds = new_secs
                except Exception:
                    pass
                seconds_val = new_secs
                seconds_adjusted = True
            else:
                seconds_val = float(getattr(settings, "max_seconds", 0.0) or 0.0)

            _telemetry_note(entry, seconds_after_schedule=seconds_val)
            predictions.append(entry)

    # v2.8.7: expose last program for replay/debugging
    try:
        globals()["last_best_program"] = best_program
    except Exception:
        pass

    # v2.8.7: universal two-attempts for non-tiling programs
    if (
        settings.always_two_attempts
        and predictions
        and not (isinstance(predictions[0], dict) and ("attempt_1" in predictions[0] or "grid" in predictions[0]))
    ):
        enhanced = []
        for grid in predictions:
            if deadline.expired():
                hit_deadline = True
                setattr(settings, "_deadline_hit", True)
                fallback = grid.tolist() if hasattr(grid, "tolist") else grid
                entry = {
                    "attempt_1": fallback,
                    "attempt_2": fallback,
                    "grid": fallback,
                    "_telemetry": {},
                }
                _telemetry_note(entry, seconds_after_schedule=float(getattr(settings, "max_seconds", 0.0) or 0.0))
                enhanced.append(entry)
                continue
            try:
                g = np.asarray(grid)
                alt_cands = [
                    np.rot90(g, k=1),
                    np.rot90(g, k=2),
                    np.rot90(g, k=3),
                    np.fliplr(g),
                    np.flipud(g),
                ]
                attempt_2 = alt_cands[0]
                telemetry = {}
                _cap_telemetry(telemetry)
                att1_entry = {"grid": g.tolist(), "_telemetry": telemetry, "meta": {"tension": prog_tension}}
                att2_entry = {"grid": attempt_2.tolist(), "_telemetry": telemetry, "meta": {"tension": prog_tension}}
                att2_entry = _enforce_diversity(att1_entry, att2_entry)
                _cap_telemetry(telemetry)
                entry = {
                    "attempt_1": att1_entry["grid"],
                    "attempt_2": att2_entry["grid"],
                    "grid": att1_entry["grid"],
                    "_telemetry": telemetry,
                }
                _telemetry_note(entry, seconds_after_schedule=float(getattr(settings, "max_seconds", 0.0) or 0.0))
                enhanced.append(entry)
            except Exception:
                fallback = grid.tolist() if hasattr(grid, "tolist") else grid
                entry = {
                    "attempt_1": fallback,
                    "attempt_2": fallback,
                    "grid": fallback,
                    "_telemetry": {},
                }
                _telemetry_note(entry, seconds_after_schedule=float(getattr(settings, "max_seconds", 0.0) or 0.0))
                enhanced.append(entry)
        predictions = enhanced

    elapsed = _now() - t0
    success = best_avg_cost < 0.01

    if meta is not None and selected_arm is not None and selected_fam is not None:
        try:
            reward = max(0.0, 1.0 - float(best_avg_cost)) / (0.05 + elapsed)
            meta.update(selected_fam, selected_arm, reward)
        except Exception:
            pass

    if trace and logger:
        print(f"[trace] collected {len(getattr(settings, '_trace_buffer', []))} op deltas")

    hit_deadline = hit_deadline or getattr(settings, "_deadline_hit", False) or deadline.expired()
    budget = float(getattr(settings, "max_seconds", 0.0) or 0.0)
    ops_tokens: List[str] = []
    ops_families: List[str] = []
    if best_program is not None:
        for step in best_program.steps:
            ops_tokens.append(_format_step_token(step))
            ops_families.append(_op_family_tag(step))
    object_soft_pref_flag = bool(getattr(settings, "_object_soft_pref", False))
    payload = {
        "budget_sec": budget,
        "elapsed_sec": float(elapsed),
        "hit_deadline": bool(hit_deadline),
        "ops_tokens": ops_tokens,
        "ops_families": ops_families,
        "program_len": int(len(ops_tokens)),
        "object_soft_pref": object_soft_pref_flag,
        "k_scale": int(getattr(settings, "scale_hard_steps", 3)),
        "k_palette": int(getattr(settings, "early_palette_block_steps", 3)),
        "rails_scale_hard": bool(getattr(settings, "rails_scale_hard", True)),
    }
    memory_seed_info = []
    try:
        memory_seed_info = [
            [step.op for step in prog.steps]
            for prog in (getattr(settings, "_memory_seeded_programs", []) or [])
        ]
    except Exception:
        memory_seed_info = []
    if memory_seed_info:
        payload["memory_seeded"] = memory_seed_info
        payload["memory_family"] = getattr(settings, "_memory_family", None)
    if getattr(settings, "_memory_priors_applied", False):
        payload["memory_priors_alpha"] = float(getattr(settings, "priors_alpha", 0.0) or 0.0)
        payload.setdefault("memory_family", getattr(settings, "_memory_family", None))
    if logger and hasattr(logger, "log_kv"):
        try:
            logger.log_kv("budget_sec", budget)
            logger.log_kv("elapsed_sec", float(elapsed))
            logger.log_kv("hit_deadline", bool(hit_deadline))
        except Exception:
            pass
    try:
        if isinstance(predictions, list) and predictions:
            first = predictions[0]
            if isinstance(first, dict):
                predictions[0] = _endgame_refine(task_json, first)
        elif isinstance(predictions, dict):
            predictions = _endgame_refine(task_json, predictions)
    except Exception:
        pass

    if isinstance(predictions, list):
        for entry in predictions:
            if isinstance(entry, dict):
                _telemetry_note(entry, **payload)
                _telemetry_note(
                    entry,
                    policy="accuracy_pack",
                    debate_refiner=(entry.get("_telemetry", {}) or {}).get("debate_refiner"),
                    diversity_nudge=(entry.get("_telemetry", {}) or {}).get("diversity_nudge"),
                )
        try:
            if predictions and isinstance(predictions[0], dict):
                telem = predictions[0].setdefault("_telemetry", {})
                telem["gof9k_monitor"] = monitor.as_dict()
                if getattr(settings, "hfp_debug_telemetry", True) and getattr(monitor, "hfp_dbg", True):
                    telem["hfp_gof"] = monitor.hfp_gof_dict()
        except Exception:
            pass
    elif isinstance(predictions, dict):
        _telemetry_note(predictions, **payload)
        _telemetry_note(
            predictions,
            policy="accuracy_pack",
            debate_refiner=(predictions.get("_telemetry", {}) or {}).get("debate_refiner"),
            diversity_nudge=(predictions.get("_telemetry", {}) or {}).get("diversity_nudge"),
        )
        try:
            telem = predictions.setdefault("_telemetry", {})
            telem["gof9k_monitor"] = monitor.as_dict()
            if getattr(settings, "hfp_debug_telemetry", True) and getattr(monitor, "hfp_dbg", True):
                telem["hfp_gof"] = monitor.hfp_gof_dict()
        except Exception:
            pass

    if logger:
        logger.log_task_end(task_id, success, elapsed)

    return predictions


# v2.8.7: progressive beam schedules
DEFAULT_SCHEDULES = [(24, 3), (48, 5), (64, 7), (96, 8)]


def solve_task_with_schedule(
    tid,
    task_json,
    settings,
    logger=None,
    schedules=None,
):
    """
    Try a tiered (beam_width, max_depth) schedule and return the best predictions and schedule.
    If settings._truths_provider is present (callable tid->list[np.ndarray]), use it to pick the best;
    otherwise returns the first successful predictions.
    """

    schedules = schedules or DEFAULT_SCHEDULES
    best_score, best_preds, best_cfg = -1.0, None, None
    last_preds = None

    truths = None
    try:
        provider = getattr(settings, "_truths_provider", None)
        if callable(provider):
            truths = provider(tid)
    except Exception:
        truths = None

    for beam_width, max_depth in schedules:
        sub = replace(settings, beam_width=beam_width, max_depth=max_depth)
        # propagate auxiliary hooks if present
        for attr in ("_truths_provider", "_policy_prior", "_trace_ops"):
            if hasattr(settings, attr):
                setattr(sub, attr, getattr(settings, attr))

        preds = _solve_task_internal(tid, task_json, sub, logger=logger)
        last_preds = preds
        score = -1.0
        if truths is not None and preds is not None and len(truths) == len(preds):
            try:
                score = float(
                    np.mean([_pixel_acc(preds[i], truths[i]) for i in range(len(preds))])
                )
            except Exception:
                score = -1.0

        if best_preds is None or score > best_score:
            best_score, best_preds, best_cfg = score, preds, (beam_width, max_depth)

        if score == 1.0:
            break

    if best_preds is not None:
        try:
            refined = _endgame_refine(task_json, best_preds)
            if refined is not None:
                best_preds = refined
        except Exception:
            pass
    return (best_preds if best_preds is not None else last_preds), best_cfg


# ============================================================================
# ============ solver/batch.py ============
# ============================================================================

def run_dir(
    tasks_dir: str,
    settings: _InternalSearchSettings,
    max_tasks: Optional[int] = None,
    logger: Optional[StepLogger] = None
) -> Dict[str, List[Any]]:
    tasks = list(load_tasks_from_dir(tasks_dir))
    tasks_dict = dict(tasks)
    panel_ids = list(tasks_dict.keys())
    if max_tasks:
        panel_ids = panel_ids[:max_tasks]
        tasks_dict = {tid: tasks_dict[tid] for tid in panel_ids}
    # _run_dir_staged expects loader to fetch tasks by id
    def _local_loader(_):
        return tasks_dict
    prev_loader = globals().get("_load_test_challenges")
    globals()["_load_test_challenges"] = _local_loader
    try:
        staged = _run_dir_staged(
            tasks_dir,
            seconds=getattr(settings, "max_seconds", None),
            panel_ids=panel_ids,
            logger=logger,
            settings_proto=settings,
        )
    finally:
        if prev_loader is not None:
            globals()["_load_test_challenges"] = prev_loader
        else:
            globals().pop("_load_test_challenges", None)
    results = {}
    for tid, preds in staged.items():
        if preds and isinstance(preds[0], dict):
            preds_ll = preds
        else:
            preds_ll = [pred.tolist() if hasattr(pred, "tolist") else pred for pred in preds]
        results[tid] = preds_ll
    return results


# ============================================================================
# ============ eval/metrics.py ============
# ============================================================================

def exact_match(pred: np.ndarray, truth: np.ndarray) -> bool:
    """Check if prediction exactly matches ground truth."""
    if pred.shape != truth.shape:
        return False
    return np.array_equal(pred, truth)


def pixel_accuracy(pred: np.ndarray, truth: np.ndarray) -> float:
    """Compute pixel-wise accuracy."""
    if pred.shape != truth.shape:
        return 0.0
    correct = np.sum(pred == truth)
    total = truth.size
    return correct / max(total, 1)


def solve_rate(preds: List[np.ndarray], truths: List[np.ndarray]) -> float:
    """Compute fraction of tasks solved."""
    if not preds or not truths:
        return 0.0
    solved = sum(exact_match(p, t) for p, t in zip(preds, truths))
    return solved / len(truths)


def evaluate_task(preds: List[np.ndarray], 
                 truths: List[np.ndarray]) -> Dict[str, float]:
    """Comprehensive evaluation metrics for one task."""
    if not preds or not truths:
        return {"exact_match": 0.0, "pixel_accuracy": 0.0, "solve_rate": 0.0}
    
    exact = all(exact_match(p, t) for p, t in zip(preds, truths))
    pixel_acc = np.mean([pixel_accuracy(p, t) for p, t in zip(preds, truths)])
    solve = solve_rate(preds, truths)
    
    return {
        "exact_match": float(exact),
        "pixel_accuracy": float(pixel_acc),
        "solve_rate": float(solve),
    }


# ============================================================================
# ============ eval/ablations.py ============
# ============================================================================

def without_oco(settings):
    """Disable OCO: no tension penalties."""
    from dataclasses import replace
    s = replace(settings)
    s.lambda1 = 0.0
    s.lambda2 = 0.0
    return s


def without_slice_guard(settings):
    """Disable slice gating."""
    from dataclasses import replace
    s = replace(settings)
    s.slice_guard_thresh = 1e9
    s.allow_offslice_early = True
    return s


def without_rotation(settings):
    """Disable controller rotations."""
    from dataclasses import replace
    s = replace(settings)
    s._disable_rotation = True
    return s


def get_ablation_config(name: str, base_settings):
    """Get settings for ablation experiment."""
    ablations = {
        "no_oco": without_oco,
        "no_slice": without_slice_guard,
        "no_rotation": without_rotation,
    }
    
    if name == "baseline":
        return base_settings
    elif name in ablations:
        return ablations[name](base_settings)
    else:
        raise ValueError(f"Unknown ablation: {name}")


# ============================================================================
# ============ Recursion-Safe Wrapper Pattern ============
# ============================================================================
# 
# CRITICAL: If you want to monkey-patch beam_search_one_pair or generate_successors
# with custom gating logic, use this pattern to avoid RecursionError.
#
# The problem: If wrapper A calls the patched function, which is now wrapper B,
# which calls wrapper A again → infinite recursion.
#
# The solution: Save the base function ONCE and always call the base.
#
# Example for beam_search_one_pair:
#
# import arc_one
#
# # Step 1: Save base exactly once (before any patching)
# if not hasattr(arc_one, "_BASE_BEAM"):
#     arc_one._BASE_BEAM = arc_one.beam_search_one_pair
#
# # Step 2: Define your wrapper
# def custom_beam_wrapper(input_grid, target_grid, phi, settings, logger):
#     """Your custom shape-then-color gating logic."""
#     # ... preprocessing ...
#     
#     # ALWAYS call the saved base, never the patched version
#     result = arc_one._BASE_BEAM(input_grid, target_grid, phi, settings, logger)
#     
#     # ... postprocessing ...
#     return result
#
# # Step 3: Guard against double-wrapping
# if getattr(arc_one.beam_search_one_pair, "__name__", "") != "custom_beam_wrapper":
#     arc_one.beam_search_one_pair = custom_beam_wrapper
#
# Same pattern for generate_successors:
# - Save as _BASE_GENERATE_SUCCESSORS
# - Always call _BASE_GENERATE_SUCCESSORS inside wrapper
# - Use try/finally to restore if needed
#
# Example with try/finally cleanup:
#
# if not hasattr(arc_one, "_BASE_GENERATE"):
#     arc_one._BASE_GENERATE = arc_one.generate_successors
#
# def custom_generate(prog, ctx):
#     # Filter or modify successor generation
#     successors = arc_one._BASE_GENERATE(prog, ctx)
#     return [s for s in successors if some_condition(s)]
#
# # Temporarily patch
# old_generate = arc_one.generate_successors
# arc_one.generate_successors = custom_generate
# try:
#     # ... run solver ...
#     results = run_dir(...)
# finally:
#     # Restore original
#     arc_one.generate_successors = old_generate
#
# ============================================================================


# ============================================================================
# ============ cli/main.py ============
# ============================================================================

def main():
    """Command-line interface for ARC-ONE solver."""
    parser = argparse.ArgumentParser(
        description="ARC-ONE: Octonionic Control Overlay Solver",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # OCO-guided two attempts (recommended)
  python arc_one.py --tasks_dir ./arc_tasks --two_attempts --out submission.json
  
  # Two attempts with manual strategy (e.g., horizontal flip)
  python arc_one.py --tasks_dir ./arc_tasks --two_attempts --attempt2_strategy flipH
  
  # Legacy single attempt (no change to file structure)
  python arc_one.py --tasks_dir ./arc_tasks --out submission.json
  
  # Quick test with validation
  python arc_one.py --tasks_dir ./arc_tasks --two_attempts --max_tasks 5
  
  # Ablation study
  python arc_one.py --tasks_dir ./arc_tasks --ablation no_oco --out no_oco.json
  
  # With telemetry logging
  python arc_one.py --tasks_dir ./arc_tasks --jsonl telemetry.jsonl --two_attempts
        """
    )
    
    # I/O
    parser.add_argument("--tasks_dir", required=True,
                       help="Directory containing task JSON files")
    parser.add_argument("--out", default="submission.json",
                       help="Output submission file")
    parser.add_argument("--two_attempts", action="store_true",
                       help="Output two attempts per test input (ARC 2025 schema).")
    parser.add_argument("--attempt2_strategy", type=str, default="oco_auto",
                       choices=["oco_auto","auto","rotate90","rot180","flipH","flipV","palette_swap","center","toward_input"],
                       help="How to generate attempt_2.")
    
    # Limits
    parser.add_argument("--max_tasks", type=int, default=None,
                       help="Max number of tasks to solve")
    
    # Search settings
    parser.add_argument("--beam", type=int, default=128,
                       help="Beam width (default: 128)")
    parser.add_argument("--depth", type=int, default=10,
                       help="Max program depth (default: 10)")
    parser.add_argument("--seconds", type=float, default=3.0,
                       help="Max seconds per task (default: 3.0)")
    
    # OCO settings
    parser.add_argument("--lambda_len", type=float, default=0.20,
                       help="Length penalty weight (default: 0.20)")
    parser.add_argument("--lambda1", type=float, default=0.30,
                       help="Program tension weight (default: 0.30)")
    parser.add_argument("--lambda2", type=float, default=0.20,
                       help="Slice tension weight (default: 0.20)")
    parser.add_argument("--div_lambda", type=float, default=0.5,
                        help="Diversity weight λ for second attempt: score = conf − λ·IoU(first).")
    parser.add_argument("--iou_cap", type=float, default=0.97,
                        help="Hard cap on IoU(A,B) for Attempt-B eligibility.")
    parser.add_argument("--max_bounces", type=int, default=8,
                        help="Maximum bounce retries for Attempt-B alternates/debate.")
    parser.add_argument("--no_octo_prior", action="store_true",
                        help="Disable palette8 octonion distance–based time scaling (training stage).")
    parser.add_argument("--octo_alpha", type=float, default=0.25,
                        help="Z-score scaling strength for octo prior (mult = 1 + alpha * z).")
    parser.add_argument("--octo_clip", type=float, default=2.0,
                        help="Clamp |z| to this many std devs for octo prior.")
    parser.add_argument("--no_polish", action="store_true",
                        help="Skip polish stage (third stage) to save time.")
    parser.add_argument("--stop_if_diversity", type=float, default=0.20,
                        help="Early-exit after a stage if (1 - IoU(attempt1,attempt2)) exceeds threshold.")
    parser.add_argument("--no_bounce", action="store_true",
                        help="Turn OFF the default bounce-on-low-div-high-octo policy.")
    parser.add_argument("--lowdiv_thr", type=float, default=0.05,
                        help="Skim diversity threshold to trigger bounce (default 0.05).")
    parser.add_argument("--octo_z_min_for_bounce", type=float, default=1.20,
                        help="Minimum octonion z to consider a task 'hard' (default 1.20).")
    parser.add_argument("--bounce_max", dest="max_bounces", type=int, help=argparse.SUPPRESS)
    parser.add_argument("--no_block_identity", action="store_true",
                        help="Disable identity ejection (tile(1,1), scale(1), resize to same size).")
    parser.add_argument(
        "--no_rails_scale_hard",
        action="store_true",
        help="Disable hard scale-sign rails for early steps (default ON).",
    )
    parser.add_argument("--scale_hard_thresh", type=float, default=1.0,
                        help="|phi[0]| threshold for hard scale rails (default 1.0).")
    parser.add_argument("--scale_hard_steps", type=int, default=3,
                        help="Early steps to enforce hard scale rails (default 3).")
    parser.add_argument("--early_palette_block_steps", type=int, default=3,
                        help="Block palette ops for the first N steps (default 3).")
    # GOF-9000 SelfMonitor options
    parser.add_argument("--no_monitor", action="store_true",
                        help="Disable GOF-9000 self-monitor (pressure/novelty guard).")
    parser.add_argument("--monitor_pressure_thresh", type=float, default=0.70,
                        help="Pressure threshold (fraction of ops in one class) before injecting diversity.")
    parser.add_argument("--monitor_plateau_N", type=int, default=3,
                        help="Reserved plateau detector window (default 3).")
    parser.add_argument("--monitor_visited_cap", type=int, default=50000,
                        help="LRU cap for duplicate-program signatures (default 50k).")
    parser.add_argument("--no_monitor_inject_underused", action="store_true",
                        help="Do not inject underused-class successors when pressure detected.")
    parser.add_argument("--no_monitor_drop_dupe_sigs", action="store_true",
                        help="Do not prune duplicate program signatures during search.")

    # Memory v1.0
    parser.add_argument("--use_memory", dest="use_memory", action="store_true",
        help="Enable memory (motifs + priors).")
    parser.add_argument("--no_use_memory", dest="use_memory", action="store_false",
        help="Disable memory (motifs + priors).")
    parser.add_argument("--no_memory_motifs", action="store_true",
        help="Disable motif seeding.")
    parser.add_argument("--no_memory_priors", action="store_true",
        help="Disable priors bias.")
    parser.add_argument("--memory_dir", type=str, default=None,
        help="Path to memory_bank directory (default: $ARC_MEMORY_DIR or ./memory_bank).")
    parser.add_argument("--motif_topk", type=int, default=None,
        help="Max motif seeds to inject (default 1).")
    parser.add_argument("--priors_alpha", type=float, default=None,
        help="Strength of priors logit bump (default 1.0).")
    # Legacy aliases (kept for backwards compatibility; hidden in help)
    parser.add_argument("--memory_max_seeded", type=int, default=None, help=argparse.SUPPRESS)
    parser.add_argument("--memory_prior_alpha", type=float, default=None, help=argparse.SUPPRESS)
    parser.set_defaults(use_memory=True)

    # Diversity guard toggles for Attempt B
    parser.add_argument("--diversity_guard", dest="diversity_guard", action="store_true",
        help="Enable Attempt-B diversity guard (default ON).")
    parser.add_argument("--no_diversity_guard", dest="diversity_guard", action="store_false",
        help="Disable Attempt-B diversity guard.")
    parser.add_argument("--diversity_b_force_first", action="store_true", default=False,
        help="Force Attempt-B to start with a different opener when Attempt-A was align-first.")
    parser.add_argument("--attemptB_beam_scale", type=float, default=1.0,
        help="Heuristic diversity scaling factor for Attempt-B candidate selection (default 1.0).")
    parser.add_argument("--attemptB_time_scale", type=float, default=1.0,
        help="Heuristic schedule scaling for Attempt-B debate/time (default 1.0).")
    parser.set_defaults(diversity_guard=True)

    # HFP × GOF integration
    parser.add_argument("--no_hfp", action="store_true",
                        help="Disable HFP×GOF integration (ρ gating + R break).")
    parser.add_argument("--hfp_alpha", type=float, default=0.50,
                        help="EMA smoothing for ρ (default 0.50).")
    parser.add_argument("--hfp_rho_hi", type=float, default=0.80,
                        help="ρ threshold for healthy band (default 0.80).")
    parser.add_argument("--hfp_rho_lo", type=float, default=0.55,
                        help="ρ threshold for degraded band (default 0.55).")
    parser.add_argument(
        "--hfp_break_R_min",
        type=float,
        default=0.010,
        help="Break threshold on canonical R(ρ); abort when R < this value (default 0.010).",
    )
    parser.add_argument(
        "--hfp_break_R",
        dest="hfp_break_R_min",
        type=float,
        help=argparse.SUPPRESS,
    )
    parser.add_argument("--no_hfp_gate_by_health", action="store_true",
                        help="Disable successor gating by health bands.")
    parser.add_argument("--no_hfp_gate_keep", action="store_true",
                        help="Do not preserve strong object/align openers during health gating.")
    parser.add_argument("--no_hfp_debug_telemetry", action="store_true",
                        help="Omit hfp_gof telemetry block from outputs.")
    
    # Ablations
    parser.add_argument("--ablation", type=str, default=None,
                       choices=["no_oco", "no_slice", "no_rotation"],
                       help="Run ablation experiment")
    
    # Logging
    parser.add_argument("--public_mode", action="store_true",
                       help="Use public-facing terminology in logs")
    parser.add_argument("--log_every", type=int, default=200,
                       help="Log every N iterations (default: 200)")
    parser.add_argument("--jsonl", type=str, default=None,
                       help="Path for JSONL telemetry log")
    parser.add_argument("--exact_wins_csv", type=str, default=None,
                       help="If set, write exact-win attribution events to this CSV.")
    parser.add_argument("--octo_stats_csv", type=str, default=None,
                       help="If set, write per-task octonion/confounds telemetry rows to this CSV.")
    
    # Determinism
    parser.add_argument("--seed", type=int, default=1337,
                       help="Random seed for determinism (default: 1337)")
    
    args = parser.parse_args()

    if getattr(args, "bounce_max", None) is not None:
        try:
            args.max_bounces = int(args.bounce_max)
        except Exception:
            pass
    args.bounce_max = args.max_bounces

    global _CLI_ARGS
    _CLI_ARGS = args

    global SEED
    try:
        SEED = int(args.seed)
    except Exception:
        SEED = 0
    _set_determinism(SEED)

    if not _NUMPY_AVAILABLE:
        print(f"ERROR: {_NUMPY_IMPORT_ERROR}", file=sys.stderr)
        sys.exit(1)

    # Derived configuration knobs (memory/diversity)
    motif_topk = args.motif_topk
    if motif_topk is None and args.memory_max_seeded is not None:
        motif_topk = args.memory_max_seeded
    if motif_topk is None:
        motif_topk = 1
    priors_alpha = args.priors_alpha
    if priors_alpha is None and args.memory_prior_alpha is not None:
        priors_alpha = args.memory_prior_alpha
    if priors_alpha is None:
        priors_alpha = 1.0
    memory_dir_effective = args.memory_dir or os.environ.get("ARC_MEMORY_DIR", "./memory_bank")
    use_memory_flag = bool(args.use_memory)
    memory_motifs_flag = not args.no_memory_motifs
    memory_priors_flag = not args.no_memory_priors
    diversity_guard_flag = bool(args.diversity_guard)

    # Banner
    print("=" * 80)
    print("ARC-ONE: Octonionic Control Overlay for Abstract Reasoning")
    print("=" * 80)
    print(f"Configuration:")
    block_identity_flag = (not args.no_block_identity)
    rails_enabled = (not args.no_rails_scale_hard)
    print(f"  Beam width: {args.beam}")
    print(f"  Max depth: {args.depth}")
    print(f"  Max seconds: {args.seconds}")
    print(f"  OCO penalties: λ_len={args.lambda_len}, λ1={args.lambda1}, λ2={args.lambda2}")
    if args.two_attempts:
        print(f"  Two attempts mode: {args.attempt2_strategy}")
    if args.ablation:
        print(f"  Ablation: {args.ablation}")
    print(
        f"  Octo prior: {'ON' if not args.no_octo_prior else 'OFF'} "
        f"(alpha={args.octo_alpha}, clip={args.octo_clip})"
    )
    print(
        f"  Stage controls: no_polish={'ON' if args.no_polish else 'OFF'}, "
        f"stop_if_diversity={args.stop_if_diversity:.2f}"
    )
    print(
        f"  Bounce policy: {'ON' if not args.no_bounce else 'OFF'}, "
        f"lowdiv_thr={args.lowdiv_thr:.3f}, "
        f"octo_z_min_for_bounce={args.octo_z_min_for_bounce:.2f}, "
        f"bounce_max={args.bounce_max}"
    )
    print(
        f"  GOF-9000: block_identity={'ON' if block_identity_flag else 'OFF'}, "
        f"rails_scale_hard={'ON' if rails_enabled else 'OFF'}, "
        f"scale_hard_thresh={args.scale_hard_thresh:.2f}, "
        f"scale_hard_steps={args.scale_hard_steps}, "
        f"early_palette_block_steps={args.early_palette_block_steps}"
    )
    print(
        f"  HFP×GOF: {'ON' if not args.no_hfp else 'OFF'}, "
        f"rho_hi={args.hfp_rho_hi:.2f}, rho_lo={args.hfp_rho_lo:.2f}, "
        f"R_break<{args.hfp_break_R_min:.3f}, gate_by_health={'ON' if not args.no_hfp_gate_by_health else 'OFF'}"
    )
    print(
        f"  Memory: use={use_memory_flag}, motifs={memory_motifs_flag}, priors={memory_priors_flag}, "
        f"dir={memory_dir_effective}, motif_topk={motif_topk}, alpha={priors_alpha:.2f}"
    )
    print(
        f"  Diversity guard: {'ON' if diversity_guard_flag else 'OFF'}, "
        f"force_first={args.diversity_b_force_first}, "
        f"attemptB_beam_scale={args.attemptB_beam_scale:.2f}, attemptB_time_scale={args.attemptB_time_scale:.2f}"
    )
    print("=" * 80)

    # Build settings
    settings = _InternalSearchSettings(
        beam_width=args.beam,
        max_depth=args.depth,
        max_seconds=args.seconds,
        lambda_len=args.lambda_len,
        lambda1=args.lambda1,
        lambda2=args.lambda2,
        public_mode=args.public_mode,
        log_every=args.log_every,
        seed=args.seed,
        div_lambda=args.div_lambda,
        iou_cap=args.iou_cap,
        max_bounces=args.max_bounces,
        use_octo_prior=(not args.no_octo_prior),
        octo_alpha=args.octo_alpha,
        octo_clip=args.octo_clip,
        no_polish=args.no_polish,
        stop_if_diversity=args.stop_if_diversity,
        bounce_if_lowdiv=(not args.no_bounce),
        lowdiv_thr=args.lowdiv_thr,
        octo_z_min_for_bounce=args.octo_z_min_for_bounce,
        bounce_max=args.bounce_max,
        block_identity=block_identity_flag,
        rails_scale_hard=rails_enabled,
        scale_hard_thresh=args.scale_hard_thresh,
        scale_hard_steps=args.scale_hard_steps,
        early_palette_block_steps=args.early_palette_block_steps,
        monitor_enable=(not args.no_monitor),
        monitor_pressure_thresh=args.monitor_pressure_thresh,
        monitor_plateau_N=args.monitor_plateau_N,
        monitor_visited_cap=args.monitor_visited_cap,
        monitor_inject_underused=(not args.no_monitor_inject_underused),
        monitor_drop_dupe_sigs=(not args.no_monitor_drop_dupe_sigs),
        use_memory=use_memory_flag,
        memory_use_motifs=memory_motifs_flag,
        memory_use_priors=memory_priors_flag,
        memory_dir=args.memory_dir,
        priors_alpha=priors_alpha,
        motif_topk=int(max(0, motif_topk)),
        diversity_guard=diversity_guard_flag,
        diversity_b_force_first=args.diversity_b_force_first,
        attemptB_beam_scale=args.attemptB_beam_scale,
        attemptB_time_scale=args.attemptB_time_scale,
        hfp_enable=(not args.no_hfp),
        hfp_alpha=args.hfp_alpha,
        hfp_rho_hi=args.hfp_rho_hi,
        hfp_rho_lo=args.hfp_rho_lo,
        hfp_break_R_min=args.hfp_break_R_min,
        hfp_gate_by_health=(not args.no_hfp_gate_by_health),
        hfp_gate_keep=(not args.no_hfp_gate_keep),
        hfp_debug_telemetry=(not args.no_hfp_debug_telemetry),
    )

    settings.iou_cap = _envfloat("ARC_IOU_CAP", getattr(settings, "iou_cap", 0.97))
    settings.div_lambda = _envfloat("ARC_DIV_LAMBDA", getattr(settings, "div_lambda", 0.30))
    plateau_val = _envint("ARC_PLATEAU_N", getattr(settings, "monitor_plateau_N", 8))
    settings.monitor_plateau_N = max(10, plateau_val)
    visited_val = _envint("ARC_VISITED_CAP", getattr(settings, "monitor_visited_cap", 4096))
    settings.monitor_visited_cap = max(8192, visited_val)
    settings.attemptB_beam_scale = _envfloat(
        "ARC_ATTEMPTB_BEAM_SCALE", getattr(settings, "attemptB_beam_scale", 1.0)
    )
    settings.attemptB_time_scale = _envfloat(
        "ARC_ATTEMPTB_TIME_SCALE", getattr(settings, "attemptB_time_scale", 1.0)
    )
    try:
        _boost = float(os.getenv("ARC_ATTEMPTB_BOOST", "1.0"))
        if _boost > 1.0:
            settings.attemptB_beam_scale = getattr(settings, "attemptB_beam_scale", 1.0) * _boost
            settings.attemptB_time_scale = getattr(settings, "attemptB_time_scale", 1.0) * _boost
            print(f"[A/B] AttemptB boosted ×{_boost:.2f}")
    except Exception:
        pass
    R_MAX = 1.0 / (7.0 * math.e)
    if "ARC_R_GOOD" in os.environ:
        try:
            settings._hfp_R_good = float(os.getenv("ARC_R_GOOD"))
        except Exception:
            settings._hfp_R_good = 0.60 * R_MAX
    else:
        settings._hfp_R_good = 0.60 * R_MAX

    if "ARC_R_BREAK" in os.environ:
        try:
            settings.hfp_break_R_min = float(os.getenv("ARC_R_BREAK"))
        except Exception:
            settings.hfp_break_R_min = 0.15 * R_MAX
    else:
        settings.hfp_break_R_min = 0.15 * R_MAX
    
    # Apply ablation if specified
    if args.ablation:
        print(f"\n⚠️  Running ablation: {args.ablation}\n")
        settings = get_ablation_config(args.ablation, settings)
    
    # Resolve tasks dir (robust against nested competition paths in Kaggle)
    resolved_tasks_dir = args.tasks_dir  # simplified: supports ARC Prize 2025 layout directly
    print(f"🔍 Using ARC tasks at: {resolved_tasks_dir}\n")
    
    # Run solver
    with StepLogger(args.public_mode, args.jsonl, args.log_every) as logger:
        results = run_dir(resolved_tasks_dir, settings, args.max_tasks, logger)
    
    # Build final predictions object (single- or two-attempts)
    if args.two_attempts:
        predictions = _two_attempts_from_results(
            results,
            tasks_dir=resolved_tasks_dir,
            strategy=args.attempt2_strategy,
            settings=settings,
        )
    else:
        predictions = results  # legacy single-output-per-test schema
    
    # Write submission (function dumps whatever dict we pass)
    write_submission_json(args.out, predictions)

    _flush_exact_wins(args.exact_wins_csv)
    _flush_octo_csv(args.octo_stats_csv)
    if args.exact_wins_csv:
        print(f"[LOG] exact-win events → {args.exact_wins_csv}")
    if args.octo_stats_csv:
        print(f"[LOG] octonion telemetry → {args.octo_stats_csv}")

    # Summary
    print("\n" + "=" * 80)
    print(f"✅ COMPLETE!")
    print("=" * 80)
    print(f"  Output: {args.out}")
    print(f"  Tasks: {len(results)}")
    if args.two_attempts:
        print(f"  Format: Two attempts ({args.attempt2_strategy})")
    if args.jsonl:
        print(f"  Telemetry: {args.jsonl}")
    print("=" * 80)


# === ARC-ONE public API shim (compat) =======================================
# Provides: SearchSettings, solve_task(task_id, task_json, settings)
# Normalizes outputs: list[{"grid": <2D list>, "_telemetry": {"ops_tokens": [...]}}]

from dataclasses import dataclass
from typing import Any, Callable, Dict, List, Tuple, Optional
import numpy as np
import inspect, sys


def make_sample_submission(preds_by_tid):
    """
    preds_by_tid: dict[task_id] -> 2D list (grid prediction)
    Returns dict in Kaggle's expected schema.
    """
    return {tid: [{"output": grid}] for tid, grid in preds_by_tid.items()}

# 1) Lightweight SearchSettings (safe defaults; ignored if unused downstream)
@dataclass
class SearchSettings:
    beam_width: int = 64
    max_depth: int = 8
    max_seconds: float = 30.0
    # StepLogger / beam-loop logging cadence (iterations between messages)
    log_every: int = _envint("ARC_LOG_EVERY", 200)
    # GOF-9000 policy knobs (default ON; modules may ignore)
    block_identity: bool = True
    rails_scale_hard: bool = True
    scale_hard_thresh: float = _envfloat("ARC_SCALE_HARD_THRESH", 0.60)
    scale_hard_steps: int = 3
    early_palette_block_steps: int = 3
    # Palette test policy (optional)
    test_palette_policy: str = "second_only_guarded"
    # --- GOF-9000 SelfMonitor -----------------------------------------------
    monitor_enable: bool = True
    monitor_pressure_thresh: float = 0.70
    monitor_plateau_N: int = 3
    monitor_visited_cap: int = 50000
    monitor_inject_underused: bool = True
    monitor_drop_dupe_sigs: bool = True
    # --- HFP × GOF integration (v2.9.34) -------------------------------------
    hfp_enable: bool = True
    hfp_alpha: float = 0.50
    hfp_rho_hi: float = 0.80
    hfp_rho_lo: float = 0.55
    hfp_break_R_min: float = 0.010
    hfp_gate_by_health: bool = True
    hfp_gate_keep: bool = True
    hfp_debug_telemetry: bool = True
    # --- Memory & Diversity (v2.10.x) ----------------------------------------
    use_memory: bool = True
    memory_use_motifs: bool = True
    memory_use_priors: bool = True
    memory_dir: Optional[str] = None
    priors_alpha: float = 1.0
    motif_topk: int = 1
    diversity_guard: bool = True
    use_meta_controller: bool = _envbool("ARC_USE_META", False)
    diversity_b_force_first: bool = False
    attemptB_beam_scale: float = 1.0
    attemptB_time_scale: float = 1.0
    div_lambda: float = _envfloat("ARC_DIV_LAMBDA", 0.70)
    iou_cap: float = _envfloat("ARC_IOU_CAP", 0.92)
    attemptB_auto_boost: bool = _envbool("ARC_ATTEMPTB_AUTO_BOOST", True)
    attemptB_boost_beam: float = _envfloat("ARC_ATTEMPTB_BOOST_BEAM", 1.5)
    attemptB_boost_time: float = _envfloat("ARC_ATTEMPTB_BOOST_TIME", 1.3)
    shaped_cost: bool = _envbool("ARC_SHAPED_COST", True)
    explore_fraction: float = _envfloat("ARC_EXPLORE_FRAC", 0.35)
    allow_uphill: bool = _envbool("ARC_ALLOW_UPHILL", True)
    uphill_keep_fraction: float = _envfloat("ARC_UPHILL_KEEP", 0.15)
    ephemeral_macros: bool = _envbool("ARC_EPHEMERAL_MACROS", False)
    macro_max_depth_start: int = _envint("ARC_MACRO_MAX_DEPTH_START", 2)
    macro_candidates_limit: int = _envint("ARC_MACRO_LIMIT", 6)
    # cost weights (align with internal solver defaults)
    lambda_len: float = _envfloat("ARC_LAMBDA_LEN", 0.20)
    lambda1: float = _envfloat("ARC_LAMBDA1", 0.30)
    lambda2: float = _envfloat("ARC_LAMBDA2", 0.20)
    max_bounces: int = 8

_INTERNAL_SETTINGS_CLS = globals().get("_InternalSearchSettings")
_SETTINGS_SYNC_ATTRS = (
    "beam_width",
    "max_depth",
    "max_seconds",
    "block_identity",
    "rails_scale_hard",
    "scale_hard_thresh",
    "scale_hard_steps",
    "early_palette_block_steps",
    "test_palette_policy",
    "monitor_enable",
    "monitor_pressure_thresh",
    "monitor_plateau_N",
    "monitor_visited_cap",
    "monitor_inject_underused",
    "monitor_drop_dupe_sigs",
    "use_memory",
    "memory_use_motifs",
    "memory_use_priors",
    "memory_dir",
    "priors_alpha",
    "motif_topk",
    "diversity_guard",
    "diversity_b_force_first",
    "attemptB_beam_scale",
    "attemptB_time_scale",
    "attemptB_auto_boost",
    "attemptB_boost_beam",
    "attemptB_boost_time",
    "div_lambda",
    "iou_cap",
    "shaped_cost",
    "explore_fraction",
    "allow_uphill",
    "uphill_keep_fraction",
    "ephemeral_macros",
    "macro_max_depth_start",
    "macro_candidates_limit",
    "max_bounces",
    "hfp_enable",
    "hfp_alpha",
    "hfp_rho_hi",
    "hfp_rho_lo",
    "hfp_break_R_min",
    "hfp_gate_by_health",
    "hfp_gate_keep",
    "hfp_debug_telemetry",
)

def _coerce_settings(settings_obj: Optional[Any]) -> Any:
    if _INTERNAL_SETTINGS_CLS is None:
        return settings_obj
    if settings_obj is None:
        return _INTERNAL_SETTINGS_CLS()
    if isinstance(settings_obj, _INTERNAL_SETTINGS_CLS):
        return settings_obj
    if isinstance(settings_obj, SearchSettings):
        coerced = _INTERNAL_SETTINGS_CLS()
        for name in _SETTINGS_SYNC_ATTRS:
            setattr(coerced, name, getattr(settings_obj, name))
        return coerced
    return settings_obj

# 2) Discover an existing single-task solver in this module
def _discover_solver() -> Tuple[Callable, int]:
    mod = sys.modules[__name__]
    candidates = (
        "_solve_task_internal",
        "solve_eval_one",
        "solve_task",
        "solve",
        "run_task",
        "infer_eval_one",
        "evaluate_one",
    )
    for name in candidates:
        fn = getattr(mod, name, None)
        if callable(fn):
            try:
                arity = len(inspect.signature(fn).parameters)
            except Exception:
                arity = 2
            return fn, arity
    raise RuntimeError("No single-task solver found. Expected one of: "
                       "_solve_task_internal / solve_eval_one / solve / run_task / infer_eval_one / evaluate_one.")

_SOLVE_FN, _SOLVE_ARITY = _discover_solver()

def _wrap_to_list(result: Any) -> List[Dict[str, Any]]:
    """Ensure we return: list of dicts with 'grid' and '_telemetry.ops_tokens'."""

    def _grid_from(first: Any):
        import numpy as _np

        if isinstance(first, dict):
            if "grid" in first:
                g = first["grid"]
                return g.tolist() if isinstance(g, _np.ndarray) else g
            if "attempt_1" in first:
                g = first["attempt_1"]
                return g.tolist() if isinstance(g, _np.ndarray) else g
        return _np.array(first).tolist() if not isinstance(first, dict) else [[0]]

    # Already a non-empty list
    if isinstance(result, list) and result:
        first = result[0]
        if isinstance(first, dict):
            for entry in result:
                if not isinstance(entry, dict):
                    continue
                td = entry.setdefault("_telemetry", {})
                if "ops_tokens" not in td:
                    prog = entry.get("program", None)
                    if prog is not None:
                        td["ops_tokens"] = _ops_tokens_from_program(prog)
                tokens = td.get("ops_tokens", [])
                if tokens is None:
                    tokens = []
                elif isinstance(tokens, (list, tuple)):
                    tokens = list(tokens)
                else:
                    tokens = [tokens]
                td["ops_tokens"] = [_gof_sanitize_token(str(t)) for t in tokens]
            return result
        first_norm = {"grid": _grid_from(first), "_telemetry": {}}
        toks = _ops_tokens_from_program(getattr(first, "program", None))
        first_norm["_telemetry"]["ops_tokens"] = [_gof_sanitize_token(str(t)) for t in toks]
        return [first_norm]

    if isinstance(result, list):
        return []

    first_norm = {"grid": _grid_from(result), "_telemetry": {}}
    toks = _ops_tokens_from_program(getattr(result, "program", None))
    first_norm["_telemetry"]["ops_tokens"] = [_gof_sanitize_token(str(t)) for t in toks]
    return [first_norm]


# 4) Public wrapper with stable signature
def solve_task(task_id: str, task_json: Dict[str, Any], settings: Optional[SearchSettings] = None):
    settings_obj = settings or SearchSettings()
    coerced_settings = _coerce_settings(settings_obj)
    if _SOLVE_ARITY >= 3:
        raw = _SOLVE_FN(task_id, task_json, coerced_settings)
    elif _SOLVE_ARITY == 2:
        raw = _SOLVE_FN(task_id, task_json)
    else:
        raw = _SOLVE_FN(task_json)  # last-resort form
    return _wrap_to_list(raw)


# 5) One-time banner
try:
    _printed_banner
except NameError:
    _printed_banner = True
    ver = globals().get("__version__", "(unknown)")
    print(f"[ARC-ONE] API shim active — version {ver} | solve_task + SearchSettings normalized.")
    try:
        _default_settings = SearchSettings()
    except Exception:
        _default_settings = None
    if _default_settings is not None:
        print(
            "[ARC-ONE] GOF-9000 monitor: "
            f"{'ON' if _default_settings.monitor_enable else 'OFF'} | "
            f"pressure≥{_default_settings.monitor_pressure_thresh:.2f} | "
            f"inject_underused={_default_settings.monitor_inject_underused} | "
            f"drop_dupes={_default_settings.monitor_drop_dupe_sigs}"
        )
        print(
            "[ARC-ONE] HFP×GOF: "
            f"{'ON' if _default_settings.hfp_enable else 'OFF'} | "
            f"ρ_hi={_default_settings.hfp_rho_hi:.2f} ρ_lo={_default_settings.hfp_rho_lo:.2f} "
            f"R_break<{_default_settings.hfp_break_R_min:.3f} | "
            f"gate_by_health={_default_settings.hfp_gate_by_health}"
        )
        print(
            "[ARC-ONE] Memory: "
            f"use={_default_settings.use_memory} | "
            f"motifs={_default_settings.memory_use_motifs} | "
            f"priors={_default_settings.memory_use_priors} | "
            f"motif_topk={_default_settings.motif_topk} | "
            f"alpha={_default_settings.priors_alpha:.2f}"
        )
        print(
            "[ARC-ONE] Diversity guard: "
            f"{'ON' if _default_settings.diversity_guard else 'OFF'} | "
            f"force_first={_default_settings.diversity_b_force_first} | "
            f"attemptB_beam_scale={_default_settings.attemptB_beam_scale:.2f} | "
            f"attemptB_time_scale={_default_settings.attemptB_time_scale:.2f}"
        )
# === end API shim ============================================================


if __name__ == "__main__":
    # Check for test mode
    if os.environ.get("ARC_ONE_RUN_TESTS") == "1":
        print("Test mode not included in this artifact - run tests separately")
        print("To use the solver, run: python arc_one.py --tasks_dir <path>")
    else:
        main()
